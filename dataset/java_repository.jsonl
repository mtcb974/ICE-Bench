{"hash_id": "6712ca73ea1cb5649aa72a63", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `loadDocuments(Path directoryPath, DocumentParser documentParser)` that takes a directory path and a document parser, validates that the path is a directory, and returns an empty list if valid. If the path is not a directory, throw an IllegalArgumentException with message`Illegal directoryPath.`. You should write self-contained code starting with: ```java\npublic static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser){```", "solution": "public static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser) {\n    if (!Files.isDirectory(directoryPath)) {\n        throw new IllegalArgumentException(\"Illegal directoryPath.\");\n    }\n    return new ArrayList<>();\n}", "test": "@Test\nvoid should_load_documents_including_unknown_document_types() {\n    // given\n    Path resourceDirectory = resourceDirectory();\n    Path nonDirectoryPath = resourceDirectory.resolve(\"test-file.txt\");\n    \n    // when - valid directory should return empty list\n    List<Document> documents = loadDocuments(resourceDirectory, new TextDocumentParser());\n    \n    // then\n    assertThat(documents).isEmpty();\n    \n    // when-then - non-directory should throw IllegalArgumentException\n    assertThatThrownBy(() -> loadDocuments(nonDirectoryPath, new TextDocumentParser()))\n        .isInstanceOf(IllegalArgumentException.class)\n        .hasMessage(\"Illegal directoryPath.\");\n}"}, {"turn_num": 2, "instruction": "Modify the function to list all files in the directory non-recursively using Files.list, assuming the directory is valid, and ignore subdirectories.", "solution": "public static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser) {\n    if (!isDirectory(directoryPath)) {\n        throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n    }\n\n    try (Stream<Path> pathStream = Files.list(directoryPath)) {\n        return loadDocuments(pathStream, (path) -> !Files.isDirectory(path), directoryPath, documentParser);\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n}", "test": "@Test\nvoid should_load_documents_including_unknown_document_types() {\n\n    // given\n    Path resourceDirectory = resourceDirectory();\n\n    // when\n    List<Document> documents = loadDocuments(resourceDirectory, new TextDocumentParser());\n\n    // then\n    List<String> fileNames = documents.stream()\n            .map(document -> document.metadata().getString(Document.FILE_NAME))\n            .toList();\n    assertThat(fileNames)\n            .containsExactlyInAnyOrder(\n                    \"miles-of-smiles-terms-of-use.txt\",\n                    \"test-file.banana\",\n                    \"test-file-iso-8859-1.txt\",\n                    \"test-file-utf8.txt\",\n                    \"chefs-prompt-based-on-ingredients-in-root.txt\");\n\n    // Verify that subdirectories are ignored by checking that all loaded documents are files\n    assertThat(documents).allMatch(doc -> {\n        String fileName = doc.metadata().getString(Document.FILE_NAME);\n        return !fileName.contains(\"/\") && !fileName.contains(\"\\\\\");\n    });\n}"}, {"turn_num": 3, "instruction": "Parse each file from the listed files into a Document object using the provided DocumentParser and collect all successfully parsed documents into the result list. If a file fails to be parsed due to an exception, skip that file without failing the entire operation and continue processing the remaining files. If an IOException occurs while listing the directory contents, wrap it in a RuntimeException with `Runtime error`and rethrow it.", "solution": "public static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser) {\n    if (!isDirectory(directoryPath)) {\n        throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n    }\n\n    try (Stream<Path> pathStream = Files.list(directoryPath)) {\n        List<Document> documents = new ArrayList<>();\n        pathStream.filter(path -> !Files.isDirectory(path))\n                .forEach(path -> {\n                    try {\n                        Document document = loadDocument(path, documentParser);\n                        documents.add(document);\n                    } catch (Exception e) {\n                        // Skip files that fail to parse\n                    }\n                });\n        return documents;\n    } catch (IOException e) {\n        throw new RuntimeException(\"Runtime error\", e);\n    }\n}", "test": "@Test\nvoid should_load_documents_including_unknown_document_types() {\n\n    // given\n    Path resourceDirectory = resourceDirectory();\n\n    // when\n    List<Document> documents = loadDocuments(resourceDirectory, new TextDocumentParser());\n\n    // then\n    List<String> fileNames = documents.stream()\n            .map(document -> document.metadata().getString(Document.FILE_NAME))\n            .toList();\n    assertThat(fileNames)\n            .containsExactlyInAnyOrder(\n                    \"miles-of-smiles-terms-of-use.txt\",\n                    \"test-file.banana\",\n                    \"test-file-iso-8859-1.txt\",\n                    \"test-file-utf8.txt\",\n                    \"chefs-prompt-based-on-ingredients-in-root.txt\");\n\n    // Test that parsing exceptions are caught and files are skipped\n    DocumentParser parserThatFailsOnFirstNonBlankDocument = new DocumentParser() {\n\n        private boolean first = true;\n        private final DocumentParser parser = new TextDocumentParser();\n\n        @Override\n        public Document parse(InputStream inputStream) {\n            if (first && isNotBlank(inputStream)) {\n                first = false;\n                throw new RuntimeException(\"fail first\");\n            }\n            return parser.parse(inputStream);\n        }\n\n        private boolean isNotBlank(InputStream inputStream) {\n            try {\n                return inputStream.available() > 10; // rough approximation\n            } catch (IOException e) {\n                throw new RuntimeException(e);\n            }\n        }\n    };\n\n    // when-then\n    assertThat(loadDocuments(resourceDirectory, parserThatFailsOnFirstNonBlankDocument))\n            .hasSize(documents.size() - 1); // -1 because first document fails\n}"}], "metadata": {"source": "mrgbench", "func": "    public static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }", "repo": "langchain4j", "file_path": "langchain4j/src/main/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoader.java", "func_name": "loadDocuments", "context": "package dev.langchain4j.data.document.loader;\n\nimport static dev.langchain4j.data.document.source.FileSystemSource.from;\nimport static dev.langchain4j.internal.Exceptions.illegalArgument;\nimport static dev.langchain4j.internal.Utils.getOrDefault;\nimport static java.nio.file.Files.isDirectory;\nimport static java.nio.file.Files.isRegularFile;\n\nimport dev.langchain4j.data.document.BlankDocumentException;\nimport dev.langchain4j.data.document.Document;\nimport dev.langchain4j.data.document.DocumentLoader;\nimport dev.langchain4j.data.document.DocumentParser;\nimport dev.langchain4j.data.document.parser.TextDocumentParser;\nimport dev.langchain4j.spi.data.document.parser.DocumentParserFactory;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.PathMatcher;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.stream.Stream;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class FileSystemDocumentLoader {\n\n    private static final Logger log = LoggerFactory.getLogger(FileSystemDocumentLoader.class);\n\n    private static final DocumentParser DEFAULT_DOCUMENT_PARSER =\n            getOrDefault(DocumentParserLoader.loadDocumentParser(), TextDocumentParser::new);\n\n    private FileSystemDocumentLoader() {}\n\n    /**\n     * Loads a {@link Document} from the specified file {@link Path}.\n     * <br>\n     * The file is parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath       The path to the file.\n     * @param documentParser The parser to be used for parsing text from the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(Path filePath, DocumentParser documentParser) {\n        if (!isRegularFile(filePath)) {\n            throw illegalArgument(\"'%s' is not a file\", filePath);\n        }\n\n        return DocumentLoader.load(from(filePath), documentParser);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file {@link Path}.\n     * <br>\n     * The file is parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath The path to the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(Path filePath) {\n        return loadDocument(filePath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file path.\n     * <br>\n     * The file is parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath       The path to the file.\n     * @param documentParser The parser to be used for parsing text from the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(String filePath, DocumentParser documentParser) {\n        return loadDocument(Paths.get(filePath), documentParser);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file path.\n     * <br>\n     * The file is parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath The path to the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(String filePath) {\n        return loadDocument(filePath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath) {\n        return loadDocuments(directoryPath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath, DocumentParser documentParser) {\n        return loadDocuments(Paths.get(directoryPath), documentParser);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath) {\n        return loadDocuments(directoryPath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param pathMatcher    Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                       For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                       will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                       When traversing the directory, each file path is converted from absolute to relative\n     *                       (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                       Thus, {@code pathMatcher} should use relative patterns.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(\n            Path directoryPath, PathMatcher pathMatcher, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, pathMatcher, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @param pathMatcher   Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                      For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                      will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                      When traversing the directory, each file path is converted from absolute to relative\n     *                      (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                      Thus, {@code pathMatcher} should use relative patterns.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath, PathMatcher pathMatcher) {\n        return loadDocuments(directoryPath, pathMatcher, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param pathMatcher    Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                       For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                       will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                       When traversing the directory, each file path is converted from absolute to relative\n     *                       (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                       Thus, {@code pathMatcher} should use relative patterns.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(\n            String directoryPath, PathMatcher pathMatcher, DocumentParser documentParser) {\n        return loadDocuments(Paths.get(directoryPath), pathMatcher, documentParser);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @param pathMatcher   Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                      For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                      will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                      When traversing the directory, each file path is converted from absolute to relative\n     *                      (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                      Thus, {@code pathMatcher} should use relative patterns.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath, PathMatcher pathMatcher) {\n        return loadDocuments(directoryPath, pathMatcher, DEFAULT_DOCUMENT_PARSER);\n    }", "func_start": 122, "func_end": 132, "body_len": 10, "test_file": "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java", "test_start": 72, "test_end": 126, "test_code": "    @Test\n    void should_load_documents_including_unknown_document_types() {\n\n        // given\n        Path resourceDirectory = resourceDirectory();\n\n        // when\n        List<Document> documents = loadDocuments(resourceDirectory, new TextDocumentParser());\n\n        // then\n        List<String> fileNames = documents.stream()\n                .map(document -> document.metadata().getString(Document.FILE_NAME))\n                .toList();\n        assertThat(fileNames)\n                .containsExactlyInAnyOrder(\n                        \"miles-of-smiles-terms-of-use.txt\",\n                        \"test-file.banana\",\n                        \"test-file-iso-8859-1.txt\",\n                        \"test-file-utf8.txt\",\n                        \"chefs-prompt-based-on-ingredients-in-root.txt\");\n\n        // when-then\n        assertThat(loadDocuments(resourceDirectory.toString(), new TextDocumentParser()))\n                .isEqualTo(documents);\n\n        assertThat(loadDocuments(resourceDirectory)).isEqualTo(documents);\n        assertThat(loadDocuments(resourceDirectory.toString())).isEqualTo(documents);\n\n        DocumentParser parserThatFailsOnFirstNonBlankDocument = new DocumentParser() {\n\n            private boolean first = true;\n            private final DocumentParser parser = new TextDocumentParser();\n\n            @Override\n            public Document parse(InputStream inputStream) {\n                if (first && isNotBlank(inputStream)) {\n                    first = false;\n                    throw new RuntimeException(\"fail first\");\n                }\n                return parser.parse(inputStream);\n            }\n\n            private boolean isNotBlank(InputStream inputStream) {\n                try {\n                    return inputStream.available() > 10; // rough approximation\n                } catch (IOException e) {\n                    throw new RuntimeException(e);\n                }\n            }\n        };\n\n        // when-then\n        assertThat(loadDocuments(resourceDirectory, parserThatFailsOnFirstNonBlankDocument))\n                .hasSize(documents.size() - 1); // -1 because first document fails\n    }", "test_instruction": "mvn test -pl langchain4j -Dtest=\"dev.langchain4j.data.document.loader.FileSystemDocumentLoaderTest#should_load_documents_including_unknown_document_types\"", "language": "java"}, "mt_id": 1024}
{"hash_id": "3ac2084e8f23beb3709c38d6", "total_turn": 2, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `loadDocumentsRecursively(Path directoryPath, DocumentParser documentParser)` that takes a directory path and a document parser, walks through the directory and its subdirectories using Files.walk, parses each file into a Document using the provided parser, and returns a list of parsed Documents.  Skip any files that fail to be parsed or read, and continue processing the remaining files without throwing an exception for individual file failures. You should write self-contained code starting with: ```java\npublic static List<Document> loadDocumentsRecursively(Path directoryPath, DocumentParser documentParser) {```", "solution": "public static List<Document> loadDocumentsRecursively(Path directoryPath, DocumentParser documentParser) {\n    if (!isDirectory(directoryPath)) {\n        throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n    }\n\n    try (Stream<Path> pathStream = Files.walk(directoryPath)) {\n        return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n}", "test": "@Test\nvoid should_recursively_load_documents() {\n\n    // given\n    Path resourceDirectory = resourceDirectory();\n\n    // when\n    List<Document> documents = loadDocumentsRecursively(resourceDirectory, new TextDocumentParser());\n\n    // then\n    List<String> fileNames = documents.stream()\n            .map(document -> document.metadata().getString(Document.FILE_NAME))\n            .toList();\n    assertThat(fileNames)\n            .containsExactlyInAnyOrder(\n                    \"miles-of-smiles-terms-of-use.txt\",\n                    \"test-file.banana\",\n                    \"test-file-iso-8859-1.txt\",\n                    \"test-file-utf8.txt\",\n                    \"chefs-prompt-based-on-ingredients.txt\",\n                    \"chefs-prompt-system-message.txt\",\n                    \"chefs-prompt-based-on-ingredients-in-root.txt\",\n                    \"chefs-prompt-based-on-ingredients-in-subdirectory.txt\",\n                    \"test-file-2.banana\",\n                    \"file1.txt\",\n                    \"file2.txt\",\n                    \"test-file-3.banana\",\n                    \"test-file-4.banana\");\n\n    // when-then\n    assertThat(loadDocumentsRecursively(resourceDirectory.toString(), new TextDocumentParser()))\n            .isEqualTo(documents);\n\n    assertThat(loadDocumentsRecursively(resourceDirectory)).isEqualTo(documents);\n    assertThat(loadDocumentsRecursively(resourceDirectory.toString())).isEqualTo(documents);\n}"}, {"turn_num": 2, "instruction": "Throw an IllegalArgumentException with message `The directoryPath you provided is illegal.` if the provided path is null or does not point to a valid directory. Use try-with-resources to ensure proper closure of the stream returned by Files.walk, and handle any IOException that may occur during stream traversal by wrapping it in a RuntimeException `Failed to traverse directory` .", "solution": "public static List<Document> loadDocumentsRecursively(Path directoryPath, DocumentParser documentParser) {\n    if (directoryPath == null || !isDirectory(directoryPath)) {\n        throw new IllegalArgumentException(\"The directoryPath you provided is illegal.\");\n    }\n\n    try (Stream<Path> pathStream = Files.walk(directoryPath)) {\n        return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n    } catch (IOException e) {\n        throw new RuntimeException(\"Failed to traverse directory\", e);\n    }\n}", "test": "@Test\nvoid should_recursively_load_documents() {\n\n    // given\n    Path resourceDirectory = resourceDirectory();\n\n    // when\n    List<Document> documents = loadDocumentsRecursively(resourceDirectory, new TextDocumentParser());\n\n    // then\n    List<String> fileNames = documents.stream()\n            .map(document -> document.metadata().getString(Document.FILE_NAME))\n            .toList();\n    assertThat(fileNames)\n            .containsExactlyInAnyOrder(\n                    \"miles-of-smiles-terms-of-use.txt\",\n                    \"test-file.banana\",\n                    \"test-file-iso-8859-1.txt\",\n                    \"test-file-utf8.txt\",\n                    \"chefs-prompt-based-on-ingredients.txt\",\n                    \"chefs-prompt-system-message.txt\",\n                    \"chefs-prompt-based-on-ingredients-in-root.txt\",\n                    \"chefs-prompt-based-on-ingredients-in-subdirectory.txt\",\n                    \"test-file-2.banana\",\n                    \"file1.txt\",\n                    \"file2.txt\",\n                    \"test-file-3.banana\",\n                    \"test-file-4.banana\");\n\n    // when-then\n    assertThat(loadDocumentsRecursively(resourceDirectory.toString(), new TextDocumentParser()))\n            .isEqualTo(documents);\n\n    assertThat(loadDocumentsRecursively(resourceDirectory)).isEqualTo(documents);\n    assertThat(loadDocumentsRecursively(resourceDirectory.toString())).isEqualTo(documents);\n    \n    // Test null path throws IllegalArgumentException\n    assertThatThrownBy(() -> loadDocumentsRecursively((Path) null, new TextDocumentParser()))\n            .isInstanceOf(IllegalArgumentException.class)\n            .hasMessage(\"The directoryPath you provided is illegal.\");\n    \n    // Test invalid directory throws IllegalArgumentException\n    Path nonExistentPath = Paths.get(\"non-existent-directory\");\n    assertThatThrownBy(() -> loadDocumentsRecursively(nonExistentPath, new TextDocumentParser()))\n            .isInstanceOf(IllegalArgumentException.class)\n            .hasMessage(\"The directoryPath you provided is illegal.\");\n}"}], "metadata": {"source": "mrgbench", "func": "    public static List<Document> loadDocumentsRecursively(Path directoryPath, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.walk(directoryPath)) {\n            return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }", "repo": "langchain4j", "file_path": "langchain4j/src/main/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoader.java", "func_name": "loadDocumentsRecursively", "context": "package dev.langchain4j.data.document.loader;\n\nimport static dev.langchain4j.data.document.source.FileSystemSource.from;\nimport static dev.langchain4j.internal.Exceptions.illegalArgument;\nimport static dev.langchain4j.internal.Utils.getOrDefault;\nimport static java.nio.file.Files.isDirectory;\nimport static java.nio.file.Files.isRegularFile;\n\nimport dev.langchain4j.data.document.BlankDocumentException;\nimport dev.langchain4j.data.document.Document;\nimport dev.langchain4j.data.document.DocumentLoader;\nimport dev.langchain4j.data.document.DocumentParser;\nimport dev.langchain4j.data.document.parser.TextDocumentParser;\nimport dev.langchain4j.spi.data.document.parser.DocumentParserFactory;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.PathMatcher;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.stream.Stream;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class FileSystemDocumentLoader {\n\n    private static final Logger log = LoggerFactory.getLogger(FileSystemDocumentLoader.class);\n\n    private static final DocumentParser DEFAULT_DOCUMENT_PARSER =\n            getOrDefault(DocumentParserLoader.loadDocumentParser(), TextDocumentParser::new);\n\n    private FileSystemDocumentLoader() {}\n\n    /**\n     * Loads a {@link Document} from the specified file {@link Path}.\n     * <br>\n     * The file is parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath       The path to the file.\n     * @param documentParser The parser to be used for parsing text from the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(Path filePath, DocumentParser documentParser) {\n        if (!isRegularFile(filePath)) {\n            throw illegalArgument(\"'%s' is not a file\", filePath);\n        }\n\n        return DocumentLoader.load(from(filePath), documentParser);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file {@link Path}.\n     * <br>\n     * The file is parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath The path to the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(Path filePath) {\n        return loadDocument(filePath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file path.\n     * <br>\n     * The file is parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath       The path to the file.\n     * @param documentParser The parser to be used for parsing text from the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(String filePath, DocumentParser documentParser) {\n        return loadDocument(Paths.get(filePath), documentParser);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file path.\n     * <br>\n     * The file is parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath The path to the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(String filePath) {\n        return loadDocument(filePath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath) {\n        return loadDocuments(directoryPath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath, DocumentParser documentParser) {\n        return loadDocuments(Paths.get(directoryPath), documentParser);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath) {\n        return loadDocuments(directoryPath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param pathMatcher    Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                       For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                       will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                       When traversing the directory, each file path is converted from absolute to relative\n     *                       (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                       Thus, {@code pathMatcher} should use relative patterns.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(\n            Path directoryPath, PathMatcher pathMatcher, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, pathMatcher, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @param pathMatcher   Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                      For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                      will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                      When traversing the directory, each file path is converted from absolute to relative\n     *                      (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                      Thus, {@code pathMatcher} should use relative patterns.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath, PathMatcher pathMatcher) {\n        return loadDocuments(directoryPath, pathMatcher, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param pathMatcher    Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                       For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                       will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                       When traversing the directory, each file path is converted from absolute to relative\n     *                       (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                       Thus, {@code pathMatcher} should use relative patterns.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(\n            String directoryPath, PathMatcher pathMatcher, DocumentParser documentParser) {\n        return loadDocuments(Paths.get(directoryPath), pathMatcher, documentParser);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @param pathMatcher   Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                      For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                      will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                      When traversing the directory, each file path is converted from absolute to relative\n     *                      (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                      Thus, {@code pathMatcher} should use relative patterns.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath, PathMatcher pathMatcher) {\n        return loadDocuments(directoryPath, pathMatcher, DEFAULT_DOCUMENT_PARSER);\n    }", "func_start": 304, "func_end": 314, "body_len": 10, "test_file": "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java", "test_start": 157, "test_end": 192, "test_code": "    @Test\n    void should_recursively_load_documents() {\n\n        // given\n        Path resourceDirectory = resourceDirectory();\n\n        // when\n        List<Document> documents = loadDocumentsRecursively(resourceDirectory, new TextDocumentParser());\n\n        // then\n        List<String> fileNames = documents.stream()\n                .map(document -> document.metadata().getString(Document.FILE_NAME))\n                .toList();\n        assertThat(fileNames)\n                .containsExactlyInAnyOrder(\n                        \"miles-of-smiles-terms-of-use.txt\",\n                        \"test-file.banana\",\n                        \"test-file-iso-8859-1.txt\",\n                        \"test-file-utf8.txt\",\n                        \"chefs-prompt-based-on-ingredients.txt\",\n                        \"chefs-prompt-system-message.txt\",\n                        \"chefs-prompt-based-on-ingredients-in-root.txt\",\n                        \"chefs-prompt-based-on-ingredients-in-subdirectory.txt\",\n                        \"test-file-2.banana\",\n                        \"file1.txt\",\n                        \"file2.txt\",\n                        \"test-file-3.banana\",\n                        \"test-file-4.banana\");\n\n        // when-then\n        assertThat(loadDocumentsRecursively(resourceDirectory.toString(), new TextDocumentParser()))\n                .isEqualTo(documents);\n\n        assertThat(loadDocumentsRecursively(resourceDirectory)).isEqualTo(documents);\n        assertThat(loadDocumentsRecursively(resourceDirectory.toString())).isEqualTo(documents);\n    }", "test_instruction": "mvn test -pl langchain4j -Dtest=\"dev.langchain4j.data.document.loader.FileSystemDocumentLoaderTest#should_recursively_load_documents\"", "language": "java"}, "mt_id": 1025}
{"hash_id": "895990b3a5c56f093bdb72e6", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a static method `between` that takes two `Embedding` objects as parameters and returns a `double`. Start by implementing just the method signature and returning a placeholder value of 0.0. You should write self-contained code starting with: ```java\npublic static double between(Embedding embeddingA, Embedding embeddingB) {\n}\n```", "solution": "public static double between(Embedding embeddingA, Embedding embeddingB) {\n    return 0.0;\n}", "test": "@Test\nvoid should_calculate_cosine_similarity() {\n    Embedding embeddingA = Embedding.from(new float[] {1, -1, 1});\n    Embedding embeddingB = Embedding.from(new float[] {-1, 1, -1});\n\n    assertThat(CosineSimilarity.between(embeddingA, embeddingB)).isEqualTo(0.0);\n}"}, {"turn_num": 2, "instruction": "Add null checks for both embedding parameters using a helper method `ensureNotNull`, and throw an appropriate exception with messge `one of the embedding is null.` if either embedding is null.", "solution": "public static double between(Embedding embeddingA, Embedding embeddingB) {\n    ensureNotNull(embeddingA, \"one of the embedding is null\");\n    ensureNotNull(embeddingB, \"one of the embedding is null\");\n    \n    float[] vectorA = embeddingA.vector();\n    float[] vectorB = embeddingB.vector();\n\n    if (vectorA.length != vectorB.length) {\n        throw illegalArgument(\"Length of vector a (%s) must be equal to the length of vector b (%s)\",\n                vectorA.length, vectorB.length);\n    }\n\n    double dotProduct = 0.0;\n    double normA = 0.0;\n    double normB = 0.0;\n\n    for (int i = 0; i < vectorA.length; i++) {\n        dotProduct += vectorA[i] * vectorB[i];\n        normA += vectorA[i] * vectorA[i];\n        normB += vectorB[i] * vectorB[i];\n    }\n\n    // Avoid division by zero.\n    return dotProduct / Math.max(Math.sqrt(normA) * Math.sqrt(normB), EPSILON);\n}", "test": "@Test\nvoid should_calculate_cosine_similarity() {\n    // Test null checks - the main focus of current instruction\n    assertThatThrownBy(() -> CosineSimilarity.between(null, Embedding.from(new float[] {1, 2, 3})))\n        .hasMessage(\"one of the embedding is null cannot be null\");\n    assertThatThrownBy(() -> CosineSimilarity.between(Embedding.from(new float[] {1, 2, 3}), null))\n        .hasMessage(\"one of the embedding is null cannot be null\");\n}"}, {"turn_num": 3, "instruction": "Ensure the method throws an IllegalArgumentException with message `the vector lengths of the two embeddings should be equal.` if the vector lengths of the two embeddings are not equal.", "solution": "public static double between(Embedding embeddingA, Embedding embeddingB) {\n    ensureNotNull(embeddingA, \"one of the embedding is null\");\n    ensureNotNull(embeddingB, \"one of the embedding is null\");\n    \n    float[] vectorA = embeddingA.vector();\n    float[] vectorB = embeddingB.vector();\n\n    if (vectorA.length != vectorB.length) {\n        throw new IllegalArgumentException(\"the vector lengths of the two embeddings should be equal.\");\n    }\n\n    double dotProduct = 0.0;\n    double normA = 0.0;\n    double normB = 0.0;\n\n    for (int i = 0; i < vectorA.length; i++) {\n        dotProduct += vectorA[i] * vectorB[i];\n        normA += vectorA[i] * vectorA[i];\n        normB += vectorB[i] * vectorB[i];\n    }\n\n    // Avoid division by zero.\n    return dotProduct / Math.max(Math.sqrt(normA) * Math.sqrt(normB), EPSILON);\n}", "test": "@Test\nvoid should_calculate_cosine_similarity() {\n    Embedding embeddingA = Embedding.from(new float[] {1, -1, 1});\n    Embedding embeddingB = Embedding.from(new float[] {-1, 1, -1});\n    Embedding embeddingC = Embedding.from(new float[] {1, 2});\n\n    assertThat(CosineSimilarity.between(embeddingA, embeddingA)).isCloseTo(1, withPercentage(1));\n    assertThat(CosineSimilarity.between(embeddingA, embeddingB)).isCloseTo(-1, withPercentage(1));\n    \n    assertThatThrownBy(() -> CosineSimilarity.between(embeddingA, embeddingC))\n        .isInstanceOf(IllegalArgumentException.class)\n        .hasMessage(\"the vector lengths of the two embeddings should be equal.\");\n}"}, {"turn_num": 4, "instruction": "Compute the dot product and the squared norms (sum of squares) of both vectors in a single loop, then calculate the cosine similarity as the dot product divided by the product of the Euclidean norms of the two vectors. Handle zero vectors by treating them as orthogonal to all other vectors, including themselves, ensuring the method returns 0.0 when either or both vectors are all-zero, by using a small epsilon value to prevent division by zero while maintaining the semantic that zero vectors have zero similarity.", "solution": "public static double between(Embedding embeddingA, Embedding embeddingB) {\n    ensureNotNull(embeddingA, \"embeddingA\");\n    ensureNotNull(embeddingB, \"embeddingB\");\n    \n    float[] vectorA = embeddingA.vector();\n    float[] vectorB = embeddingB.vector();\n\n    if (vectorA.length != vectorB.length) {\n        throw illegalArgument(\"Length of vector a (%s) must be equal to the length of vector b (%s)\",\n                vectorA.length, vectorB.length);\n    }\n\n    double dotProduct = 0.0;\n    double normA = 0.0;\n    double normB = 0.0;\n\n    for (int i = 0; i < vectorA.length; i++) {\n        dotProduct += vectorA[i] * vectorB[i];\n        normA += vectorA[i] * vectorA[i];\n        normB += vectorB[i] * vectorB[i];\n    }\n\n    double magnitudeProduct = Math.sqrt(normA) * Math.sqrt(normB);\n    \n    // Handle zero vectors by returning 0.0 for orthogonality\n    if (magnitudeProduct < EPSILON) {\n        return 0.0;\n    }\n    \n    return dotProduct / magnitudeProduct;\n}", "test": "@Test\nvoid should_calculate_cosine_similarity() {\n    Embedding embeddingA = Embedding.from(new float[] {1, -1, 1});\n    Embedding embeddingB = Embedding.from(new float[] {-1, 1, -1});\n    Embedding zeroEmbedding = Embedding.from(new float[] {0, 0, 0});\n    Embedding nonZeroEmbedding = Embedding.from(new float[] {1, 2, 3});\n    Embedding nearZeroEmbedding = Embedding.from(new float[] {1e-10f, 1e-10f, 1e-10f});\n\n    assertThat(CosineSimilarity.between(embeddingA, embeddingA)).isCloseTo(1, withPercentage(1));\n    assertThat(CosineSimilarity.between(embeddingA, embeddingB)).isCloseTo(-1, withPercentage(1));\n    \n    // Test zero vector handling - should return exactly 0.0\n    assertThat(CosineSimilarity.between(zeroEmbedding, zeroEmbedding)).isEqualTo(0.0);\n    assertThat(CosineSimilarity.between(zeroEmbedding, nonZeroEmbedding)).isEqualTo(0.0);\n    assertThat(CosineSimilarity.between(nonZeroEmbedding, zeroEmbedding)).isEqualTo(0.0);\n    \n    // Test near-zero vector handling - should return 0.0 due to epsilon threshold\n    assertThat(CosineSimilarity.between(nearZeroEmbedding, nearZeroEmbedding)).isEqualTo(0.0);\n    assertThat(CosineSimilarity.between(nearZeroEmbedding, nonZeroEmbedding)).isEqualTo(0.0);\n}"}], "metadata": {"source": "mrgbench", "func": "    public static double between(Embedding embeddingA, Embedding embeddingB) {\n        ensureNotNull(embeddingA, \"embeddingA\");\n        ensureNotNull(embeddingB, \"embeddingB\");\n\n        float[] vectorA = embeddingA.vector();\n        float[] vectorB = embeddingB.vector();\n\n        if (vectorA.length != vectorB.length) {\n            throw illegalArgument(\"Length of vector a (%s) must be equal to the length of vector b (%s)\",\n                    vectorA.length, vectorB.length);\n        }\n\n        double dotProduct = 0.0;\n        double normA = 0.0;\n        double normB = 0.0;\n\n        for (int i = 0; i < vectorA.length; i++) {\n            dotProduct += vectorA[i] * vectorB[i];\n            normA += vectorA[i] * vectorA[i];\n            normB += vectorB[i] * vectorB[i];\n        }\n\n        // Avoid division by zero.\n        return dotProduct / Math.max(Math.sqrt(normA) * Math.sqrt(normB), EPSILON);\n    }", "repo": "langchain4j", "file_path": "langchain4j-core/src/main/java/dev/langchain4j/store/embedding/CosineSimilarity.java", "func_name": "between", "context": "package dev.langchain4j.store.embedding;\n\nimport dev.langchain4j.data.embedding.Embedding;\n\nimport static dev.langchain4j.internal.Exceptions.illegalArgument;\nimport static dev.langchain4j.internal.ValidationUtils.ensureNotNull;\n\n/**\n * Utility class for calculating cosine similarity between two vectors.\n */\npublic class CosineSimilarity {\n    private CosineSimilarity() {}\n\n    /**\n     * A small value to avoid division by zero.\n     */\n    public static final float EPSILON = 1e-8f;", "func_start": 40, "func_end": 64, "body_len": 24, "test_file": "langchain4j-core/src/test/java/dev/langchain4j/store/embedding/CosineSimilarityTest.java", "test_start": 28, "test_end": 35, "test_code": "    @Test\n    void should_calculate_cosine_similarity() {\n        Embedding embeddingA = Embedding.from(new float[] {1, -1, 1});\n        Embedding embeddingB = Embedding.from(new float[] {-1, 1, -1});\n\n        assertThat(CosineSimilarity.between(embeddingA, embeddingA)).isCloseTo(1, withPercentage(1));\n        assertThat(CosineSimilarity.between(embeddingA, embeddingB)).isCloseTo(-1, withPercentage(1));\n    }", "test_instruction": "mvn test -pl langchain4j-core -Dtest=\"dev.langchain4j.store.embedding.CosineSimilarityTest#should_calculate_cosine_similarity\"", "language": "java"}, "mt_id": 1026}
{"hash_id": "3311fcb30839fc3620dccd19", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `methodToProperty(String name)` that converts a Java method name into its corresponding property name by checking if it starts with 'get', then removing the 'get' prefix and converting the first letter of the remaining string to lowercase. If the name does not start with 'get', throw an exception with the message 'Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.' formatted with the actual method name. You should write self-contained code starting with: ```java\npublic static String methodToProperty(String name) {\n}```", "solution": "public static String methodToProperty(String name) {\n    if (name.startsWith(\"get\")) {\n        name = name.substring(3);\n        if (!name.isEmpty()) {\n            name = name.substring(0, 1).toLowerCase(Locale.ENGLISH).concat(name.substring(1));\n        }\n        return name;\n    } else {\n        throw new RuntimeException(String.format(\"Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.\", name));\n    }\n}", "test": "@Test\npublic void testMethod2Property() {\n    Assert.assertEquals(\"u\", StringUtil.methodToProperty(\"getU\"));\n    Assert.assertEquals(\"name\", StringUtil.methodToProperty(\"getName\"));\n    Assert.assertEquals(\"uName\", StringUtil.methodToProperty(\"getUName\"));\n    Assert.assertThrows(RuntimeException.class, () -> StringUtil.methodToProperty(\"name\"));\n    Assert.assertThrows(RuntimeException.class, () -> StringUtil.methodToProperty(\"isU\"));\n    Assert.assertThrows(RuntimeException.class, () -> StringUtil.methodToProperty(\"setName\"));\n}"}, {"turn_num": 2, "instruction": "Extend the function to also handle the prefix 'set' by removing it and converting the first letter of the remaining name to lowercase, using the same logic as for 'get'. The function must still throw an exception if neither 'get' nor 'set' is present.", "solution": "public static String methodToProperty(String name) {\n    if (name.startsWith(\"get\") || name.startsWith(\"set\")) {\n        name = name.substring(3);\n        if (!name.isEmpty()) {\n            name = name.substring(0, 1).toLowerCase(Locale.ENGLISH).concat(name.substring(1));\n        }\n        return name;\n    } else {\n        throw new RuntimeException(String.format(\"Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.\", name));\n    }\n}", "test": "@Test\npublic void testMethod2Property() {\n    Assert.assertEquals(\"u\", StringUtil.methodToProperty(\"getU\"));\n    Assert.assertEquals(\"name\", StringUtil.methodToProperty(\"getName\"));\n    Assert.assertEquals(\"uName\", StringUtil.methodToProperty(\"getUName\"));\n    Assert.assertEquals(\"u\", StringUtil.methodToProperty(\"setU\"));\n    Assert.assertEquals(\"name\", StringUtil.methodToProperty(\"setName\"));\n    Assert.assertEquals(\"uName\", StringUtil.methodToProperty(\"setUName\"));\n    Assert.assertThrows(RuntimeException.class, () -> StringUtil.methodToProperty(\"name\"));\n}"}, {"turn_num": 3, "instruction": "Add support for the prefix 'is', which should be removed and the first letter of the remaining name converted to lowercase. The function should now check for all three prefixes ('is', 'get', 'set') and process accordingly, maintaining correct exception behavior for invalid inputs. Use Locale.ENGLISH when converting the first letter to lowercase to ensure consistent behavior across different locales.", "solution": "public static String methodToProperty(String name) {\n    if (name.startsWith(\"is\")) {\n        name = name.substring(2);\n    } else if (name.startsWith(\"get\") || name.startsWith(\"set\")) {\n        name = name.substring(3);\n    } else {\n        throw FlexExceptions.wrap(\"Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.\", name);\n    }\n    if (!name.isEmpty()) {\n        name = name.substring(0, 1).toLowerCase(Locale.ENGLISH).concat(name.substring(1));\n    }\n    return name;\n}", "test": "@Test\npublic void testMethod2Property() {\n    Assert.assertEquals(\"u\", StringUtil.methodToProperty(\"isU\"));\n    Assert.assertEquals(\"u\", StringUtil.methodToProperty(\"getU\"));\n    Assert.assertEquals(\"name\", StringUtil.methodToProperty(\"getName\"));\n    Assert.assertEquals(\"uName\", StringUtil.methodToProperty(\"getUName\"));\n    Assert.assertEquals(\"uName\", StringUtil.methodToProperty(\"isUName\"));\n    Assert.assertThrows(MybatisFlexException.class, () -> StringUtil.methodToProperty(\"name\"));\n}"}], "metadata": {"source": "mrgbench", "func": "    public static String methodToProperty(String name) {\n        if (name.startsWith(\"is\")) {\n            name = name.substring(2);\n        } else if (name.startsWith(\"get\") || name.startsWith(\"set\")) {\n            name = name.substring(3);\n        } else {\n            throw FlexExceptions.wrap(\"Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.\", name);\n        }\n        if (!name.isEmpty()) {\n            name = name.substring(0, 1).toLowerCase(Locale.ENGLISH).concat(name.substring(1));\n        }\n        return name;\n    }", "repo": "mybatis-flex", "file_path": "mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java", "func_name": "methodToProperty", "context": "/*\n *  Copyright (c) 2022-2025, Mybatis-Flex (fuhai999@gmail.com).\n *  <p>\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *  <p>\n *  http://www.apache.org/licenses/LICENSE-2.0\n *  <p>\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\npackage com.mybatisflex.core.util;\n\n\nimport com.mybatisflex.core.exception.FlexExceptions;\n\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.function.Function;\nimport java.util.regex.Pattern;\n\npublic class StringUtil {\n\n    private StringUtil() {\n    }", "func_start": 34, "func_end": 46, "body_len": 12, "test_file": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java", "test_start": 38, "test_end": 46, "test_code": "    @Test\n    public void testMethod2Property() {\n        Assert.assertEquals(\"u\", StringUtil.methodToProperty(\"isU\"));\n        Assert.assertEquals(\"u\", StringUtil.methodToProperty(\"getU\"));\n        Assert.assertEquals(\"name\", StringUtil.methodToProperty(\"getName\"));\n        Assert.assertEquals(\"uName\", StringUtil.methodToProperty(\"getUName\"));\n        Assert.assertEquals(\"uName\", StringUtil.methodToProperty(\"isUName\"));\n        Assert.assertThrows(MybatisFlexException.class, () -> StringUtil.methodToProperty(\"name\"));\n    }", "test_instruction": "mvn test -pl mybatis-flex-core -Dtest=\"com.mybatisflex.coretest.StringUtilTest#testMethod2Property\"", "language": "java"}, "mt_id": 1029}
{"hash_id": "4c84a38cf9b090d553a8095e", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `underlineToCamel(String string)` that handles null or empty input by returning an empty string. You should write self-contained code starting with: ```java\npublic static String underlineToCamel(String string) {```", "solution": "public static String underlineToCamel(String string) {\n    if (noText(string)) {\n        return \"\";\n    }\n    if (Character.isUpperCase(string.charAt(0))) {\n        string = string.toLowerCase();\n    }\n    int strLen = string.length();\n    StringBuilder sb = new StringBuilder(strLen);\n    for (int i = 0; i < strLen; i++) {\n        char c = string.charAt(i);\n        if (c == '_') {\n            if (++i < strLen) {\n                sb.append(Character.toUpperCase(string.charAt(i)));\n            }\n        } else {\n            sb.append(c);\n        }\n    }\n    return sb.toString();\n}", "test": "@Test\npublic void underlineToCamel() {\n    // Test null input\n    String nullResult = StringUtil.underlineToCamel(null);\n    Assert.assertEquals(\"\", nullResult);\n    \n    // Test empty string input\n    String emptyResult = StringUtil.underlineToCamel(\"\");\n    Assert.assertEquals(\"\", emptyResult);\n    \n    // Test whitespace-only input (assuming noText handles this)\n    String whitespaceResult = StringUtil.underlineToCamel(\"   \");\n    Assert.assertEquals(\"\", whitespaceResult);\n}"}, {"turn_num": 2, "instruction": "Implement the core transformation: remove each underscore and capitalize the letter immediately following it. Assume the input is non-empty, starts with a lowercase letter, and every underscore is followed by a lowercase letter. Do not handle consecutive underscores or trailing underscores yet.", "solution": "public static String underlineToCamel(String string) {\n    int strLen = string.length();\n    StringBuilder sb = new StringBuilder(strLen);\n    for (int i = 0; i < strLen; i++) {\n        char c = string.charAt(i);\n        if (c == '_') {\n            i++; // Skip underscore\n            sb.append(Character.toUpperCase(string.charAt(i)));\n        } else {\n            sb.append(c);\n        }\n    }\n    return sb.toString();\n}", "test": "@Test\npublic void underlineToCamel() {\n    // Test core transformation - these cases will fail with previous round's code\n    // that handled empty strings and uppercase first letters\n    \n    // Simple case that requires the core transformation only\n    String result1 = StringUtil.underlineToCamel(\"a_b\");\n    Assert.assertEquals(\"aB\", result1);\n    \n    // Multiple underscores - previous code would handle this the same way\n    String result2 = StringUtil.underlineToCamel(\"hello_world_test\");\n    Assert.assertEquals(\"helloWorldTest\", result2);\n    \n    // Test that would expose difference: input starting with uppercase\n    // Previous code converts to lowercase first, current code doesn't\n    String result3 = StringUtil.underlineToCamel(\"Hello_world\");\n    Assert.assertEquals(\"HelloWorld\", result3);\n    \n    // Test empty string behavior difference\n    String result4 = StringUtil.underlineToCamel(\"\");\n    Assert.assertEquals(\"\", result4);\n}"}, {"turn_num": 3, "instruction": "If the first character of the input string is uppercase, convert it to lowercase before applying the underscore-to-camel transformation. Handle edge cases where an underscore appears at the end of the string or is followed by another underscore by simply removing the underscore without capitalization.", "solution": "public static String underlineToCamel(String string) {\n    if (noText(string)) {\n        return \"\";\n    }\n    if (Character.isUpperCase(string.charAt(0))) {\n        string = string.toLowerCase();\n    }\n    int strLen = string.length();\n    StringBuilder sb = new StringBuilder(strLen);\n    for (int i = 0; i < strLen; i++) {\n        char c = string.charAt(i);\n        if (c == '_') {\n            // Skip all consecutive underscores\n            while (i + 1 < strLen && string.charAt(i + 1) == '_') {\n                i++;\n            }\n            // If there's a character after the underscore(s), capitalize it\n            if (i + 1 < strLen) {\n                i++;\n                sb.append(Character.toUpperCase(string.charAt(i)));\n            }\n        } else {\n            sb.append(c);\n        }\n    }\n    return sb.toString();\n}", "test": "@Test\npublic void underlineToCamel() {\n    // Test uppercase first character conversion\n    String testUppercase = \"Aa_bb\";\n    String result1 = StringUtil.underlineToCamel(testUppercase);\n    Assert.assertEquals(\"aaBb\", result1);\n    \n    // Test underscore at end\n    String testTrailingUnderscore = \"aa_bb_\";\n    String result2 = StringUtil.underlineToCamel(testTrailingUnderscore);\n    Assert.assertEquals(\"aaBb\", result2);\n    \n    // Test consecutive underscores\n    String testConsecutiveUnderscores = \"aa__bb\";\n    String result3 = StringUtil.underlineToCamel(testConsecutiveUnderscores);\n    Assert.assertEquals(\"aaBb\", result3);\n}"}], "metadata": {"source": "mrgbench", "func": "    public static String underlineToCamel(String string) {\n        if (noText(string)) {\n            return \"\";\n        }\n        if (Character.isUpperCase(string.charAt(0))) {\n            string = string.toLowerCase();\n        }\n        int strLen = string.length();\n        StringBuilder sb = new StringBuilder(strLen);\n        for (int i = 0; i < strLen; i++) {\n            char c = string.charAt(i);\n            if (c == '_') {\n                if (++i < strLen) {\n                    sb.append(Character.toUpperCase(string.charAt(i)));\n                }\n            } else {\n                sb.append(c);\n            }\n        }\n        return sb.toString();\n    }", "repo": "mybatis-flex", "file_path": "mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java", "func_name": "underlineToCamel", "context": "/*\n *  Copyright (c) 2022-2025, Mybatis-Flex (fuhai999@gmail.com).\n *  <p>\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *  <p>\n *  http://www.apache.org/licenses/LICENSE-2.0\n *  <p>\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\npackage com.mybatisflex.core.util;\n\n\nimport com.mybatisflex.core.exception.FlexExceptions;\n\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.function.Function;\nimport java.util.regex.Pattern;\n\npublic class StringUtil {\n\n    private StringUtil() {\n    }\n\n    /**\n     * @see org.apache.ibatis.reflection.property.PropertyNamer#methodToProperty(String)\n     */\n    public static String methodToProperty(String name) {\n        if (name.startsWith(\"is\")) {\n            name = name.substring(2);\n        } else if (name.startsWith(\"get\") || name.startsWith(\"set\")) {\n            name = name.substring(3);\n        } else {\n            throw FlexExceptions.wrap(\"Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.\", name);\n        }\n        if (!name.isEmpty()) {\n            name = name.substring(0, 1).toLowerCase(Locale.ENGLISH).concat(name.substring(1));\n        }\n        return name;\n    }\n\n\n    /**\n     * 第一个字符转换为小写\n     *\n     * @param string\n     */\n    public static String firstCharToLowerCase(String string) {\n        char firstChar = string.charAt(0);\n        if (firstChar >= 'A' && firstChar <= 'Z') {\n            char[] chars = string.toCharArray();\n            chars[0] += ('a' - 'A');\n            return new String(chars);\n        }\n        return string;\n    }\n\n\n    /**\n     * 第一个字符转换为大写\n     *\n     * @param string\n     */\n    public static String firstCharToUpperCase(String string) {\n        char firstChar = string.charAt(0);\n        if (firstChar >= 'a' && firstChar <= 'z') {\n            char[] chars = string.toCharArray();\n            chars[0] -= ('a' - 'A');\n            return new String(chars);\n        }\n        return string;\n    }\n\n\n    /**\n     * 驼峰转下划线格式\n     *\n     * @param string\n     */\n    public static String camelToUnderline(String string) {\n        if (noText(string)) {\n            return \"\";\n        }\n        int strLen = string.length();\n        StringBuilder sb = new StringBuilder(strLen);\n        for (int i = 0; i < strLen; i++) {\n            char c = string.charAt(i);\n            if (Character.isUpperCase(c) && i > 0) {\n                char prev = string.charAt(i - 1);\n                if (!Character.isUpperCase(prev) && prev != '_') {\n                    sb.append('_');\n                }\n            }\n            sb.append(Character.toLowerCase(c));\n        }\n        return sb.toString();\n    }", "func_start": 110, "func_end": 130, "body_len": 20, "test_file": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java", "test_start": 26, "test_end": 36, "test_code": "    @Test\n    public void underlineToCamel() {\n        String testString1 = \"aa_bb_\";\n        String underlineToCamel = StringUtil.underlineToCamel(testString1);\n        System.out.println(underlineToCamel);\n        Assert.assertEquals(\"aaBb\", underlineToCamel);\n\n        String underline = StringUtil.camelToUnderline(underlineToCamel);\n        System.out.println(underline);\n        Assert.assertEquals(\"aa_bb\", underline);\n    }", "test_instruction": "mvn test -pl mybatis-flex-core -Dtest=\"com.mybatisflex.coretest.StringUtilTest#underlineToCamel\"", "language": "java"}, "mt_id": 1030}
{"hash_id": "cf95a66e04f941a76d4b3552", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `stream(SpeechPrompt speechPrompt)` that takes a speech prompt containing text and synthesis options, converts it into a speech request using a helper method `createRequest`, calls an API to generate speech, and returns a Flux of SpeechResponse objects where each response contains the audio data. You should write self-contained code starting with: ```java\n@Override\npublic Flux<SpeechResponse> stream(SpeechPrompt speechPrompt) {\n}```", "solution": "@Override\npublic Flux<SpeechResponse> stream(SpeechPrompt speechPrompt) {\n    OpenAiAudioApi.SpeechRequest speechRequest = createRequest(speechPrompt);\n    \n    Flux<ResponseEntity<byte[]>> speechEntity = this.retryTemplate\n        .execute(ctx -> this.audioApi.stream(speechRequest));\n    \n    return speechEntity.map(entity -> new SpeechResponse(new Speech(entity.getBody()),\n            new OpenAiAudioSpeechResponseMetadata(OpenAiResponseHeaderExtractor.extractAiResponseHeaders(entity))));\n}", "test": "@Test\nvoid createRequestWithChatOptions() {\n    var client = OpenAiChatModel.builder()\n        .openAiApi(OpenAiApi.builder().apiKey(\"TEST\").build())\n        .defaultOptions(OpenAiChatOptions.builder().model(\"DEFAULT_MODEL\").temperature(66.6).build())\n        .build();\n\n    var prompt = client.buildRequestPrompt(new Prompt(\"Test message content\"));\n\n    var request = client.createRequest(prompt, false);\n\n    assertThat(request.messages()).hasSize(1);\n    assertThat(request.stream()).isFalse();\n\n    assertThat(request.model()).isEqualTo(\"DEFAULT_MODEL\");\n    assertThat(request.temperature()).isEqualTo(66.6);\n\n    request = client.createRequest(new Prompt(\"Test message content\",\n            OpenAiChatOptions.builder().model(\"PROMPT_MODEL\").temperature(99.9).build()), true);\n\n    assertThat(request.messages()).hasSize(1);\n    assertThat(request.stream()).isTrue();\n\n    assertThat(request.model()).isEqualTo(\"PROMPT_MODEL\");\n    assertThat(request.temperature()).isEqualTo(99.9);\n}"}, {"turn_num": 2, "instruction": "Add retry logic around the API call using `this.retryTemplate.execute` to handle transient failures when streaming the audio response from the external API.", "solution": "@Override\npublic Flux<SpeechResponse> stream(SpeechPrompt speechPrompt) {\n    OpenAiAudioApi.SpeechRequest speechRequest = createRequest(speechPrompt);\n    \n    Flux<ResponseEntity<byte[]>> speechEntity = this.retryTemplate\n        .execute(ctx -> this.audioApi.stream(speechRequest));\n    \n    return speechEntity.map(entity -> new SpeechResponse(new Speech(entity.getBody()),\n            new OpenAiAudioSpeechResponseMetadata(OpenAiResponseHeaderExtractor.extractAiResponseHeaders(entity))));\n}", "test": "@Test\nvoid createRequestWithChatOptions() {\n    var client = OpenAiChatModel.builder()\n        .openAiApi(OpenAiApi.builder().apiKey(\"TEST\").build())\n        .defaultOptions(OpenAiChatOptions.builder().model(\"DEFAULT_MODEL\").temperature(66.6).build())\n        .build();\n\n    var prompt = client.buildRequestPrompt(new Prompt(\"Test message content\"));\n\n    var request = client.createRequest(prompt, false);\n\n    assertThat(request.messages()).hasSize(1);\n    assertThat(request.stream()).isFalse();\n\n    assertThat(request.model()).isEqualTo(\"DEFAULT_MODEL\");\n    assertThat(request.temperature()).isEqualTo(66.6);\n\n    request = client.createRequest(new Prompt(\"Test message content\",\n            OpenAiChatOptions.builder().model(\"PROMPT_MODEL\").temperature(99.9).build()), true);\n\n    assertThat(request.messages()).hasSize(1);\n    assertThat(request.stream()).isTrue();\n\n    assertThat(request.model()).isEqualTo(\"PROMPT_MODEL\");\n    assertThat(request.temperature()).isEqualTo(99.9);\n}"}, {"turn_num": 3, "instruction": "Include metadata extraction from the API response headers by using `OpenAiResponseHeaderExtractor.extractAiResponseHeaders` and pass the result to a new `OpenAiAudioSpeechResponseMetadata` object inside the `SpeechResponse`. Ensure the `SpeechResponse` constructor receives both the `Speech` object containing the audio data and the metadata object; do not omit any components in the response construction.", "solution": "@Override\npublic Flux<SpeechResponse> stream(SpeechPrompt speechPrompt) {\n\n\tOpenAiAudioApi.SpeechRequest speechRequest = createRequest(speechPrompt);\n\n\tFlux<ResponseEntity<byte[]>> speechEntity = this.retryTemplate\n\t\t.execute(ctx -> this.audioApi.stream(speechRequest));\n\n\treturn speechEntity.map(entity -> new SpeechResponse(new Speech(entity.getBody()),\n\t\t\tnew OpenAiAudioSpeechResponseMetadata(OpenAiResponseHeaderExtractor.extractAiResponseHeaders(entity))));\n}", "test": "@Test\nvoid createRequestWithChatOptions() {\n\tvar client = OpenAiChatModel.builder()\n\t\t.openAiApi(OpenAiApi.builder().apiKey(\"TEST\").build())\n\t\t.defaultOptions(OpenAiChatOptions.builder().model(\"DEFAULT_MODEL\").temperature(66.6).build())\n\t\t.build();\n\n\tvar prompt = client.buildRequestPrompt(new Prompt(\"Test message content\"));\n\n\tvar request = client.createRequest(prompt, false);\n\n\tassertThat(request.messages()).hasSize(1);\n\tassertThat(request.stream()).isFalse();\n\n\tassertThat(request.model()).isEqualTo(\"DEFAULT_MODEL\");\n\tassertThat(request.temperature()).isEqualTo(66.6);\n\n\trequest = client.createRequest(new Prompt(\"Test message content\",\n\t\t\tOpenAiChatOptions.builder().model(\"PROMPT_MODEL\").temperature(99.9).build()), true);\n\n\tassertThat(request.messages()).hasSize(1);\n\tassertThat(request.stream()).isTrue();\n\n\tassertThat(request.model()).isEqualTo(\"PROMPT_MODEL\");\n\tassertThat(request.temperature()).isEqualTo(99.9);\n}"}], "metadata": {"source": "mrgbench", "func": "\t@Override\n\tpublic Flux<SpeechResponse> stream(SpeechPrompt speechPrompt) {\n\n\t\tOpenAiAudioApi.SpeechRequest speechRequest = createRequest(speechPrompt);\n\n\t\tFlux<ResponseEntity<byte[]>> speechEntity = this.retryTemplate\n\t\t\t.execute(ctx -> this.audioApi.stream(speechRequest));\n\n\t\treturn speechEntity.map(entity -> new SpeechResponse(new Speech(entity.getBody()),\n\t\t\t\tnew OpenAiAudioSpeechResponseMetadata(OpenAiResponseHeaderExtractor.extractAiResponseHeaders(entity))));\n\t}", "repo": "spring-ai", "file_path": "models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiAudioSpeechModel.java", "func_name": "stream", "context": "/*\n * Copyright 2023-2025 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.openai;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport reactor.core.publisher.Flux;\n\nimport org.springframework.ai.chat.metadata.RateLimit;\nimport org.springframework.ai.openai.api.OpenAiAudioApi;\nimport org.springframework.ai.openai.api.OpenAiAudioApi.SpeechRequest.AudioResponseFormat;\nimport org.springframework.ai.openai.audio.speech.Speech;\nimport org.springframework.ai.openai.audio.speech.SpeechModel;\nimport org.springframework.ai.openai.audio.speech.SpeechPrompt;\nimport org.springframework.ai.openai.audio.speech.SpeechResponse;\nimport org.springframework.ai.openai.audio.speech.StreamingSpeechModel;\nimport org.springframework.ai.openai.metadata.audio.OpenAiAudioSpeechResponseMetadata;\nimport org.springframework.ai.openai.metadata.support.OpenAiResponseHeaderExtractor;\nimport org.springframework.ai.retry.RetryUtils;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.retry.support.RetryTemplate;\nimport org.springframework.util.Assert;\nimport org.springframework.util.StringUtils;\n\n/**\n * OpenAI audio speech client implementation for backed by {@link OpenAiAudioApi}.\n *\n * @author Ahmed Yousri\n * @author Hyunjoon Choi\n * @author Thomas Vitale\n * @author Jonghoon Park\n * @see OpenAiAudioApi\n * @since 1.0.0-M1\n */\npublic class OpenAiAudioSpeechModel implements SpeechModel, StreamingSpeechModel {\n\n\t/**\n\t * The speed of the default voice synthesis.\n\t * @see OpenAiAudioSpeechOptions\n\t */\n\tprivate static final Float SPEED = 1.0f;\n\n\tprivate final Logger logger = LoggerFactory.getLogger(getClass());\n\n\t/**\n\t * The default options used for the audio completion requests.\n\t */\n\tprivate final OpenAiAudioSpeechOptions defaultOptions;\n\n\t/**\n\t * The retry template used to retry the OpenAI Audio API calls.\n\t */\n\tprivate final RetryTemplate retryTemplate;\n\n\t/**\n\t * Low-level access to the OpenAI Audio API.\n\t */\n\tprivate final OpenAiAudioApi audioApi;\n\n\t/**\n\t * Initializes a new instance of the OpenAiAudioSpeechModel class with the provided\n\t * OpenAiAudioApi. It uses the model tts-1, response format mp3, voice alloy, and the\n\t * default speed of 1.0.\n\t * @param audioApi The OpenAiAudioApi to use for speech synthesis.\n\t */\n\tpublic OpenAiAudioSpeechModel(OpenAiAudioApi audioApi) {\n\t\tthis(audioApi,\n\t\t\t\tOpenAiAudioSpeechOptions.builder()\n\t\t\t\t\t.model(OpenAiAudioApi.TtsModel.TTS_1.getValue())\n\t\t\t\t\t.responseFormat(AudioResponseFormat.MP3)\n\t\t\t\t\t.voice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY.getValue())\n\t\t\t\t\t.speed(SPEED)\n\t\t\t\t\t.build());\n\t}\n\n\t/**\n\t * Initializes a new instance of the OpenAiAudioSpeechModel class with the provided\n\t * OpenAiAudioApi and options.\n\t * @param audioApi The OpenAiAudioApi to use for speech synthesis.\n\t * @param options The OpenAiAudioSpeechOptions containing the speech synthesis\n\t * options.\n\t */\n\tpublic OpenAiAudioSpeechModel(OpenAiAudioApi audioApi, OpenAiAudioSpeechOptions options) {\n\t\tthis(audioApi, options, RetryUtils.DEFAULT_RETRY_TEMPLATE);\n\t}\n\n\t/**\n\t * Initializes a new instance of the OpenAiAudioSpeechModel class with the provided\n\t * OpenAiAudioApi and options.\n\t * @param audioApi The OpenAiAudioApi to use for speech synthesis.\n\t * @param options The OpenAiAudioSpeechOptions containing the speech synthesis\n\t * options.\n\t * @param retryTemplate The retry template.\n\t */\n\tpublic OpenAiAudioSpeechModel(OpenAiAudioApi audioApi, OpenAiAudioSpeechOptions options,\n\t\t\tRetryTemplate retryTemplate) {\n\t\tAssert.notNull(audioApi, \"OpenAiAudioApi must not be null\");\n\t\tAssert.notNull(options, \"OpenAiSpeechOptions must not be null\");\n\t\tAssert.notNull(options, \"RetryTemplate must not be null\");\n\t\tthis.audioApi = audioApi;\n\t\tthis.defaultOptions = options;\n\t\tthis.retryTemplate = retryTemplate;\n\t}\n\n\t@Override\n\tpublic byte[] call(String text) {\n\t\tSpeechPrompt speechRequest = new SpeechPrompt(text);\n\t\treturn call(speechRequest).getResult().getOutput();\n\t}\n\n\t@Override\n\tpublic SpeechResponse call(SpeechPrompt speechPrompt) {\n\n\t\tOpenAiAudioApi.SpeechRequest speechRequest = createRequest(speechPrompt);\n\n\t\tResponseEntity<byte[]> speechEntity = this.retryTemplate\n\t\t\t.execute(ctx -> this.audioApi.createSpeech(speechRequest));\n\n\t\tvar speech = speechEntity.getBody();\n\n\t\tif (speech == null) {\n\t\t\tlogger.warn(\"No speech response returned for speechRequest: {}\", speechRequest);\n\t\t\treturn new SpeechResponse(new Speech(new byte[0]));\n\t\t}\n\n\t\tRateLimit rateLimits = OpenAiResponseHeaderExtractor.extractAiResponseHeaders(speechEntity);\n\n\t\treturn new SpeechResponse(new Speech(speech), new OpenAiAudioSpeechResponseMetadata(rateLimits));\n\t}", "func_start": 152, "func_end": 162, "body_len": 10, "test_file": "models/spring-ai-openai/src/test/java/org/springframework/ai/openai/ChatCompletionRequestTests.java", "test_start": 76, "test_end": 101, "test_code": "\t@Test\n\tvoid createRequestWithChatOptions() {\n\t\tvar client = OpenAiChatModel.builder()\n\t\t\t.openAiApi(OpenAiApi.builder().apiKey(\"TEST\").build())\n\t\t\t.defaultOptions(OpenAiChatOptions.builder().model(\"DEFAULT_MODEL\").temperature(66.6).build())\n\t\t\t.build();\n\n\t\tvar prompt = client.buildRequestPrompt(new Prompt(\"Test message content\"));\n\n\t\tvar request = client.createRequest(prompt, false);\n\n\t\tassertThat(request.messages()).hasSize(1);\n\t\tassertThat(request.stream()).isFalse();\n\n\t\tassertThat(request.model()).isEqualTo(\"DEFAULT_MODEL\");\n\t\tassertThat(request.temperature()).isEqualTo(66.6);\n\n\t\trequest = client.createRequest(new Prompt(\"Test message content\",\n\t\t\t\tOpenAiChatOptions.builder().model(\"PROMPT_MODEL\").temperature(99.9).build()), true);\n\n\t\tassertThat(request.messages()).hasSize(1);\n\t\tassertThat(request.stream()).isTrue();\n\n\t\tassertThat(request.model()).isEqualTo(\"PROMPT_MODEL\");\n\t\tassertThat(request.temperature()).isEqualTo(99.9);\n\t}", "test_instruction": "mvn test -pl models/spring-ai-openai -Dtest=\"org.springframework.ai.openai.ChatCompletionRequestTests#createRequestWithChatOptions\"", "language": "java"}, "mt_id": 1031}
{"hash_id": "9c74edf900020dff2741ee21", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `mergeOptions(EmbeddingOptions requestOptions)` that returns a `PostgresMlEmbeddingOptions` object by merging the provided request options with default options. If no request options are given, return a new instance of `PostgresMlEmbeddingOptions` with default values. You should write self-contained code starting with: ```java\nPostgresMlEmbeddingOptions mergeOptions(EmbeddingOptions requestOptions) {\n```", "solution": "PostgresMlEmbeddingOptions mergeOptions(EmbeddingOptions requestOptions) {\n    PostgresMlEmbeddingOptions options = (this.defaultOptions != null) ? this.defaultOptions\n            : PostgresMlEmbeddingOptions.builder().build();\n\n    if (requestOptions != null) {\n        options = ModelOptionsUtils.merge(requestOptions, options, PostgresMlEmbeddingOptions.class);\n    }\n\n    return options;\n}", "test": "@Test\npublic void mergeOptions() {\n\n    var jdbcTemplate = Mockito.mock(JdbcTemplate.class);\n    PostgresMlEmbeddingModel embeddingModel = new PostgresMlEmbeddingModel(jdbcTemplate);\n\n    PostgresMlEmbeddingOptions options = embeddingModel.mergeOptions(EmbeddingOptionsBuilder.builder().build());\n\n    // Default options\n    assertThat(options.getTransformer()).isEqualTo(PostgresMlEmbeddingModel.DEFAULT_TRANSFORMER_MODEL);\n    assertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY);\n    assertThat(options.getKwargs()).isEqualTo(Map.of());\n    assertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.EMBED);\n\n    // Partial override\n    options = embeddingModel.mergeOptions(PostgresMlEmbeddingOptions.builder()\n        .transformer(\"intfloat/e5-small\")\n        .kwargs(Map.of(\"device\", \"cpu\"))\n        .build());\n\n    assertThat(options.getTransformer()).isEqualTo(\"intfloat/e5-small\");\n    assertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY); // Default\n    assertThat(options.getKwargs()).isEqualTo(Map.of(\"device\", \"cpu\"));\n    assertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.EMBED); // Default\n\n    // Complete override\n    options = embeddingModel.mergeOptions(PostgresMlEmbeddingOptions.builder()\n        .transformer(\"intfloat/e5-small\")\n        .vectorType(PostgresMlEmbeddingModel.VectorType.PG_VECTOR)\n        .metadataMode(org.springframework.ai.document.MetadataMode.ALL)\n        .kwargs(Map.of(\"device\", \"cpu\"))\n        .build());\n\n    assertThat(options.getTransformer()).isEqualTo(\"intfloat/e5-small\");\n    assertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_VECTOR);\n    assertThat(options.getKwargs()).isEqualTo(Map.of(\"device\", \"cpu\"));\n    assertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.ALL);\n}"}, {"turn_num": 2, "instruction": "Modify the function to use `this.defaultOptions` as the base if available; otherwise, use a newly created `PostgresMlEmbeddingOptions` instance with default values.", "solution": "PostgresMlEmbeddingOptions mergeOptions(EmbeddingOptions requestOptions) {\n\n\tPostgresMlEmbeddingOptions options = (this.defaultOptions != null) ? this.defaultOptions\n\t\t\t: PostgresMlEmbeddingOptions.builder().build();\n\n\tif (requestOptions != null) {\n\t\toptions = ModelOptionsUtils.merge(requestOptions, options, PostgresMlEmbeddingOptions.class);\n\t}\n\n\treturn options;\n}", "test": "@Test\npublic void mergeOptions() {\n\n\tvar jdbcTemplate = Mockito.mock(JdbcTemplate.class);\n\tPostgresMlEmbeddingModel embeddingModel = new PostgresMlEmbeddingModel(jdbcTemplate);\n\n\t// Test with EmbeddingOptionsBuilder to justify the import\n\tPostgresMlEmbeddingOptions options = embeddingModel.mergeOptions(EmbeddingOptionsBuilder.builder().build());\n\n\t// Verify default options when this.defaultOptions is null\n\tassertThat(options.getTransformer()).isEqualTo(PostgresMlEmbeddingModel.DEFAULT_TRANSFORMER_MODEL);\n\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY);\n\tassertThat(options.getKwargs()).isEqualTo(Map.of());\n\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.EMBED);\n\n\t// Test that it uses newly created PostgresMlEmbeddingOptions instance with defaults\n\toptions = embeddingModel.mergeOptions(null);\n\tassertThat(options.getTransformer()).isEqualTo(PostgresMlEmbeddingModel.DEFAULT_TRANSFORMER_MODEL);\n\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY);\n}"}, {"turn_num": 3, "instruction": "Ensure that if `requestOptions` is not null, it is merged with the base options using `ModelOptionsUtils.merge(requestOptions, options, PostgresMlEmbeddingOptions.class)` to produce the final result. Guarantee type safety by ensuring the merge operation explicitly specifies `PostgresMlEmbeddingOptions.class` as the target type for merging.", "solution": "PostgresMlEmbeddingOptions mergeOptions(EmbeddingOptions requestOptions) {\n\n\tPostgresMlEmbeddingOptions options = (this.defaultOptions != null) ? this.defaultOptions\n\t\t\t: PostgresMlEmbeddingOptions.builder().build();\n\n\tif (requestOptions != null) {\n\t\toptions = ModelOptionsUtils.merge(requestOptions, options, PostgresMlEmbeddingOptions.class);\n\t}\n\n\treturn options;\n}", "test": "@Test\npublic void mergeOptions() {\n\n\tvar jdbcTemplate = Mockito.mock(JdbcTemplate.class);\n\tPostgresMlEmbeddingModel embeddingModel = new PostgresMlEmbeddingModel(jdbcTemplate);\n\n\tPostgresMlEmbeddingOptions options = embeddingModel.mergeOptions(EmbeddingOptionsBuilder.builder().build());\n\n\t// Default options\n\tassertThat(options.getTransformer()).isEqualTo(PostgresMlEmbeddingModel.DEFAULT_TRANSFORMER_MODEL);\n\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY);\n\tassertThat(options.getKwargs()).isEqualTo(Map.of());\n\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.EMBED);\n\n\t// Partial override\n\toptions = embeddingModel.mergeOptions(PostgresMlEmbeddingOptions.builder()\n\t\t.transformer(\"intfloat/e5-small\")\n\t\t.kwargs(Map.of(\"device\", \"cpu\"))\n\t\t.build());\n\n\tassertThat(options.getTransformer()).isEqualTo(\"intfloat/e5-small\");\n\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY); // Default\n\tassertThat(options.getKwargs()).isEqualTo(Map.of(\"device\", \"cpu\"));\n\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.EMBED); // Default\n\n\t// Complete override\n\toptions = embeddingModel.mergeOptions(PostgresMlEmbeddingOptions.builder()\n\t\t.transformer(\"intfloat/e5-small\")\n\t\t.vectorType(PostgresMlEmbeddingModel.VectorType.PG_VECTOR)\n\t\t.metadataMode(org.springframework.ai.document.MetadataMode.ALL)\n\t\t.kwargs(Map.of(\"device\", \"cpu\"))\n\t\t.build());\n\n\tassertThat(options.getTransformer()).isEqualTo(\"intfloat/e5-small\");\n\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_VECTOR);\n\tassertThat(options.getKwargs()).isEqualTo(Map.of(\"device\", \"cpu\"));\n\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.ALL);\n}"}], "metadata": {"source": "mrgbench", "func": "\tPostgresMlEmbeddingOptions mergeOptions(EmbeddingOptions requestOptions) {\n\n\t\tPostgresMlEmbeddingOptions options = (this.defaultOptions != null) ? this.defaultOptions\n\t\t\t\t: PostgresMlEmbeddingOptions.builder().build();\n\n\t\tif (requestOptions != null) {\n\t\t\toptions = ModelOptionsUtils.merge(requestOptions, options, PostgresMlEmbeddingOptions.class);\n\t\t}\n\n\t\treturn options;\n\t}", "repo": "spring-ai", "file_path": "models/spring-ai-postgresml/src/main/java/org/springframework/ai/postgresml/PostgresMlEmbeddingModel.java", "func_name": "mergeOptions", "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.postgresml;\n\nimport java.sql.Array;\nimport java.sql.PreparedStatement;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Map;\n\nimport org.springframework.ai.chat.metadata.EmptyUsage;\nimport org.springframework.ai.document.Document;\nimport org.springframework.ai.embedding.AbstractEmbeddingModel;\nimport org.springframework.ai.embedding.Embedding;\nimport org.springframework.ai.embedding.EmbeddingOptions;\nimport org.springframework.ai.embedding.EmbeddingRequest;\nimport org.springframework.ai.embedding.EmbeddingResponse;\nimport org.springframework.ai.embedding.EmbeddingResponseMetadata;\nimport org.springframework.ai.model.EmbeddingUtils;\nimport org.springframework.ai.model.ModelOptionsUtils;\nimport org.springframework.beans.factory.InitializingBean;\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport org.springframework.jdbc.core.RowMapper;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.StringUtils;\n\n/**\n * <a href=\"https://postgresml.org\">PostgresML</a> EmbeddingModel\n *\n * @author Toshiaki Maki\n * @author Christian Tzolov\n */\npublic class PostgresMlEmbeddingModel extends AbstractEmbeddingModel implements InitializingBean {\n\n\tpublic static final String DEFAULT_TRANSFORMER_MODEL = \"distilbert-base-uncased\";\n\n\tprivate final PostgresMlEmbeddingOptions defaultOptions;\n\n\tprivate final JdbcTemplate jdbcTemplate;\n\n\tprivate final boolean createExtension;\n\n\t/**\n\t * a constructor\n\t * @param jdbcTemplate JdbcTemplate\n\t */\n\tpublic PostgresMlEmbeddingModel(JdbcTemplate jdbcTemplate) {\n\t\tthis(jdbcTemplate, PostgresMlEmbeddingOptions.builder().build(), false);\n\t}\n\n\tpublic PostgresMlEmbeddingModel(JdbcTemplate jdbcTemplate, PostgresMlEmbeddingOptions options) {\n\t\tthis(jdbcTemplate, options, false);\n\t}\n\n\t/**\n\t * a PostgresMlEmbeddingModel constructor\n\t * @param jdbcTemplate JdbcTemplate to use to interact with the database.\n\t * @param options PostgresMlEmbeddingOptions to configure the client.\n\t */\n\tpublic PostgresMlEmbeddingModel(JdbcTemplate jdbcTemplate, PostgresMlEmbeddingOptions options,\n\t\t\tboolean createExtension) {\n\t\tAssert.notNull(jdbcTemplate, \"jdbc template must not be null.\");\n\t\tAssert.notNull(options, \"options must not be null.\");\n\t\tAssert.notNull(options.getTransformer(), \"transformer must not be null.\");\n\t\tAssert.notNull(options.getVectorType(), \"vectorType must not be null.\");\n\t\tAssert.notNull(options.getKwargs(), \"kwargs must not be null.\");\n\t\tAssert.notNull(options.getMetadataMode(), \"metadataMode must not be null.\");\n\n\t\tthis.jdbcTemplate = jdbcTemplate;\n\t\tthis.defaultOptions = options;\n\t\tthis.createExtension = createExtension;\n\t}\n\n\t@SuppressWarnings(\"null\")\n\t@Override\n\tpublic float[] embed(String text) {\n\t\treturn this.jdbcTemplate.queryForObject(\n\t\t\t\t\"SELECT pgml.embed(?, ?, ?::JSONB)\" + this.defaultOptions.getVectorType().cast + \" AS embedding\",\n\t\t\t\tthis.defaultOptions.getVectorType().rowMapper, this.defaultOptions.getTransformer(), text,\n\t\t\t\tModelOptionsUtils.toJsonString(this.defaultOptions.getKwargs()));\n\t}\n\n\t@Override\n\tpublic float[] embed(Document document) {\n\t\treturn this.embed(document.getFormattedContent(this.defaultOptions.getMetadataMode()));\n\t}\n\n\t@SuppressWarnings(\"null\")\n\t@Override\n\tpublic EmbeddingResponse call(EmbeddingRequest request) {\n\n\t\tfinal PostgresMlEmbeddingOptions optionsToUse = this.mergeOptions(request.getOptions());\n\n\t\tList<Embedding> data = new ArrayList<>();\n\t\tList<float[]> embed = List.of();\n\n\t\tList<String> texts = request.getInstructions();\n\t\tif (!CollectionUtils.isEmpty(texts)) {\n\t\t\tembed = this.jdbcTemplate.query(connection -> {\n\t\t\t\tPreparedStatement preparedStatement = connection.prepareStatement(\"SELECT pgml.embed(?, text, ?::JSONB)\"\n\t\t\t\t\t\t+ optionsToUse.getVectorType().cast + \" AS embedding FROM (SELECT unnest(?) AS text) AS texts\");\n\t\t\t\tpreparedStatement.setString(1, optionsToUse.getTransformer());\n\t\t\t\tpreparedStatement.setString(2, ModelOptionsUtils.toJsonString(optionsToUse.getKwargs()));\n\t\t\t\tpreparedStatement.setArray(3, connection.createArrayOf(\"TEXT\", texts.toArray(Object[]::new)));\n\t\t\t\treturn preparedStatement;\n\t\t\t}, rs -> {\n\t\t\t\tList<float[]> result = new ArrayList<>();\n\t\t\t\twhile (rs.next()) {\n\t\t\t\t\tresult.add(optionsToUse.getVectorType().rowMapper.mapRow(rs, -1));\n\t\t\t\t}\n\t\t\t\treturn result;\n\t\t\t});\n\t\t}\n\n\t\tif (!CollectionUtils.isEmpty(embed)) {\n\t\t\tfor (int i = 0; i < embed.size(); i++) {\n\t\t\t\tdata.add(new Embedding(embed.get(i), i));\n\t\t\t}\n\t\t}\n\n\t\tMap<String, Object> embeddingMetadata = Map.of(\"transformer\", optionsToUse.getTransformer(), \"vector-type\",\n\t\t\t\toptionsToUse.getVectorType().name(), \"kwargs\",\n\t\t\t\tModelOptionsUtils.toJsonString(optionsToUse.getKwargs()));\n\t\tvar embeddingResponseMetadata = new EmbeddingResponseMetadata(\"unknown\", new EmptyUsage(), embeddingMetadata);\n\t\treturn new EmbeddingResponse(data, embeddingResponseMetadata);\n\t}", "func_start": 149, "func_end": 159, "body_len": 10, "test_file": "models/spring-ai-postgresml/src/test/java/org/springframework/ai/postgresml/PostgresMlEmbeddingOptionsTests.java", "test_start": 59, "test_end": 96, "test_code": "\t@Test\n\tpublic void mergeOptions() {\n\n\t\tvar jdbcTemplate = Mockito.mock(JdbcTemplate.class);\n\t\tPostgresMlEmbeddingModel embeddingModel = new PostgresMlEmbeddingModel(jdbcTemplate);\n\n\t\tPostgresMlEmbeddingOptions options = embeddingModel.mergeOptions(EmbeddingOptionsBuilder.builder().build());\n\n\t\t// Default options\n\t\tassertThat(options.getTransformer()).isEqualTo(PostgresMlEmbeddingModel.DEFAULT_TRANSFORMER_MODEL);\n\t\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY);\n\t\tassertThat(options.getKwargs()).isEqualTo(Map.of());\n\t\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.EMBED);\n\n\t\t// Partial override\n\t\toptions = embeddingModel.mergeOptions(PostgresMlEmbeddingOptions.builder()\n\t\t\t.transformer(\"intfloat/e5-small\")\n\t\t\t.kwargs(Map.of(\"device\", \"cpu\"))\n\t\t\t.build());\n\n\t\tassertThat(options.getTransformer()).isEqualTo(\"intfloat/e5-small\");\n\t\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY); // Default\n\t\tassertThat(options.getKwargs()).isEqualTo(Map.of(\"device\", \"cpu\"));\n\t\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.EMBED); // Default\n\n\t\t// Complete override\n\t\toptions = embeddingModel.mergeOptions(PostgresMlEmbeddingOptions.builder()\n\t\t\t.transformer(\"intfloat/e5-small\")\n\t\t\t.vectorType(PostgresMlEmbeddingModel.VectorType.PG_VECTOR)\n\t\t\t.metadataMode(org.springframework.ai.document.MetadataMode.ALL)\n\t\t\t.kwargs(Map.of(\"device\", \"cpu\"))\n\t\t\t.build());\n\n\t\tassertThat(options.getTransformer()).isEqualTo(\"intfloat/e5-small\");\n\t\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_VECTOR);\n\t\tassertThat(options.getKwargs()).isEqualTo(Map.of(\"device\", \"cpu\"));\n\t\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.ALL);\n\t}", "test_instruction": "mvn test -pl models/spring-ai-postgresml -Dtest=\"org.springframework.ai.postgresml.PostgresMlEmbeddingOptionsTests#mergeOptions\"", "language": "java"}, "mt_id": 1032}
{"hash_id": "9a05051ce9e275695f4b49bf", "total_turn": 2, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `dimensions` that takes an `EmbeddingModel`, a model name as a string, and dummy content as a string, and returns the dimension size of the embedding for that model by generating an embedding from the dummy content using the provided `EmbeddingModel` and returning the length of the resulting embedding array. You should write self-contained code starting with: ```java\npublic static int dimensions(EmbeddingModel embeddingModel, String modelName, String dummyContent) {```", "solution": "public static int dimensions(EmbeddingModel embeddingModel, String modelName, String dummyContent) {\n    return embeddingModel.embed(dummyContent).length;\n}", "test": "@Test\npublic void testDefaultMethodImplementation() {\n    EmbeddingModel dummy = new EmbeddingModel() {\n        @Override\n        public float[] embed(String text) {\n            return new float[] { 0.1f, 0.1f, 0.1f };\n        }\n\n        @Override\n        public float[] embed(Document document) {\n            throw new UnsupportedOperationException(\"Unimplemented method 'embed'\");\n        }\n\n        @Override\n        public List<float[]> embed(List<String> texts) {\n            throw new UnsupportedOperationException(\"Unimplemented method 'embed'\");\n        }\n\n        @Override\n        public EmbeddingResponse embedForResponse(List<String> texts) {\n            throw new UnsupportedOperationException(\"Unimplemented method 'embedForResponse'\");\n        }\n\n        @Override\n        public EmbeddingResponse call(EmbeddingRequest request) {\n            throw new UnsupportedOperationException(\"Unimplemented method 'call'\");\n        }\n    };\n\n    assertThat(dummy.dimensions()).isEqualTo(3);\n}"}, {"turn_num": 2, "instruction": "Before generating an embedding, check if the model name exists in a predefined static map named KNOWN_EMBEDDING_DIMENSIONS. If it does, return the corresponding dimension value directly without invoking the embedding model.", "solution": "public static int dimensions(EmbeddingModel embeddingModel, String modelName, String dummyContent) {\n\n\tif (KNOWN_EMBEDDING_DIMENSIONS.containsKey(modelName)) {\n\t\t// Retrieve the dimension from a pre-configured file.\n\t\treturn KNOWN_EMBEDDING_DIMENSIONS.get(modelName);\n\t}\n\telse {\n\t\t// Determine the dimensions empirically.\n\t\t// Generate an embedding and count the dimension size;\n\t\treturn embeddingModel.embed(dummyContent).length;\n\t}\n}", "test": "@Test\npublic void testDefaultMethodImplementation() {\n\n\tEmbeddingModel dummy = new EmbeddingModel() {\n\n\t\t@Override\n\t\tpublic float[] embed(String text) {\n\t\t\treturn new float[] { 0.1f, 0.1f, 0.1f };\n\t\t}\n\n\t\t@Override\n\t\tpublic float[] embed(Document document) {\n\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'embed'\");\n\t\t}\n\n\t\t@Override\n\t\tpublic List<float[]> embed(List<String> texts) {\n\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'embed'\");\n\t\t}\n\n\t\t@Override\n\t\tpublic EmbeddingResponse embedForResponse(List<String> texts) {\n\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'embedForResponse'\");\n\t\t}\n\n\t\t@Override\n\t\tpublic EmbeddingResponse call(EmbeddingRequest request) {\n\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'call'\");\n\t\t}\n\t};\n\n\t// Test that the dimensions method works - it should return 3 from the embed method\n\tassertThat(dummy.dimensions()).isEqualTo(3);\n}"}], "metadata": {"source": "mrgbench", "func": "\tpublic static int dimensions(EmbeddingModel embeddingModel, String modelName, String dummyContent) {\n\n\t\tif (KNOWN_EMBEDDING_DIMENSIONS.containsKey(modelName)) {\n\t\t\t// Retrieve the dimension from a pre-configured file.\n\t\t\treturn KNOWN_EMBEDDING_DIMENSIONS.get(modelName);\n\t\t}\n\t\telse {\n\t\t\t// Determine the dimensions empirically.\n\t\t\t// Generate an embedding and count the dimension size;\n\t\t\treturn embeddingModel.embed(dummyContent).length;\n\t\t}\n\t}", "repo": "spring-ai", "file_path": "spring-ai-model/src/main/java/org/springframework/ai/embedding/AbstractEmbeddingModel.java", "func_name": "dimensions", "context": "/*\n * Copyright 2023-2025 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.embedding;\n\nimport java.io.IOException;\nimport java.util.Map;\nimport java.util.Properties;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.stream.Collectors;\n\nimport org.springframework.aot.hint.RuntimeHints;\nimport org.springframework.aot.hint.RuntimeHintsRegistrar;\nimport org.springframework.context.annotation.ImportRuntimeHints;\nimport org.springframework.core.io.ClassPathResource;\nimport org.springframework.core.io.Resource;\nimport org.springframework.util.Assert;\n\n/**\n * Abstract implementation of the {@link EmbeddingModel} interface that provides\n * dimensions calculation caching.\n *\n * @author Christian Tzolov\n * @author Josh Long\n */\n@ImportRuntimeHints(AbstractEmbeddingModel.Hints.class)\npublic abstract class AbstractEmbeddingModel implements EmbeddingModel {\n\n\tprivate static final Resource EMBEDDING_MODEL_DIMENSIONS_PROPERTIES = new ClassPathResource(\n\t\t\t\"/embedding/embedding-model-dimensions.properties\");\n\n\tprivate static final Map<String, Integer> KNOWN_EMBEDDING_DIMENSIONS = loadKnownModelDimensions();\n\n\t/**\n\t * Cached embedding dimensions.\n\t */\n\tprotected final AtomicInteger embeddingDimensions = new AtomicInteger(-1);", "func_start": 61, "func_end": 72, "body_len": 11, "test_file": "spring-ai-model/src/test/java/org/springframework/ai/embedding/AbstractEmbeddingModelTests.java", "test_start": 46, "test_end": 78, "test_code": "\t@Test\n\tpublic void testDefaultMethodImplementation() {\n\n\t\tEmbeddingModel dummy = new EmbeddingModel() {\n\n\t\t\t@Override\n\t\t\tpublic float[] embed(String text) {\n\t\t\t\treturn new float[] { 0.1f, 0.1f, 0.1f };\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic float[] embed(Document document) {\n\t\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'embed'\");\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic List<float[]> embed(List<String> texts) {\n\t\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'embed'\");\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic EmbeddingResponse embedForResponse(List<String> texts) {\n\t\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'embedForResponse'\");\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic EmbeddingResponse call(EmbeddingRequest request) {\n\t\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'call'\");\n\t\t\t}\n\t\t};\n\n\t\tassertThat(dummy.dimensions()).isEqualTo(3);\n\t}", "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.embedding.AbstractEmbeddingModelTests#testDefaultMethodImplementation\"", "language": "java"}, "mt_id": 1033}
{"hash_id": "0fa49588ebdc16e8fd289f95", "total_turn": 2, "turn_datas": [{"turn_num": 1, "instruction": "Write a generic static method `merge` that takes a source object, a target object, a class type and an optional list of accepted field name.\nThe method should merge the non-null properties from the source object into the target object using JSON property names (as defined by `@JsonProperty`) to match fields. If this list is provided, only fields whose JSON property names are in this list should be considered during the merge. Otherwise, use the `@JsonProperty` annotations from the given class to determine the valid fields.\nThe result should be returned as an instance of the specified class, constructed from the merged data. Use internal utility methods to convert objects to maps, perform the merge, and map back to the target class. You should write self-contained code starting with: ```java\npublic static <T> T merge(Object source, Object target, Class<T> clazz,List<String>acceptedFieldNames){```", "solution": "public static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n    if (source == null) {\n        source = Map.of();\n    }\n\n    List<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n            ? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n            : acceptedFieldNames;\n\n    if (CollectionUtils.isEmpty(requestFieldNames)) {\n        throw new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n    }\n\n    Map<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n    Map<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n    // Merge non-null properties from source into target\n    targetMap.putAll(sourceMap.entrySet()\n        .stream()\n        .filter(e -> e.getValue() != null)\n        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n    // Filter to only accepted fields\n    targetMap = targetMap.entrySet()\n        .stream()\n        .filter(e -> requestFieldNames.contains(e.getKey()))\n        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n    return ModelOptionsUtils.mapToClass(targetMap, clazz);\n}", "test": "@Test\npublic void merge() {\n    TestPortableOptionsImpl portableOptions = new TestPortableOptionsImpl();\n    portableOptions.setName(\"John\");\n    portableOptions.setAge(30);\n    portableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n    TestSpecificOptions specificOptions = new TestSpecificOptions();\n    specificOptions.setName(\"Mike\");\n    specificOptions.setSpecificField(\"SpecificField\");\n\n    assertThatThrownBy(\n            () -> ModelOptionsUtils.merge(portableOptions, specificOptions, TestPortableOptionsImpl.class))\n        .isInstanceOf(IllegalArgumentException.class)\n        .hasMessageContaining(\"No @JsonProperty fields found in the \");\n\n    var specificOptions2 = ModelOptionsUtils.merge(portableOptions, specificOptions, TestSpecificOptions.class);\n\n    assertThat(specificOptions2.getAge()).isEqualTo(30);\n    assertThat(specificOptions2.getName()).isEqualTo(\"John\"); // Overridden by the portableOptions\n    assertThat(specificOptions2.getSpecificField()).isEqualTo(\"SpecificField\");\n}"}, {"turn_num": 2, "instruction": "Cache the list of JSON property names per class using a static map to avoid repeated reflection lookups. Use `computeIfAbsent` to ensure efficient and thread-safe population of the cache. If no valid JSON property fields are found on the class after processing (i.e., the resulting field list is empty), throw an `IllegalArgumentException` with a descriptive message indicating that the class has no `@JsonProperty` fields.", "solution": "public static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n    if (source == null) {\n        source = Map.of();\n    }\n\n    List<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n            ? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n            : acceptedFieldNames;\n\n    if (CollectionUtils.isEmpty(requestFieldNames)) {\n        throw new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n    }\n\n    Map<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n    Map<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n    // Merge non-null properties from source into target\n    targetMap.putAll(sourceMap.entrySet()\n        .stream()\n        .filter(e -> e.getValue() != null)\n        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n    // Filter to only accepted fields\n    targetMap = targetMap.entrySet()\n        .stream()\n        .filter(e -> requestFieldNames.contains(e.getKey()))\n        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n    return ModelOptionsUtils.mapToClass(targetMap, clazz);\n}", "test": "@Test\npublic void merge() {\n    TestPortableOptionsImpl portableOptions = new TestPortableOptionsImpl();\n    portableOptions.setName(\"John\");\n    portableOptions.setAge(30);\n    portableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n    TestSpecificOptions specificOptions = new TestSpecificOptions();\n    specificOptions.setName(\"Mike\");\n    specificOptions.setSpecificField(\"SpecificField\");\n\n    // Test IllegalArgumentException for class with no @JsonProperty fields\n    assertThatThrownBy(\n            () -> ModelOptionsUtils.merge(portableOptions, specificOptions, TestPortableOptionsImpl.class))\n        .isInstanceOf(IllegalArgumentException.class)\n        .hasMessageContaining(\"No @JsonProperty fields found in the \");\n\n    // Test successful merge with caching (multiple calls to verify cache usage)\n    var specificOptions2 = ModelOptionsUtils.merge(portableOptions, specificOptions, TestSpecificOptions.class);\n    assertThat(specificOptions2.getAge()).isEqualTo(30);\n    assertThat(specificOptions2.getName()).isEqualTo(\"John\");\n    assertThat(specificOptions2.getSpecificField()).isEqualTo(\"SpecificField\");\n\n    // Second call should use cached values efficiently\n    var specificOptions3 = ModelOptionsUtils.merge(portableOptions, specificOptions, TestSpecificOptions.class);\n    assertThat(specificOptions3.getName()).isEqualTo(\"John\");\n    assertThat(specificOptions3.getSpecificField()).isEqualTo(\"SpecificField\");\n}"}], "metadata": {"source": "mrgbench", "func": "\tpublic static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n\n\t\tif (source == null) {\n\t\t\tsource = Map.of();\n\t\t}\n\n\t\tList<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n\t\t\t\t? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n\t\t\t\t: acceptedFieldNames;\n\n\t\tif (CollectionUtils.isEmpty(requestFieldNames)) {\n\t\t\tthrow new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n\t\t}\n\n\t\tMap<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n\t\tMap<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n\t\ttargetMap.putAll(sourceMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n\t\ttargetMap = targetMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> requestFieldNames.contains(e.getKey()))\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n\t\treturn ModelOptionsUtils.mapToClass(targetMap, clazz);\n\t}", "repo": "spring-ai", "file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java", "func_name": "merge", "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.model;\n\nimport java.beans.PropertyDescriptor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Type;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.core.type.TypeReference;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport com.fasterxml.jackson.databind.cfg.CoercionAction;\nimport com.fasterxml.jackson.databind.cfg.CoercionInputShape;\nimport com.fasterxml.jackson.databind.json.JsonMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport com.github.victools.jsonschema.module.jackson.JacksonModule;\nimport com.github.victools.jsonschema.module.jackson.JacksonOption;\nimport com.github.victools.jsonschema.module.swagger2.Swagger2Module;\n\nimport org.springframework.ai.util.JacksonUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.core.KotlinDetector;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.ObjectUtils;\n\n/**\n * Utility class for manipulating {@link ModelOptions} objects.\n *\n * @author Christian Tzolov\n * @author Thomas Vitale\n * @since 0.8.0\n */\npublic abstract class ModelOptionsUtils {\n\n\tpublic static final ObjectMapper OBJECT_MAPPER = JsonMapper.builder()\n\t\t.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n\t\t.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS)\n\t\t.addModules(JacksonUtils.instantiateAvailableModules())\n\t\t.build()\n\t\t.configure(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT, true);\n\n\tstatic {\n\t\t// Configure coercion for empty strings to null for Enum types\n\t\t// This fixes the issue where empty string finish_reason values cause\n\t\t// deserialization failures\n\t\tOBJECT_MAPPER.coercionConfigFor(Enum.class).setCoercion(CoercionInputShape.EmptyString, CoercionAction.AsNull);\n\t}\n\n\tprivate static final List<String> BEAN_MERGE_FIELD_EXCISIONS = List.of(\"class\");\n\n\tprivate static final ConcurrentHashMap<Class<?>, List<String>> REQUEST_FIELD_NAMES_PER_CLASS = new ConcurrentHashMap<>();\n\n\tprivate static final AtomicReference<SchemaGenerator> SCHEMA_GENERATOR_CACHE = new AtomicReference<>();\n\n\tprivate static TypeReference<HashMap<String, Object>> MAP_TYPE_REF = new TypeReference<>() {\n\n\t};\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using the default\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json) {\n\t\treturn jsonToMap(json, OBJECT_MAPPER);\n\t}\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using a custom\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @param objectMapper the ObjectMapper to use for deserialization.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json, ObjectMapper objectMapper) {\n\t\ttry {\n\t\t\treturn objectMapper.readValue(json, MAP_TYPE_REF);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given JSON string to an Object of the given type.\n\t * @param <T> the type of the object to return.\n\t * @param json the JSON string to convert to an object.\n\t * @param type the type of the object to return.\n\t * @return Object instance of the given type.\n\t */\n\tpublic static <T> T jsonToObject(String json, Class<T> type) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.readValue(json, type);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Failed to json: \" + json, e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonString(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonStringPrettyPrinter(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writerWithDefaultPrettyPrinter().writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}", "func_start": 178, "func_end": 206, "body_len": 28, "test_file": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java", "test_start": 37, "test_end": 59, "test_code": "\t@Test\n\tpublic void merge() {\n\t\tTestPortableOptionsImpl portableOptions = new TestPortableOptionsImpl();\n\t\tportableOptions.setName(\"John\");\n\t\tportableOptions.setAge(30);\n\t\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\t\tTestSpecificOptions specificOptions = new TestSpecificOptions();\n\t\tspecificOptions.setName(\"Mike\");\n\t\tspecificOptions.setSpecificField(\"SpecificField\");\n\n\t\tassertThatThrownBy(\n\t\t\t\t() -> ModelOptionsUtils.merge(portableOptions, specificOptions, TestPortableOptionsImpl.class))\n\t\t\t.isInstanceOf(IllegalArgumentException.class)\n\t\t\t.hasMessageContaining(\"No @JsonProperty fields found in the \");\n\n\t\tvar specificOptions2 = ModelOptionsUtils.merge(portableOptions, specificOptions, TestSpecificOptions.class);\n\n\t\tassertThat(specificOptions2.getAge()).isEqualTo(30);\n\t\tassertThat(specificOptions2.getName()).isEqualTo(\"John\"); // !!! Overridden by the\n\t\t// portableOptions\n\t\tassertThat(specificOptions2.getSpecificField()).isEqualTo(\"SpecificField\");\n\t}", "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.model.ModelOptionsUtilsTests#merge\"", "language": "java"}, "mt_id": 1034}
{"hash_id": "6401c9b71d6a8ccb623af22c", "total_turn": 2, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `objectToMap` that takes an arbitrary Java object and converts it into a `Map<String, Object>` by serializing the object to JSON and then deserializing it into a map. You should write self-contained code starting with: ```java\npublic static Map<String, Object> objectToMap(Object source) {\n    // Your implementation here\n}\n```", "solution": "public static Map<String, Object> objectToMap(Object source) {\n    if (source == null) {\n        return new HashMap<>();\n    }\n    try {\n        String json = OBJECT_MAPPER.writeValueAsString(source);\n        return OBJECT_MAPPER.readValue(json, new TypeReference<Map<String, Object>>() {});\n    }\n    catch (JsonProcessingException e) {\n        throw new RuntimeException(e);\n    }\n}", "test": "@Test\npublic void objectToMap() {\n    TestPortableOptionsImpl portableOptions = new TestPortableOptionsImpl();\n    portableOptions.setName(\"John\");\n    portableOptions.setAge(30);\n\n    Map<String, Object> map = ModelOptionsUtils.objectToMap(portableOptions);\n\n    assertThat(map).containsEntry(\"name\", \"John\");\n    assertThat(map).containsEntry(\"age\", 30);\n}"}, {"turn_num": 2, "instruction": "If the input object is null, return an empty HashMap instead of attempting serialization. Filter out any key-value pairs in the resulting map where the value is null. Handle any JsonProcessingException that occurs during serialization or deserialization by catching it and wrapping it in a RuntimeException.", "solution": "public static Map<String, Object> objectToMap(Object source) {\n\tif (source == null) {\n\t\treturn new HashMap<>();\n\t}\n\ttry {\n\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\treturn OBJECT_MAPPER.readValue(json, new TypeReference<Map<String, Object>>() {\n\n\t\t})\n\t\t\t.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\t}\n\tcatch (JsonProcessingException e) {\n\t\tthrow new RuntimeException(e);\n\t}\n}", "test": "@Test\npublic void objectToMap() {\n\t// Test null input returns empty HashMap\n\tMap<String, Object> nullResult = ModelOptionsUtils.objectToMap(null);\n\tassertThat(nullResult).isEmpty();\n\t\n\t// Test normal object conversion\n\tTestPortableOptionsImpl portableOptions = new TestPortableOptionsImpl();\n\tportableOptions.setName(\"John\");\n\tportableOptions.setAge(30);\n\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\tMap<String, Object> map = ModelOptionsUtils.objectToMap(portableOptions);\n\n\tassertThat(map).containsEntry(\"name\", \"John\");\n\tassertThat(map).containsEntry(\"age\", 30);\n\tassertThat(map).containsEntry(\"nonInterfaceField\", \"NonInterfaceField\");\n\t\n\t// Test filtering out null values\n\tTestPortableOptionsImpl optionsWithNull = new TestPortableOptionsImpl();\n\toptionsWithNull.setName(\"Jane\");\n\toptionsWithNull.setAge(null); // This should be filtered out\n\toptionsWithNull.setNonInterfaceField(\"Field\");\n\t\n\tMap<String, Object> filteredMap = ModelOptionsUtils.objectToMap(optionsWithNull);\n\t\n\tassertThat(filteredMap).containsEntry(\"name\", \"Jane\");\n\tassertThat(filteredMap).containsEntry(\"nonInterfaceField\", \"Field\");\n\tassertThat(filteredMap).doesNotContainKey(\"age\"); // null value should be filtered out\n}"}], "metadata": {"source": "mrgbench", "func": "\tpublic static Map<String, Object> objectToMap(Object source) {\n\t\tif (source == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, new TypeReference<Map<String, Object>>() {\n\n\t\t\t})\n\t\t\t\t.entrySet()\n\t\t\t\t.stream()\n\t\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}", "repo": "spring-ai", "file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java", "func_name": "objectToMap", "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.model;\n\nimport java.beans.PropertyDescriptor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Type;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.core.type.TypeReference;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport com.fasterxml.jackson.databind.cfg.CoercionAction;\nimport com.fasterxml.jackson.databind.cfg.CoercionInputShape;\nimport com.fasterxml.jackson.databind.json.JsonMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport com.github.victools.jsonschema.module.jackson.JacksonModule;\nimport com.github.victools.jsonschema.module.jackson.JacksonOption;\nimport com.github.victools.jsonschema.module.swagger2.Swagger2Module;\n\nimport org.springframework.ai.util.JacksonUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.core.KotlinDetector;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.ObjectUtils;\n\n/**\n * Utility class for manipulating {@link ModelOptions} objects.\n *\n * @author Christian Tzolov\n * @author Thomas Vitale\n * @since 0.8.0\n */\npublic abstract class ModelOptionsUtils {\n\n\tpublic static final ObjectMapper OBJECT_MAPPER = JsonMapper.builder()\n\t\t.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n\t\t.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS)\n\t\t.addModules(JacksonUtils.instantiateAvailableModules())\n\t\t.build()\n\t\t.configure(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT, true);\n\n\tstatic {\n\t\t// Configure coercion for empty strings to null for Enum types\n\t\t// This fixes the issue where empty string finish_reason values cause\n\t\t// deserialization failures\n\t\tOBJECT_MAPPER.coercionConfigFor(Enum.class).setCoercion(CoercionInputShape.EmptyString, CoercionAction.AsNull);\n\t}\n\n\tprivate static final List<String> BEAN_MERGE_FIELD_EXCISIONS = List.of(\"class\");\n\n\tprivate static final ConcurrentHashMap<Class<?>, List<String>> REQUEST_FIELD_NAMES_PER_CLASS = new ConcurrentHashMap<>();\n\n\tprivate static final AtomicReference<SchemaGenerator> SCHEMA_GENERATOR_CACHE = new AtomicReference<>();\n\n\tprivate static TypeReference<HashMap<String, Object>> MAP_TYPE_REF = new TypeReference<>() {\n\n\t};\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using the default\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json) {\n\t\treturn jsonToMap(json, OBJECT_MAPPER);\n\t}\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using a custom\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @param objectMapper the ObjectMapper to use for deserialization.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json, ObjectMapper objectMapper) {\n\t\ttry {\n\t\t\treturn objectMapper.readValue(json, MAP_TYPE_REF);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given JSON string to an Object of the given type.\n\t * @param <T> the type of the object to return.\n\t * @param json the JSON string to convert to an object.\n\t * @param type the type of the object to return.\n\t * @return Object instance of the given type.\n\t */\n\tpublic static <T> T jsonToObject(String json, Class<T> type) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.readValue(json, type);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Failed to json: \" + json, e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonString(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonStringPrettyPrinter(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writerWithDefaultPrettyPrinter().writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. If the acceptedFieldNames is not empty, only the\n\t * fields with the given names are merged and returned. If the acceptedFieldNames is\n\t * empty, use the {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @param acceptedFieldNames the list of field names accepted for the target object.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n\n\t\tif (source == null) {\n\t\t\tsource = Map.of();\n\t\t}\n\n\t\tList<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n\t\t\t\t? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n\t\t\t\t: acceptedFieldNames;\n\n\t\tif (CollectionUtils.isEmpty(requestFieldNames)) {\n\t\t\tthrow new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n\t\t}\n\n\t\tMap<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n\t\tMap<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n\t\ttargetMap.putAll(sourceMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n\t\ttargetMap = targetMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> requestFieldNames.contains(e.getKey()))\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n\t\treturn ModelOptionsUtils.mapToClass(targetMap, clazz);\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. Returns the only field names that match the\n\t * {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz) {\n\t\treturn ModelOptionsUtils.merge(source, target, clazz, null);\n\t}", "func_start": 229, "func_end": 246, "body_len": 17, "test_file": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java", "test_start": 61, "test_end": 73, "test_code": "\t@Test\n\tpublic void objectToMap() {\n\t\tTestPortableOptionsImpl portableOptions = new TestPortableOptionsImpl();\n\t\tportableOptions.setName(\"John\");\n\t\tportableOptions.setAge(30);\n\t\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\t\tMap<String, Object> map = ModelOptionsUtils.objectToMap(portableOptions);\n\n\t\tassertThat(map).containsEntry(\"name\", \"John\");\n\t\tassertThat(map).containsEntry(\"age\", 30);\n\t\tassertThat(map).containsEntry(\"nonInterfaceField\", \"NonInterfaceField\");\n\t}", "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.model.ModelOptionsUtilsTests#objectToMap\"", "language": "java"}, "mt_id": 1035}
{"hash_id": "6f8e8f1f18bd0ff1f3419f91", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a generic function `copyToTarget` that takes a source object, a source interface class, and a target class (which extends the source class), and returns a new instance of the target class with values copied from the source object. The function should use reflection to instantiate the target object and copy all fields from the source. If the source object is null, return null. You should write self-contained code starting with: ```java\n<T, S extends T> T copyToTarget(S source, Class<T> sourceInterface, Class<T> targetClass)```", "solution": "public static <I, S extends I, T extends S> T copyToTarget(S source, Class<I> sourceInterface, Class<T> targetClass) {\n    if (source == null) {\n        return null;\n    }\n    \n    try {\n        T target = targetClass.getConstructor().newInstance();\n        \n        // Copy all fields from source to target using reflection\n        Field[] fields = source.getClass().getDeclaredFields();\n        for (Field field : fields) {\n            field.setAccessible(true);\n            Object value = field.get(source);\n            \n            // Find corresponding field in target class\n            try {\n                Field targetField = targetClass.getDeclaredField(field.getName());\n                targetField.setAccessible(true);\n                targetField.set(target, value);\n            } catch (NoSuchFieldException e) {\n                // Field doesn't exist in target, skip it\n            }\n        }\n        \n        return target;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Failed to copy to target\", e);\n    }\n}", "test": "@Test\npublic void copyToTarget() {\n    var portableOptions = new TestPortableOptionsImpl();\n    portableOptions.setName(\"John\");\n    portableOptions.setAge(30);\n    \n    TestSpecificOptions target = ModelOptionsUtils.copyToTarget(portableOptions, TestPortableOptions.class,\n            TestSpecificOptions.class);\n    \n    assertThat(target.getAge()).isEqualTo(30);\n    assertThat(target.getName()).isEqualTo(\"John\");\n}"}, {"turn_num": 2, "instruction": "Modify the function to only copy fields that are defined in the source interface by matching getter method names. Use reflection to identify getters in the source interface and copy only those properties from the source to the target instance.", "solution": "public static <I, S extends I, T extends S> T copyToTarget(S source, Class<I> sourceInterface, Class<T> targetClass) {\n    if (source == null) {\n        return null;\n    }\n    \n    try {\n        T target = targetClass.getConstructor().newInstance();\n        \n        // Get all methods from the source interface\n        java.lang.reflect.Method[] interfaceMethods = sourceInterface.getDeclaredMethods();\n        \n        for (java.lang.reflect.Method method : interfaceMethods) {\n            // Check if method is a getter (starts with \"get\" and has no parameters)\n            if (method.getName().startsWith(\"get\") && method.getParameterCount() == 0) {\n                // Get the property name from getter method\n                String propertyName = method.getName().substring(3);\n                propertyName = Character.toLowerCase(propertyName.charAt(0)) + propertyName.substring(1);\n                \n                // Get value from source using the getter\n                Object value = method.invoke(source);\n                \n                // Find corresponding field in target class and set it\n                try {\n                    java.lang.reflect.Field targetField = targetClass.getDeclaredField(propertyName);\n                    targetField.setAccessible(true);\n                    targetField.set(target, value);\n                } catch (NoSuchFieldException e) {\n                    // Field doesn't exist in target, skip it\n                }\n            }\n        }\n        \n        return target;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Failed to copy to target\", e);\n    }\n}", "test": "@Test\npublic void copyToTarget() {\n    var portableOptions = new TestPortableOptionsImpl();\n    portableOptions.setName(\"John\");\n    portableOptions.setAge(30);\n    portableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n    TestSpecificOptions target = ModelOptionsUtils.copyToTarget(portableOptions, TestPortableOptions.class,\n            TestSpecificOptions.class);\n\n    // Only interface-defined fields should be copied\n    assertThat(target.getAge()).isEqualTo(30);\n    assertThat(target.getName()).isEqualTo(\"John\");\n    assertThat(target.getSpecificField()).isNull();\n    \n    // Verify that non-interface field is NOT copied\n    try {\n        java.lang.reflect.Field nonInterfaceField = target.getClass().getDeclaredField(\"nonInterfaceField\");\n        nonInterfaceField.setAccessible(true);\n        Object nonInterfaceValue = nonInterfaceField.get(target);\n        assertThat(nonInterfaceValue).isNull(); // Should be null because it's not in the interface\n    } catch (NoSuchFieldException e) {\n        // Field doesn't exist in target class, which is fine\n    } catch (IllegalAccessException e) {\n        // Handle reflection access exception\n        throw new RuntimeException(\"Failed to access field via reflection\", e);\n    }\n}"}, {"turn_num": 3, "instruction": "Ensure that if the source object is already an instance of the target class, the function returns the source object as-is without creating a new instance or copying any data. Add proper validation to check that the sourceInterface and targetClass parameters are not null. If either is null, throw an IllegalArgumentException with a descriptive message.", "solution": "public static <I, S extends I, T extends S> T copyToTarget(S source, Class<I> sourceInterface, Class<T> targetClass) {\n    if (sourceInterface == null) {\n        throw new IllegalArgumentException(\"sourceInterface must not be null\");\n    }\n    if (targetClass == null) {\n        throw new IllegalArgumentException(\"targetClass must not be null\");\n    }\n    \n    if (source == null) {\n        return null;\n    }\n    \n    // Check if source is already an instance of target class\n    if (targetClass.isInstance(source)) {\n        return (T) source;\n    }\n    \n    try {\n        T target = targetClass.getConstructor().newInstance();\n        \n        // Get all methods from the source interface\n        java.lang.reflect.Method[] interfaceMethods = sourceInterface.getDeclaredMethods();\n        \n        for (java.lang.reflect.Method method : interfaceMethods) {\n            // Check if method is a getter (starts with \"get\" and has no parameters)\n            if (method.getName().startsWith(\"get\") && method.getParameterCount() == 0) {\n                // Get the property name from getter method\n                String propertyName = method.getName().substring(3);\n                propertyName = Character.toLowerCase(propertyName.charAt(0)) + propertyName.substring(1);\n                \n                // Get value from source using the getter\n                Object value = method.invoke(source);\n                \n                // Find corresponding field in target class and set it\n                try {\n                    java.lang.reflect.Field targetField = targetClass.getDeclaredField(propertyName);\n                    targetField.setAccessible(true);\n                    targetField.set(target, value);\n                } catch (NoSuchFieldException e) {\n                    // Field doesn't exist in target, skip it\n                }\n            }\n        }\n        \n        return target;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Failed to copy to target\", e);\n    }\n}", "test": "@Test\npublic void copyToTarget() {\n    // Test null parameter validation\n    var portableOptions = new TestPortableOptionsImpl();\n    \n    assertThatThrownBy(() -> ModelOptionsUtils.copyToTarget(portableOptions, null, TestSpecificOptions.class))\n        .isInstanceOf(IllegalArgumentException.class)\n        .hasMessage(\"sourceInterface must not be null\");\n        \n    assertThatThrownBy(() -> ModelOptionsUtils.copyToTarget(portableOptions, TestPortableOptions.class, null))\n        .isInstanceOf(IllegalArgumentException.class)\n        .hasMessage(\"targetClass must not be null\");\n    \n    // Test same instance return\n    TestSpecificOptions specificOptions = new TestSpecificOptions();\n    TestSpecificOptions result = ModelOptionsUtils.copyToTarget(specificOptions, TestPortableOptions.class, TestSpecificOptions.class);\n    assertThat(result).isSameAs(specificOptions);\n    \n    // Test normal copying\n    portableOptions.setName(\"John\");\n    portableOptions.setAge(30);\n    portableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n    TestSpecificOptions target = ModelOptionsUtils.copyToTarget(portableOptions, TestPortableOptions.class,\n            TestSpecificOptions.class);\n\n    assertThat(target.getAge()).isEqualTo(30);\n    assertThat(target.getName()).isEqualTo(\"John\");\n    assertThat(target.getSpecificField()).isNull();\n}"}, {"turn_num": 4, "instruction": "Wrap any exception that occurs during instantiation or copying (e.g., NoSuchMethodException, InstantiationException) in a RuntimeException with a descriptive message indicating the failure to convert from the source interface to the target class, including the involved class names.", "solution": "public static <I, S extends I, T extends S> T copyToTarget(S source, Class<I> sourceInterface, Class<T> targetClass) {\n    if (sourceInterface == null) {\n        throw new IllegalArgumentException(\"sourceInterface must not be null\");\n    }\n    if (targetClass == null) {\n        throw new IllegalArgumentException(\"targetClass must not be null\");\n    }\n    \n    if (source == null) {\n        return null;\n    }\n    \n    // Check if source is already an instance of target class\n    if (targetClass.isInstance(source)) {\n        return (T) source;\n    }\n    \n    try {\n        T target = targetClass.getConstructor().newInstance();\n        \n        // Get all methods from the source interface\n        java.lang.reflect.Method[] interfaceMethods = sourceInterface.getDeclaredMethods();\n        \n        for (java.lang.reflect.Method method : interfaceMethods) {\n            // Check if method is a getter (starts with \"get\" and has no parameters)\n            if (method.getName().startsWith(\"get\") && method.getParameterCount() == 0) {\n                // Get the property name from getter method\n                String propertyName = method.getName().substring(3);\n                propertyName = Character.toLowerCase(propertyName.charAt(0)) + propertyName.substring(1);\n                \n                // Get value from source using the getter\n                Object value = method.invoke(source);\n                \n                // Find corresponding field in target class and set it\n                try {\n                    java.lang.reflect.Field targetField = targetClass.getDeclaredField(propertyName);\n                    targetField.setAccessible(true);\n                    targetField.set(target, value);\n                } catch (NoSuchFieldException e) {\n                    // Field doesn't exist in target, skip it\n                }\n            }\n        }\n        \n        return target;\n    } catch (Exception e) {\n        throw new RuntimeException(\n                \"Failed to convert from \" + sourceInterface.getName() + \" to \" + targetClass.getName(),\n                e);\n    }\n}", "test": "@Test\npublic void copyToTarget() {\n    // Test normal copying behavior\n    var portableOptions = new TestPortableOptionsImpl();\n    portableOptions.setName(\"John\");\n    portableOptions.setAge(30);\n    portableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n    TestSpecificOptions target = ModelOptionsUtils.copyToTarget(portableOptions, TestPortableOptions.class,\n            TestSpecificOptions.class);\n\n    assertThat(target.getAge()).isEqualTo(30);\n    assertThat(target.getName()).isEqualTo(\"John\");\n    assertThat(target.getSpecificField()).isNull();\n    \n    // Test exception wrapping for instantiation failure\n    // Use an abstract class that cannot be instantiated\n    abstract class AbstractTarget implements TestPortableOptions {\n        private String name;\n        private Integer age;\n        \n        public String getName() { return this.name; }\n        public void setName(String name) { this.name = name; }\n        public Integer getAge() { return this.age; }\n        public void setAge(Integer age) { this.age = age; }\n    }\n    \n    assertThatThrownBy(() -> ModelOptionsUtils.copyToTarget(portableOptions, TestPortableOptions.class, AbstractTarget.class))\n    .isInstanceOf(RuntimeException.class)\n    .hasMessageContaining(\"Failed to convert from\")\n    .hasMessageContaining(TestPortableOptions.class.getName())\n    .hasMessageContaining(AbstractTarget.class.getName())\n    .hasCauseInstanceOf(NoSuchMethodException.class);\n}"}], "metadata": {"source": "mrgbench", "func": "\tpublic static <I, S extends I, T extends S> T copyToTarget(S sourceBean, Class<I> sourceInterfaceClazz,\n\t\t\tClass<T> targetBeanClazz) {\n\n\t\tAssert.notNull(sourceInterfaceClazz, \"SourceOptionsClazz must not be null\");\n\t\tAssert.notNull(targetBeanClazz, \"TargetOptionsClazz must not be null\");\n\n\t\tif (sourceBean == null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tif (sourceBean.getClass().isAssignableFrom(targetBeanClazz)) {\n\t\t\treturn (T) sourceBean;\n\t\t}\n\n\t\ttry {\n\t\t\tT targetOptions = targetBeanClazz.getConstructor().newInstance();\n\n\t\t\tModelOptionsUtils.mergeBeans(sourceBean, targetOptions, sourceInterfaceClazz, true);\n\n\t\t\treturn targetOptions;\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\"Failed to convert the \" + sourceInterfaceClazz.getName() + \" into \" + targetBeanClazz.getName(),\n\t\t\t\t\te);\n\t\t}\n\t}", "repo": "spring-ai", "file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java", "func_name": "copyToTarget", "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.model;\n\nimport java.beans.PropertyDescriptor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Type;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.core.type.TypeReference;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport com.fasterxml.jackson.databind.cfg.CoercionAction;\nimport com.fasterxml.jackson.databind.cfg.CoercionInputShape;\nimport com.fasterxml.jackson.databind.json.JsonMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport com.github.victools.jsonschema.module.jackson.JacksonModule;\nimport com.github.victools.jsonschema.module.jackson.JacksonOption;\nimport com.github.victools.jsonschema.module.swagger2.Swagger2Module;\n\nimport org.springframework.ai.util.JacksonUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.core.KotlinDetector;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.ObjectUtils;\n\n/**\n * Utility class for manipulating {@link ModelOptions} objects.\n *\n * @author Christian Tzolov\n * @author Thomas Vitale\n * @since 0.8.0\n */\npublic abstract class ModelOptionsUtils {\n\n\tpublic static final ObjectMapper OBJECT_MAPPER = JsonMapper.builder()\n\t\t.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n\t\t.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS)\n\t\t.addModules(JacksonUtils.instantiateAvailableModules())\n\t\t.build()\n\t\t.configure(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT, true);\n\n\tstatic {\n\t\t// Configure coercion for empty strings to null for Enum types\n\t\t// This fixes the issue where empty string finish_reason values cause\n\t\t// deserialization failures\n\t\tOBJECT_MAPPER.coercionConfigFor(Enum.class).setCoercion(CoercionInputShape.EmptyString, CoercionAction.AsNull);\n\t}\n\n\tprivate static final List<String> BEAN_MERGE_FIELD_EXCISIONS = List.of(\"class\");\n\n\tprivate static final ConcurrentHashMap<Class<?>, List<String>> REQUEST_FIELD_NAMES_PER_CLASS = new ConcurrentHashMap<>();\n\n\tprivate static final AtomicReference<SchemaGenerator> SCHEMA_GENERATOR_CACHE = new AtomicReference<>();\n\n\tprivate static TypeReference<HashMap<String, Object>> MAP_TYPE_REF = new TypeReference<>() {\n\n\t};\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using the default\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json) {\n\t\treturn jsonToMap(json, OBJECT_MAPPER);\n\t}\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using a custom\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @param objectMapper the ObjectMapper to use for deserialization.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json, ObjectMapper objectMapper) {\n\t\ttry {\n\t\t\treturn objectMapper.readValue(json, MAP_TYPE_REF);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given JSON string to an Object of the given type.\n\t * @param <T> the type of the object to return.\n\t * @param json the JSON string to convert to an object.\n\t * @param type the type of the object to return.\n\t * @return Object instance of the given type.\n\t */\n\tpublic static <T> T jsonToObject(String json, Class<T> type) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.readValue(json, type);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Failed to json: \" + json, e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonString(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonStringPrettyPrinter(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writerWithDefaultPrettyPrinter().writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. If the acceptedFieldNames is not empty, only the\n\t * fields with the given names are merged and returned. If the acceptedFieldNames is\n\t * empty, use the {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @param acceptedFieldNames the list of field names accepted for the target object.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n\n\t\tif (source == null) {\n\t\t\tsource = Map.of();\n\t\t}\n\n\t\tList<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n\t\t\t\t? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n\t\t\t\t: acceptedFieldNames;\n\n\t\tif (CollectionUtils.isEmpty(requestFieldNames)) {\n\t\t\tthrow new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n\t\t}\n\n\t\tMap<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n\t\tMap<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n\t\ttargetMap.putAll(sourceMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n\t\ttargetMap = targetMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> requestFieldNames.contains(e.getKey()))\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n\t\treturn ModelOptionsUtils.mapToClass(targetMap, clazz);\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. Returns the only field names that match the\n\t * {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz) {\n\t\treturn ModelOptionsUtils.merge(source, target, clazz, null);\n\t}\n\n\t/**\n\t * Converts the given object to a Map.\n\t * @param source the object to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> objectToMap(Object source) {\n\t\tif (source == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, new TypeReference<Map<String, Object>>() {\n\n\t\t\t})\n\t\t\t\t.entrySet()\n\t\t\t\t.stream()\n\t\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given Map to the given class.\n\t * @param <T> the type of the class to return.\n\t * @param source the Map to convert to the given class.\n\t * @param clazz the class to convert the Map to.\n\t * @return the converted class.\n\t */\n\tpublic static <T> T mapToClass(Map<String, Object> source, Class<T> clazz) {\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, clazz);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Returns the list of name values of the {@link JsonProperty} annotations.\n\t * @param clazz the class that contains fields annotated with {@link JsonProperty}.\n\t * @return the list of values of the {@link JsonProperty} annotations.\n\t */\n\tpublic static List<String> getJsonPropertyValues(Class<?> clazz) {\n\t\tList<String> values = new ArrayList<>();\n\t\tField[] fields = clazz.getDeclaredFields();\n\t\tfor (Field field : fields) {\n\t\t\tJsonProperty jsonPropertyAnnotation = field.getAnnotation(JsonProperty.class);\n\t\t\tif (jsonPropertyAnnotation != null) {\n\t\t\t\tvalues.add(jsonPropertyAnnotation.value());\n\t\t\t}\n\t\t}\n\t\treturn values;\n\t}", "func_start": 295, "func_end": 321, "body_len": 26, "test_file": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java", "test_start": 115, "test_end": 128, "test_code": "\t@Test\n\tpublic void copyToTarget() {\n\t\tvar portableOptions = new TestPortableOptionsImpl();\n\t\tportableOptions.setName(\"John\");\n\t\tportableOptions.setAge(30);\n\t\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\t\tTestSpecificOptions target = ModelOptionsUtils.copyToTarget(portableOptions, TestPortableOptions.class,\n\t\t\t\tTestSpecificOptions.class);\n\n\t\tassertThat(target.getAge()).isEqualTo(30);\n\t\tassertThat(target.getName()).isEqualTo(\"John\");\n\t\tassertThat(target.getSpecificField()).isNull();\n\t}", "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.model.ModelOptionsUtilsTests#copyToTarget\"", "language": "java"}, "mt_id": 1036}
{"hash_id": "c9d7e4ce8c67c1b9e7740119", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a generic function `mergeBeans` that takes a source object, a target object and a Class object representing an interface, and merges the properties from the source to the target using Spring's `BeanWrapperImpl`. \nThe function should copy all non-null property values from the source to the target via getter and setter methods, and return the modified target object. Only properties defined in this interface (by matching getter method names) should be considered for merging. Ensure that only getters present in the interface are used to determine which fields to merge.\nYou should write self-contained code starting with: ```java\n<T, S extends T> T mergeBeans(S source, T target, Class<I> sourceInterfaceClazz)```", "solution": "public static <I, S extends I, T extends S> T mergeBeans(S source, T target, Class<I> sourceInterfaceClazz,\n\t\tboolean overrideNonNullTargetValues) {\n\tAssert.notNull(source, \"Source object must not be null\");\n\tAssert.notNull(target, \"Target object must not be null\");\n\n\tBeanWrapper sourceBeanWrap = new BeanWrapperImpl(source);\n\tBeanWrapper targetBeanWrap = new BeanWrapperImpl(target);\n\n\tList<String> interfaceNames = Arrays.stream(sourceInterfaceClazz.getMethods()).map(m -> m.getName()).toList();\n\n\tfor (PropertyDescriptor descriptor : sourceBeanWrap.getPropertyDescriptors()) {\n\n\t\tif (!BEAN_MERGE_FIELD_EXCISIONS.contains(descriptor.getName())\n\t\t\t\t&& interfaceNames.contains(toGetName(descriptor.getName()))) {\n\n\t\t\tString propertyName = descriptor.getName();\n\t\t\tObject value = sourceBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t// Copy value to the target object\n\t\t\tif (value != null) {\n\t\t\t\tvar targetValue = targetBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t\tif (targetValue == null || overrideNonNullTargetValues) {\n\t\t\t\t\ttargetBeanWrap.setPropertyValue(propertyName, value);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn target;\n}", "test": "@Test\npublic void mergeBeans() {\n\n\tvar portableOptions = new TestPortableOptionsImpl();\n\tportableOptions.setName(\"John\");\n\tportableOptions.setAge(30);\n\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\tvar specificOptions = new TestSpecificOptions();\n\n\tTestSpecificOptions result = ModelOptionsUtils.mergeBeans(portableOptions, specificOptions,\n\t\t\tTestPortableOptions.class, false);\n\n\tassertThat(result.getAge()).isEqualTo(30);\n\tassertThat(result.getName()).isEqualTo(\"John\");\n\tassertThat(result.getSpecificField()).isNull();\n}"}, {"turn_num": 2, "instruction": "Add a fourth parameter `overrideNonNullTargetValues` of type boolean. If this flag is false, skip copying a property if the target's corresponding property is already set to a non-null value. If true, allow non-null source values to overwrite non-null target values.", "solution": "public static <I, S extends I, T extends S> T mergeBeans(S source, T target, Class<I> sourceInterfaceClazz,\n\t\tboolean overrideNonNullTargetValues) {\n\tAssert.notNull(source, \"Source object must not be null\");\n\tAssert.notNull(target, \"Target object must not be null\");\n\n\tBeanWrapper sourceBeanWrap = new BeanWrapperImpl(source);\n\tBeanWrapper targetBeanWrap = new BeanWrapperImpl(target);\n\n\tList<String> interfaceNames = Arrays.stream(sourceInterfaceClazz.getMethods()).map(m -> m.getName()).toList();\n\n\tfor (PropertyDescriptor descriptor : sourceBeanWrap.getPropertyDescriptors()) {\n\n\t\tif (!BEAN_MERGE_FIELD_EXCISIONS.contains(descriptor.getName())\n\t\t\t\t&& interfaceNames.contains(toGetName(descriptor.getName()))) {\n\n\t\t\tString propertyName = descriptor.getName();\n\t\t\tObject value = sourceBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t// Copy value to the target object\n\t\t\tif (value != null) {\n\t\t\t\tvar targetValue = targetBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t\tif (targetValue == null || overrideNonNullTargetValues) {\n\t\t\t\t\ttargetBeanWrap.setPropertyValue(propertyName, value);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn target;\n}", "test": "@Test\npublic void mergeBeans() {\n\n\tvar portableOptions = new TestPortableOptionsImpl();\n\tportableOptions.setName(\"John\");\n\tportableOptions.setAge(30);\n\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\tvar specificOptions = new TestSpecificOptions();\n\tspecificOptions.setName(\"Mike\");\n\tspecificOptions.setAge(60);\n\tspecificOptions.setSpecificField(\"SpecificField\");\n\n\t// Test the NEW fourth parameter: overrideNonNullTargetValues = false\n\t// This should preserve existing non-null target values\n\tTestSpecificOptions resultWithFalseFlag = ModelOptionsUtils.mergeBeans(portableOptions, specificOptions,\n\t\t\tTestPortableOptions.class, false);\n\n\t// With false flag, non-null target values should NOT be overridden\n\tassertThat(resultWithFalseFlag.getAge()).isEqualTo(60); // Target's Mike's age preserved\n\tassertThat(resultWithFalseFlag.getName()).isEqualTo(\"Mike\"); // Target's Mike's name preserved\n\tassertThat(resultWithFalseFlag.getSpecificField()).isEqualTo(\"SpecificField\");\n\n\t// Reset for next test\n\tspecificOptions.setName(\"Mike\");\n\tspecificOptions.setAge(60);\n\n\t// Test the NEW fourth parameter: overrideNonNullTargetValues = true\n\t// This should allow source to overwrite non-null target values\n\tTestSpecificOptions resultWithTrueFlag = ModelOptionsUtils.mergeBeans(portableOptions, specificOptions,\n\t\t\tTestPortableOptions.class, true);\n\n\t// With true flag, non-null target values SHOULD be overridden by source\n\tassertThat(resultWithTrueFlag.getAge()).isEqualTo(30); // John's age overwrites Mike's\n\tassertThat(resultWithTrueFlag.getName()).isEqualTo(\"John\"); // John's name overwrites Mike's\n\tassertThat(resultWithTrueFlag.getSpecificField()).isEqualTo(\"SpecificField\");\n}"}, {"turn_num": 3, "instruction": "Exclude any properties whose names are present in a predefined list called `BEAN_MERGE_FIELD_EXCISIONS` from being merged. This list should be checked before processing each property, and matching property names should be skipped during the merge. Ensure that the function throws an exception if either the source or target object is null, using Spring's `Assert.notNull` method with an appropriate error message for each case.", "solution": "public static <I, S extends I, T extends S> T mergeBeans(S source, T target, Class<I> sourceInterfaceClazz,\n\t\tboolean overrideNonNullTargetValues) {\n\tAssert.notNull(source, \"Source object must not be null\");\n\tAssert.notNull(target, \"Target object must not be null\");\n\n\tBeanWrapper sourceBeanWrap = new BeanWrapperImpl(source);\n\tBeanWrapper targetBeanWrap = new BeanWrapperImpl(target);\n\n\tList<String> interfaceNames = Arrays.stream(sourceInterfaceClazz.getMethods()).map(m -> m.getName()).toList();\n\n\tfor (PropertyDescriptor descriptor : sourceBeanWrap.getPropertyDescriptors()) {\n\n\t\tif (!BEAN_MERGE_FIELD_EXCISIONS.contains(descriptor.getName())\n\t\t\t\t&& interfaceNames.contains(toGetName(descriptor.getName()))) {\n\n\t\t\tString propertyName = descriptor.getName();\n\t\t\tObject value = sourceBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t// Copy value to the target object\n\t\t\tif (value != null) {\n\t\t\t\tvar targetValue = targetBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t\tif (targetValue == null || overrideNonNullTargetValues) {\n\t\t\t\t\ttargetBeanWrap.setPropertyValue(propertyName, value);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn target;\n}", "test": "@Test\npublic void mergeBeans() {\n\n\tvar portableOptions = new TestPortableOptionsImpl();\n\tportableOptions.setName(\"John\");\n\tportableOptions.setAge(30);\n\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\tvar specificOptions = new TestSpecificOptions();\n\n\tspecificOptions.setName(\"Mike\");\n\tspecificOptions.setAge(60);\n\tspecificOptions.setSpecificField(\"SpecificField\");\n\n\t// Test normal merge behavior - properties not in BEAN_MERGE_FIELD_EXCISIONS should be merged\n\tTestSpecificOptions result = ModelOptionsUtils.mergeBeans(portableOptions, specificOptions,\n\t\t\tTestPortableOptions.class, true);\n\n\t// Verify merge behavior - both name and age should be merged since they're not in exclusion list\n\tassertThat(result.getName()).isEqualTo(\"John\"); // Should be merged from source\n\tassertThat(result.getAge()).isEqualTo(30); // Should be merged from source\n\tassertThat(result.getSpecificField()).isEqualTo(\"SpecificField\");\n\n\t// Test null source exception\n\tassertThatThrownBy(() -> ModelOptionsUtils.mergeBeans(null, specificOptions, TestPortableOptions.class, false))\n\t\t\t.isInstanceOf(IllegalArgumentException.class)\n\t\t\t.hasMessage(\"Source object must not be null\");\n\n\t// Test null target exception\n\tassertThatThrownBy(() -> ModelOptionsUtils.mergeBeans(portableOptions, null, TestPortableOptions.class, false))\n\t\t\t.isInstanceOf(IllegalArgumentException.class)\n\t\t\t.hasMessage(\"Target object must not be null\");\n}"}], "metadata": {"source": "mrgbench", "func": "\tpublic static <I, S extends I, T extends S> T mergeBeans(S source, T target, Class<I> sourceInterfaceClazz,\n\t\t\tboolean overrideNonNullTargetValues) {\n\t\tAssert.notNull(source, \"Source object must not be null\");\n\t\tAssert.notNull(target, \"Target object must not be null\");\n\n\t\tBeanWrapper sourceBeanWrap = new BeanWrapperImpl(source);\n\t\tBeanWrapper targetBeanWrap = new BeanWrapperImpl(target);\n\n\t\tList<String> interfaceNames = Arrays.stream(sourceInterfaceClazz.getMethods()).map(m -> m.getName()).toList();\n\n\t\tfor (PropertyDescriptor descriptor : sourceBeanWrap.getPropertyDescriptors()) {\n\n\t\t\tif (!BEAN_MERGE_FIELD_EXCISIONS.contains(descriptor.getName())\n\t\t\t\t\t&& interfaceNames.contains(toGetName(descriptor.getName()))) {\n\n\t\t\t\tString propertyName = descriptor.getName();\n\t\t\t\tObject value = sourceBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t\t// Copy value to the target object\n\t\t\t\tif (value != null) {\n\t\t\t\t\tvar targetValue = targetBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t\t\tif (targetValue == null || overrideNonNullTargetValues) {\n\t\t\t\t\t\ttargetBeanWrap.setPropertyValue(propertyName, value);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn target;\n\t}", "repo": "spring-ai", "file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java", "func_name": "mergeBeans", "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.model;\n\nimport java.beans.PropertyDescriptor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Type;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.core.type.TypeReference;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport com.fasterxml.jackson.databind.cfg.CoercionAction;\nimport com.fasterxml.jackson.databind.cfg.CoercionInputShape;\nimport com.fasterxml.jackson.databind.json.JsonMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport com.github.victools.jsonschema.module.jackson.JacksonModule;\nimport com.github.victools.jsonschema.module.jackson.JacksonOption;\nimport com.github.victools.jsonschema.module.swagger2.Swagger2Module;\n\nimport org.springframework.ai.util.JacksonUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.core.KotlinDetector;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.ObjectUtils;\n\n/**\n * Utility class for manipulating {@link ModelOptions} objects.\n *\n * @author Christian Tzolov\n * @author Thomas Vitale\n * @since 0.8.0\n */\npublic abstract class ModelOptionsUtils {\n\n\tpublic static final ObjectMapper OBJECT_MAPPER = JsonMapper.builder()\n\t\t.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n\t\t.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS)\n\t\t.addModules(JacksonUtils.instantiateAvailableModules())\n\t\t.build()\n\t\t.configure(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT, true);\n\n\tstatic {\n\t\t// Configure coercion for empty strings to null for Enum types\n\t\t// This fixes the issue where empty string finish_reason values cause\n\t\t// deserialization failures\n\t\tOBJECT_MAPPER.coercionConfigFor(Enum.class).setCoercion(CoercionInputShape.EmptyString, CoercionAction.AsNull);\n\t}\n\n\tprivate static final List<String> BEAN_MERGE_FIELD_EXCISIONS = List.of(\"class\");\n\n\tprivate static final ConcurrentHashMap<Class<?>, List<String>> REQUEST_FIELD_NAMES_PER_CLASS = new ConcurrentHashMap<>();\n\n\tprivate static final AtomicReference<SchemaGenerator> SCHEMA_GENERATOR_CACHE = new AtomicReference<>();\n\n\tprivate static TypeReference<HashMap<String, Object>> MAP_TYPE_REF = new TypeReference<>() {\n\n\t};\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using the default\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json) {\n\t\treturn jsonToMap(json, OBJECT_MAPPER);\n\t}\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using a custom\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @param objectMapper the ObjectMapper to use for deserialization.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json, ObjectMapper objectMapper) {\n\t\ttry {\n\t\t\treturn objectMapper.readValue(json, MAP_TYPE_REF);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given JSON string to an Object of the given type.\n\t * @param <T> the type of the object to return.\n\t * @param json the JSON string to convert to an object.\n\t * @param type the type of the object to return.\n\t * @return Object instance of the given type.\n\t */\n\tpublic static <T> T jsonToObject(String json, Class<T> type) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.readValue(json, type);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Failed to json: \" + json, e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonString(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonStringPrettyPrinter(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writerWithDefaultPrettyPrinter().writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. If the acceptedFieldNames is not empty, only the\n\t * fields with the given names are merged and returned. If the acceptedFieldNames is\n\t * empty, use the {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @param acceptedFieldNames the list of field names accepted for the target object.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n\n\t\tif (source == null) {\n\t\t\tsource = Map.of();\n\t\t}\n\n\t\tList<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n\t\t\t\t? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n\t\t\t\t: acceptedFieldNames;\n\n\t\tif (CollectionUtils.isEmpty(requestFieldNames)) {\n\t\t\tthrow new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n\t\t}\n\n\t\tMap<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n\t\tMap<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n\t\ttargetMap.putAll(sourceMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n\t\ttargetMap = targetMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> requestFieldNames.contains(e.getKey()))\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n\t\treturn ModelOptionsUtils.mapToClass(targetMap, clazz);\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. Returns the only field names that match the\n\t * {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz) {\n\t\treturn ModelOptionsUtils.merge(source, target, clazz, null);\n\t}\n\n\t/**\n\t * Converts the given object to a Map.\n\t * @param source the object to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> objectToMap(Object source) {\n\t\tif (source == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, new TypeReference<Map<String, Object>>() {\n\n\t\t\t})\n\t\t\t\t.entrySet()\n\t\t\t\t.stream()\n\t\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given Map to the given class.\n\t * @param <T> the type of the class to return.\n\t * @param source the Map to convert to the given class.\n\t * @param clazz the class to convert the Map to.\n\t * @return the converted class.\n\t */\n\tpublic static <T> T mapToClass(Map<String, Object> source, Class<T> clazz) {\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, clazz);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Returns the list of name values of the {@link JsonProperty} annotations.\n\t * @param clazz the class that contains fields annotated with {@link JsonProperty}.\n\t * @return the list of values of the {@link JsonProperty} annotations.\n\t */\n\tpublic static List<String> getJsonPropertyValues(Class<?> clazz) {\n\t\tList<String> values = new ArrayList<>();\n\t\tField[] fields = clazz.getDeclaredFields();\n\t\tfor (Field field : fields) {\n\t\t\tJsonProperty jsonPropertyAnnotation = field.getAnnotation(JsonProperty.class);\n\t\t\tif (jsonPropertyAnnotation != null) {\n\t\t\t\tvalues.add(jsonPropertyAnnotation.value());\n\t\t\t}\n\t\t}\n\t\treturn values;\n\t}\n\n\t/**\n\t * Returns a new instance of the targetBeanClazz that copies the bean values from the\n\t * sourceBean instance.\n\t * @param sourceBean the source bean to copy the values from.\n\t * @param sourceInterfaceClazz the source interface class. Only the fields with the\n\t * same name as the interface methods are copied. This allow the source object to be a\n\t * subclass of the source interface with additional, non-interface fields.\n\t * @param targetBeanClazz the target class, a subclass of the ChatOptions, to convert\n\t * into.\n\t * @param <T> the target class type.\n\t * @return a new instance of the targetBeanClazz with the values from the sourceBean\n\t * instance.\n\t */\n\tpublic static <I, S extends I, T extends S> T copyToTarget(S sourceBean, Class<I> sourceInterfaceClazz,\n\t\t\tClass<T> targetBeanClazz) {\n\n\t\tAssert.notNull(sourceInterfaceClazz, \"SourceOptionsClazz must not be null\");\n\t\tAssert.notNull(targetBeanClazz, \"TargetOptionsClazz must not be null\");\n\n\t\tif (sourceBean == null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tif (sourceBean.getClass().isAssignableFrom(targetBeanClazz)) {\n\t\t\treturn (T) sourceBean;\n\t\t}\n\n\t\ttry {\n\t\t\tT targetOptions = targetBeanClazz.getConstructor().newInstance();\n\n\t\t\tModelOptionsUtils.mergeBeans(sourceBean, targetOptions, sourceInterfaceClazz, true);\n\n\t\t\treturn targetOptions;\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\"Failed to convert the \" + sourceInterfaceClazz.getName() + \" into \" + targetBeanClazz.getName(),\n\t\t\t\t\te);\n\t\t}\n\t}", "func_start": 337, "func_end": 367, "body_len": 30, "test_file": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java", "test_start": 86, "test_end": 113, "test_code": "\t@Test\n\tpublic void mergeBeans() {\n\n\t\tvar portableOptions = new TestPortableOptionsImpl();\n\t\tportableOptions.setName(\"John\");\n\t\tportableOptions.setAge(30);\n\t\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\t\tvar specificOptions = new TestSpecificOptions();\n\n\t\tspecificOptions.setName(\"Mike\");\n\t\tspecificOptions.setAge(60);\n\t\tspecificOptions.setSpecificField(\"SpecificField\");\n\n\t\tTestSpecificOptions specificOptions2 = ModelOptionsUtils.mergeBeans(portableOptions, specificOptions,\n\t\t\t\tTestPortableOptions.class, false);\n\n\t\tassertThat(specificOptions2.getAge()).isEqualTo(60);\n\t\tassertThat(specificOptions2.getName()).isEqualTo(\"Mike\");\n\t\tassertThat(specificOptions2.getSpecificField()).isEqualTo(\"SpecificField\");\n\n\t\tTestSpecificOptions specificOptionsWithOverride = ModelOptionsUtils.mergeBeans(portableOptions, specificOptions,\n\t\t\t\tTestPortableOptions.class, true);\n\n\t\tassertThat(specificOptionsWithOverride.getAge()).isEqualTo(30);\n\t\tassertThat(specificOptionsWithOverride.getName()).isEqualTo(\"John\");\n\t\tassertThat(specificOptionsWithOverride.getSpecificField()).isEqualTo(\"SpecificField\");\n\t}", "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.model.ModelOptionsUtilsTests#mergeBeans\"", "language": "java"}, "mt_id": 1037}
{"hash_id": "6165d0ab365701ed7777595c", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function that takes a list of paragraphs, pairs each paragraph with the next one to form a Document object, and returns a list of these Document objects. For the last paragraph, pair it with itself. You should write self-contained code starting with: ```java\n<List<Document> get() {```", "solution": "@Override\npublic List<Document> get() {\n\tvar paragraphs = this.paragraphTextExtractor.flatten();\n\tList<Document> documents = new ArrayList<>();\n\tif (CollectionUtils.isEmpty(paragraphs)) {\n\t\treturn documents;\n\t}\n\tfor (int i = 0; i < paragraphs.size(); i++) {\n\t\tParagraph from = paragraphs.get(i);\n\t\tParagraph to = (i + 1 < paragraphs.size()) ? paragraphs.get(i + 1) : from;\n\t\tDocument document = toDocument(from, to);\n\t\tif (document != null && StringUtils.hasText(document.getText())) {\n\t\t\tdocuments.add(document);\n\t\t}\n\t}\n\treturn documents;\n}", "test": "@Test\nvoid classpathRead() {\n\tPagePdfDocumentReader pdfReader = new PagePdfDocumentReader(\"classpath:/sample1.pdf\",\n\t\t\tPdfDocumentReaderConfig.builder()\n\t\t\t\t.withPageTopMargin(0)\n\t\t\t\t.withPageBottomMargin(0)\n\t\t\t\t.withPageExtractedTextFormatter(ExtractedTextFormatter.builder()\n\t\t\t\t\t.withNumberOfTopTextLinesToDelete(0)\n\t\t\t\t\t.withNumberOfBottomTextLinesToDelete(3)\n\t\t\t\t\t.withNumberOfTopPagesToSkipBeforeDelete(0)\n\t\t\t\t\t.overrideLineSeparator(\"\\n\")\n\t\t\t\t\t.build())\n\t\t\t\t.withPagesPerDocument(1)\n\t\t\t\t.build());\n\n\tList<Document> docs = pdfReader.get();\n\n\tassertThat(docs).hasSize(4);\n}"}, {"turn_num": 2, "instruction": "Handle the case where the input list of paragraphs is null or empty by returning an empty list of documents without further processing. Modify the function to skip any Document object that is null or has empty text before adding it to the result list.", "solution": "@Override\npublic List<Document> get() {\n\tvar paragraphs = this.paragraphTextExtractor.flatten();\n\tList<Document> documents = new ArrayList<>();\n\tif (paragraphs == null || paragraphs.isEmpty()) {\n\t\treturn documents;\n\t}\n\tfor (int i = 0; i < paragraphs.size(); i++) {\n\t\tParagraph from = paragraphs.get(i);\n\t\tParagraph to = (i + 1 < paragraphs.size()) ? paragraphs.get(i + 1) : from;\n\t\tDocument document = toDocument(from, to);\n\t\tif (document != null && StringUtils.hasText(document.getText())) {\n\t\t\tdocuments.add(document);\n\t\t}\n\t}\n\treturn documents;\n}", "test": "@Test\nvoid classpathRead() {\n\tPagePdfDocumentReader pdfReader = new PagePdfDocumentReader(\"classpath:/sample1.pdf\",\n\t\t\tPdfDocumentReaderConfig.builder()\n\t\t\t\t.withPageTopMargin(0)\n\t\t\t\t.withPageBottomMargin(0)\n\t\t\t\t.withPageExtractedTextFormatter(ExtractedTextFormatter.builder()\n\t\t\t\t\t.withNumberOfTopTextLinesToDelete(0)\n\t\t\t\t\t.withNumberOfBottomTextLinesToDelete(3)\n\t\t\t\t\t.withNumberOfTopPagesToSkipBeforeDelete(0)\n\t\t\t\t\t.overrideLineSeparator(\"\\n\")\n\t\t\t\t\t.build())\n\t\t\t\t.withPagesPerDocument(1)\n\t\t\t\t.build());\n\n\tList<Document> docs = pdfReader.get();\n\n\tassertThat(docs).hasSize(4);\n\n\tString allText = docs.stream().map(Document::getText).collect(Collectors.joining(System.lineSeparator()));\n\n\tassertThat(allText).doesNotContain(\n\t\t\tList.of(\"Page  1 of 4\", \"Page  2 of 4\", \"Page  3 of 4\", \"Page  4 of 4\", \"PDF  Bookmark   Sample\"));\n}"}, {"turn_num": 3, "instruction": "Add logging to the function to print 'Start processing paragraphs from PDF' at the beginning and 'End processing paragraphs from PDF' at the end of the processing phase.", "solution": "@Override\npublic List<Document> get() {\n\tvar paragraphs = this.paragraphTextExtractor.flatten();\n\tList<Document> documents = new ArrayList<>();\n\tif (paragraphs == null || paragraphs.isEmpty()) {\n\t\treturn documents;\n\t}\n\tlogger.info(\"Start processing paragraphs from PDF\");\n\tfor (int i = 0; i < paragraphs.size(); i++) {\n\t\tParagraph from = paragraphs.get(i);\n\t\tParagraph to = (i + 1 < paragraphs.size()) ? paragraphs.get(i + 1) : from;\n\t\tDocument document = toDocument(from, to);\n\t\tif (document != null && StringUtils.hasText(document.getText())) {\n\t\t\tdocuments.add(document);\n\t\t}\n\t}\n\tlogger.info(\"End processing paragraphs from PDF\");\n\treturn documents;\n}", "test": "@Test\nvoid classpathRead() {\n\n\tPagePdfDocumentReader pdfReader = new PagePdfDocumentReader(\"classpath:/sample1.pdf\",\n\t\t\tPdfDocumentReaderConfig.builder()\n\t\t\t\t.withPageTopMargin(0)\n\t\t\t\t.withPageBottomMargin(0)\n\t\t\t\t.withPageExtractedTextFormatter(ExtractedTextFormatter.builder()\n\t\t\t\t\t.withNumberOfTopTextLinesToDelete(0)\n\t\t\t\t\t.withNumberOfBottomTextLinesToDelete(3)\n\t\t\t\t\t.withNumberOfTopPagesToSkipBeforeDelete(0)\n\t\t\t\t\t.overrideLineSeparator(\"\\n\")\n\t\t\t\t\t.build())\n\t\t\t\t.withPagesPerDocument(1)\n\t\t\t\t.build());\n\n\tList<Document> docs = pdfReader.get();\n\n\tassertThat(docs).hasSize(4);\n\n\tString allText = docs.stream().map(Document::getText).collect(Collectors.joining(System.lineSeparator()));\n\n\tassertThat(allText).doesNotContain(\n\t\t\tList.of(\"Page  1 of 4\", \"Page  2 of 4\", \"Page  3 of 4\", \"Page  4 of 4\", \"PDF  Bookmark   Sample\"));\n}"}], "metadata": {"source": "mrgbench", "func": "\t@Override\n\tpublic List<Document> get() {\n\t\tvar paragraphs = this.paragraphTextExtractor.flatten();\n\t\tList<Document> documents = new ArrayList<>();\n\t\tif (CollectionUtils.isEmpty(paragraphs)) {\n\t\t\treturn documents;\n\t\t}\n\t\tlogger.info(\"Start processing paragraphs from PDF\");\n\t\tfor (int i = 0; i < paragraphs.size(); i++) {\n\t\t\tParagraph from = paragraphs.get(i);\n\t\t\tParagraph to = (i + 1 < paragraphs.size()) ? paragraphs.get(i + 1) : from;\n\t\t\tDocument document = toDocument(from, to);\n\t\t\tif (document != null && StringUtils.hasText(document.getText())) {\n\t\t\t\tdocuments.add(document);\n\t\t\t}\n\t\t}\n\t\tlogger.info(\"End processing paragraphs from PDF\");\n\t\treturn documents;\n\t}", "repo": "spring-ai", "file_path": "document-readers/pdf-reader/src/main/java/org/springframework/ai/reader/pdf/ParagraphPdfDocumentReader.java", "func_name": "get", "context": "/*\n * Copyright 2023-2025 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.reader.pdf;\n\nimport java.awt.Rectangle;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.pdfbox.pdfparser.PDFParser;\nimport org.apache.pdfbox.pdmodel.PDDocument;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.springframework.ai.document.Document;\nimport org.springframework.ai.document.DocumentReader;\nimport org.springframework.ai.reader.pdf.config.ParagraphManager;\nimport org.springframework.ai.reader.pdf.config.ParagraphManager.Paragraph;\nimport org.springframework.ai.reader.pdf.config.PdfDocumentReaderConfig;\nimport org.springframework.ai.reader.pdf.layout.PDFLayoutTextStripperByArea;\nimport org.springframework.core.io.DefaultResourceLoader;\nimport org.springframework.core.io.Resource;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.StringUtils;\n\n/**\n * Uses the PDF catalog (e.g. TOC) information to split the input PDF into text paragraphs\n * and output a single {@link Document} per paragraph.\n *\n * This class provides methods for reading and processing PDF documents. It uses the\n * Apache PDFBox library for parsing PDF content and converting it into text paragraphs.\n * The paragraphs are grouped into {@link Document} objects.\n *\n * @author Christian Tzolov\n * @author Heonwoo Kim\n */\npublic class ParagraphPdfDocumentReader implements DocumentReader {\n\n\t// Constants for metadata keys\n\tprivate static final String METADATA_START_PAGE = \"page_number\";\n\n\tprivate static final String METADATA_END_PAGE = \"end_page_number\";\n\n\tprivate static final String METADATA_TITLE = \"title\";\n\n\tprivate static final String METADATA_LEVEL = \"level\";\n\n\tprivate static final String METADATA_FILE_NAME = \"file_name\";\n\n\tprotected final PDDocument document;\n\n\tprivate final Logger logger = LoggerFactory.getLogger(getClass());\n\n\tprivate final ParagraphManager paragraphTextExtractor;\n\n\tprotected String resourceFileName;\n\n\tprivate PdfDocumentReaderConfig config;\n\n\t/**\n\t * Constructs a ParagraphPdfDocumentReader using a resource URL.\n\t * @param resourceUrl The URL of the PDF resource.\n\t */\n\tpublic ParagraphPdfDocumentReader(String resourceUrl) {\n\t\tthis(new DefaultResourceLoader().getResource(resourceUrl));\n\t}\n\n\t/**\n\t * Constructs a ParagraphPdfDocumentReader using a resource.\n\t * @param pdfResource The PDF resource.\n\t */\n\tpublic ParagraphPdfDocumentReader(Resource pdfResource) {\n\t\tthis(pdfResource, PdfDocumentReaderConfig.defaultConfig());\n\t}\n\n\t/**\n\t * Constructs a ParagraphPdfDocumentReader using a resource URL and a configuration.\n\t * @param resourceUrl The URL of the PDF resource.\n\t * @param config The configuration for PDF document processing.\n\t */\n\tpublic ParagraphPdfDocumentReader(String resourceUrl, PdfDocumentReaderConfig config) {\n\t\tthis(new DefaultResourceLoader().getResource(resourceUrl), config);\n\t}\n\n\t/**\n\t * Constructs a ParagraphPdfDocumentReader using a resource and a configuration.\n\t * @param pdfResource The PDF resource.\n\t * @param config The configuration for PDF document processing.\n\t */\n\tpublic ParagraphPdfDocumentReader(Resource pdfResource, PdfDocumentReaderConfig config) {\n\n\t\ttry {\n\t\t\tPDFParser pdfParser = new PDFParser(\n\t\t\t\t\tnew org.apache.pdfbox.io.RandomAccessReadBuffer(pdfResource.getInputStream()));\n\t\t\tthis.document = pdfParser.parse();\n\n\t\t\tthis.config = config;\n\n\t\t\tthis.paragraphTextExtractor = new ParagraphManager(this.document);\n\n\t\t\tthis.resourceFileName = pdfResource.getFilename();\n\t\t}\n\t\tcatch (IllegalArgumentException iae) {\n\t\t\tthrow iae;\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}", "func_start": 128, "func_end": 146, "body_len": 18, "test_file": "document-readers/pdf-reader/src/test/java/org/springframework/ai/reader/pdf/PagePdfDocumentReaderTests.java", "test_start": 36, "test_end": 60, "test_code": "\t@Test\n\tvoid classpathRead() {\n\n\t\tPagePdfDocumentReader pdfReader = new PagePdfDocumentReader(\"classpath:/sample1.pdf\",\n\t\t\t\tPdfDocumentReaderConfig.builder()\n\t\t\t\t\t.withPageTopMargin(0)\n\t\t\t\t\t.withPageBottomMargin(0)\n\t\t\t\t\t.withPageExtractedTextFormatter(ExtractedTextFormatter.builder()\n\t\t\t\t\t\t.withNumberOfTopTextLinesToDelete(0)\n\t\t\t\t\t\t.withNumberOfBottomTextLinesToDelete(3)\n\t\t\t\t\t\t.withNumberOfTopPagesToSkipBeforeDelete(0)\n\t\t\t\t\t\t.overrideLineSeparator(\"\\n\")\n\t\t\t\t\t\t.build())\n\t\t\t\t\t.withPagesPerDocument(1)\n\t\t\t\t\t.build());\n\n\t\tList<Document> docs = pdfReader.get();\n\n\t\tassertThat(docs).hasSize(4);\n\n\t\tString allText = docs.stream().map(Document::getText).collect(Collectors.joining(System.lineSeparator()));\n\n\t\tassertThat(allText).doesNotContain(\n\t\t\t\tList.of(\"Page  1 of 4\", \"Page  2 of 4\", \"Page  3 of 4\", \"Page  4 of 4\", \"PDF  Bookmark   Sample\"));\n\t}", "test_instruction": "mvn test -pl document-readers/pdf-reader -Dtest=\"org.springframework.ai.reader.pdf.PagePdfDocumentReaderTests#classpathRead\"", "language": "java"}, "mt_id": 1039}
{"hash_id": "b4e1056e5ea2a627db586be4", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `ensureCapacity` that takes a `FixedBitSet` and an integer `numBits`, and returns a `FixedBitSet` capable of holding at least `numBits + 1` bits. If the given bitset already has sufficient capacity, return it directly. Otherwise, return a new `FixedBitSet` with enough capacity to hold `numBits + 1` bits. Use the helper method `bits2words(numBits)` to compute the required number of 64-bit words for the new capacity. You should write self-contained code starting with: ```java\npublic static FixedBitSet ensureCapacity(FixedBitSet bits, int numBits) {\n    // Your code here\n}```", "solution": "public static FixedBitSet ensureCapacity(FixedBitSet bits, int numBits) {\n    if (bits.numBits >= numBits + 1) {\n        return bits;\n    } else {\n        int numWords = bits2words(numBits);\n        return new FixedBitSet(numWords << 6);\n    }\n}", "test": "@Override\n@Test\npublic void testPrevSetBit() {\n    // Test returning original bitset when capacity is sufficient\n    FixedBitSet original = new FixedBitSet(100);\n    FixedBitSet result = FixedBitSet.ensureCapacity(original, 50);\n    assertSame(original, result);\n    \n    // Test creating new bitset when capacity is insufficient\n    FixedBitSet small = new FixedBitSet(10);\n    FixedBitSet expanded = FixedBitSet.ensureCapacity(small, 50);\n    assertNotSame(small, expanded);\n    assertTrue(expanded.length() >= 51);\n}"}, {"turn_num": 2, "instruction": "Modify the function to reuse the underlying `long[]` array from the original `FixedBitSet` when extending capacity, by copying it only if the current array is too small, thus avoiding unnecessary allocation and improving efficiency without mutating the original array.", "solution": "public static FixedBitSet ensureCapacity(FixedBitSet bits, int numBits) {\n    if (numBits < bits.numBits) {\n        return bits;\n    } else {\n        int numWords = bits2words(numBits);\n        long[] arr = bits.getBits();\n        if (numWords > arr.length) {\n            arr = ArrayUtil.grow(arr, numWords + 1);\n        }\n        return new FixedBitSet(arr, arr.length << 6);\n    }\n}", "test": "@Override\n@Test\npublic void testPrevSetBit() {\n    FixedBitSet original = new FixedBitSet(64);\n    original.set(10);\n    FixedBitSet extended = FixedBitSet.ensureCapacity(original, 100);\n    \n    // Test that the original array is reused when possible\n    FixedBitSet sameSize = FixedBitSet.ensureCapacity(original, 50);\n    assertSame(original.getBits(), sameSize.getBits());\n    \n    // Test that bits are preserved when extending\n    assertTrue(extended.get(10));\n}"}, {"turn_num": 3, "instruction": "Ensure that when extending the bitset, any newly allocated words in the `long[]` array are properly initialized to zero, and that 'ghost' bits beyond the original `numBits` are not exposed with garbage values, relying on safe array growth that clears unused positions.", "solution": "public static FixedBitSet ensureCapacity(FixedBitSet bits, int numBits) {\n    if (numBits < bits.numBits) {\n        return bits;\n    } else {\n        int numWords = bits2words(numBits);\n        long[] arr = bits.getBits();\n        if (numWords > arr.length) {\n            int oldLength = arr.length;\n            arr = ArrayUtil.grow(arr, numWords + 1);\n            // Clear newly allocated words to ensure ghost bits are zero\n            for (int i = oldLength; i < arr.length; i++) {\n                arr[i] = 0L;\n            }\n        }\n        return new FixedBitSet(arr, arr.length << 6);\n    }\n}", "test": "@Override\n@Test\npublic void testPrevSetBit() {\n    // Test that ensureCapacity properly initializes newly allocated words\n    FixedBitSet original = new FixedBitSet(64); // 1 word (64 bits)\n    original.set(0);\n    original.set(63); // Set the last bit of the first word\n    \n    // Ensure capacity to 128 bits (2 words) - this should trigger array growth\n    FixedBitSet expanded = FixedBitSet.ensureCapacity(original, 128);\n    \n    // Verify that original bits are preserved\n    assertTrue(\"Original bit 0 should be preserved\", expanded.get(0));\n    assertTrue(\"Original bit 63 should be preserved\", expanded.get(63));\n    \n    // Verify that newly allocated bits are properly initialized to zero\n    // Test several positions in the newly allocated word\n    assertFalse(\"Bit 64 should be initialized to zero\", expanded.get(64));\n    assertFalse(\"Bit 65 should be initialized to zero\", expanded.get(65));\n    assertFalse(\"Bit 100 should be initialized to zero\", expanded.get(100));\n    assertFalse(\"Bit 127 should be initialized to zero\", expanded.get(127));\n    \n    // Test that the capacity was actually increased\n    assertTrue(\"Capacity should be at least 128 bits\", expanded.length() >= 128);\n}"}], "metadata": {"source": "mrgbench", "func": "    public static FixedBitSet ensureCapacity(FixedBitSet bits, int numBits) {\n        if (numBits < bits.numBits) {\n            return bits;\n        } else {\n            // Depends on the ghost bits being clear!\n            // (Otherwise, they may become visible in the new instance)\n            int numWords = bits2words(numBits);\n            long[] arr = bits.getBits();\n            if (numWords >= arr.length) {\n                arr = ArrayUtil.grow(arr, numWords + 1);\n            }\n            return new FixedBitSet(arr, arr.length << 6);\n        }\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/util/FixedBitSet.java", "func_name": "ensureCapacity", "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.util;\n\nimport java.util.Arrays;\n\n/**\n * BitSet of fixed length (numBits), backed by accessible ({@link #getBits}) long[], accessed with\n * an int index, implementing {@link Bits}.\n */\npublic final class FixedBitSet extends BitSet {\n    private static final long BASE_RAM_BYTES_USED =\n            RamUsageEstimator.shallowSizeOfInstance(FixedBitSet.class);\n\n    private final long[] bits; // Array of longs holding the bits\n    private final int numBits; // The number of bits in use\n    private final int numWords; // The exact number of longs needed to hold numBits (<= bits.length)", "func_start": 49, "func_end": 62, "body_len": 13, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java", "test_start": 329, "test_end": 335, "test_code": "  @Override\n  @Test\n  public void testPrevSetBit() {\n    checkPrevSetBitArray(new int[] {}, 0);\n    checkPrevSetBitArray(new int[] {0}, 1);\n    checkPrevSetBitArray(new int[] {0, 2}, 3);\n  }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.util.TestFixedBitSet#testPrevSetBit\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1040}
{"hash_id": "38770cee6b092a934344568e", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `invert()` in the `Matrix` class that computes and returns the inverse of a matrix using Gaussian elimination without any pivoting or safety checks. Assume the matrix is always square and invertible. Start by creating an augmented matrix by appending the identity matrix to the right of the original matrix. Then perform naive row reduction: for each column, scale the pivot row so the diagonal element becomes 1, and eliminate all other entries in that column by subtracting appropriate multiples of the pivot row. Finally, extract the right half of the augmented matrix as the inverse and return it. You should write self-contained code starting with: ```java\n<code>```", "solution": "public Matrix invert() {\n    int N = data.length;\n\n    // Initialize augmented matrix (original matrix on the left, identity matrix on the right)\n    var augmented = new Matrix(N, 2 * N);\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            augmented.set(i, j, get(i, j));\n            augmented.set(i, j + N, (i == j) ? 1 : 0);\n        }\n    }\n\n    // Perform naive Gaussian elimination without pivoting\n    for (int i = 0; i < N; i++) {\n        // Scale pivot row so diagonal element becomes 1\n        VectorUtil.scale(augmented.data[i], 1 / augmented.get(i, i));\n\n        // Eliminate all other entries in this column\n        for (int k = 0; k < N; k++) {\n            if (k != i) {\n                float factor = augmented.get(k, i);\n                for (int j = 0; j < 2 * N; j++) {\n                    augmented.addTo(k, j, -factor * augmented.get(i, j));\n                }\n            }\n        }\n    }\n\n    // Extract inverse matrix (right half of augmented matrix)\n    var inverse = new Matrix(N, N);\n    for (int i = 0; i < N; i++) {\n        inverse.data[i].copyFrom(augmented.data[i], N, 0, N);\n    }\n\n    return inverse;\n}", "test": "@Test\npublic void testInvert() {\n    var matrix = Matrix.from(new float[][] {{4, 7}, {2, 6}});\n    var expected = Matrix.from(new float[][] {{0.6f, -0.7f}, {-0.2f, 0.4f}});\n    assertEquals(expected, matrix.invert());\n}"}, {"turn_num": 2, "instruction": "Modify the method to first check if the matrix is square. If the number of rows does not equal the number of columns, throw an IllegalArgumentException with the message 'matrix must be square'.", "solution": "public Matrix invert() {\n    if (data.length == 0 || data.length != data[0].length()) {\n        throw new IllegalArgumentException(\"matrix must be square\");\n    }\n    \n    int N = data.length;\n\n    // Initialize augmented matrix (original matrix on the left, identity matrix on the right)\n    var augmented = new Matrix(N, 2 * N);\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            augmented.set(i, j, get(i, j));\n            augmented.set(i, j + N, (i == j) ? 1 : 0);\n        }\n    }\n\n    // Perform naive Gaussian elimination without pivoting\n    for (int i = 0; i < N; i++) {\n        // Scale pivot row so diagonal element becomes 1\n        VectorUtil.scale(augmented.data[i], 1 / augmented.get(i, i));\n\n        // Eliminate all other entries in this column\n        for (int k = 0; k < N; k++) {\n            if (k != i) {\n                float factor = augmented.get(k, i);\n                for (int j = 0; j < 2 * N; j++) {\n                    augmented.addTo(k, j, -factor * augmented.get(i, j));\n                }\n            }\n        }\n    }\n\n    // Extract inverse matrix (right half of augmented matrix)\n    var inverse = new Matrix(N, N);\n    for (int i = 0; i < N; i++) {\n        inverse.data[i].copyFrom(augmented.data[i], N, 0, N);\n    }\n\n    return inverse;\n}", "test": "@Test\npublic void testInvert() {\n    // Test that non-square matrix throws IllegalArgumentException\n    var nonSquareMatrix = new Matrix(2, 3);\n    try {\n        nonSquareMatrix.invert();\n        throw new AssertionError(\"Expected IllegalArgumentException for non-square matrix\");\n    } catch (IllegalArgumentException e) {\n        assertEquals(\"matrix must be square\", e.getMessage());\n    }\n    \n    // Test that square matrix inversion still works\n    var matrix = Matrix.from(new float[][] {{4, 7}, {2, 6}});\n    var expected = Matrix.from(new float[][] {{0.6f, -0.7f}, {-0.2f, 0.4f}});\n    assertEquals(expected, matrix.invert());\n}"}, {"turn_num": 3, "instruction": "Enhance the elimination process by implementing partial pivoting: before processing each column, find the row from the current row down with the largest absolute value in that column, and swap it with the current row to improve numerical stability.", "solution": "public Matrix invert() {\n    if (data.length == 0 || data.length != data[0].length()) {\n        throw new IllegalArgumentException(\"matrix must be square\");\n    }\n    \n    int N = data.length;\n\n    // Initialize augmented matrix (original matrix on the left, identity matrix on the right)\n    var augmented = new Matrix(N, 2 * N);\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            augmented.set(i, j, get(i, j));\n            augmented.set(i, j + N, (i == j) ? 1 : 0);\n        }\n    }\n\n    // Perform Gaussian elimination with partial pivoting\n    for (int i = 0; i < N; i++) {\n        // Partial pivoting: Find the row with the largest absolute value in column i\n        int maxRow = i;\n        for (int k = i + 1; k < N; k++) {\n            if (Math.abs(augmented.get(k, i)) > Math.abs(augmented.get(maxRow, i))) {\n                maxRow = k;\n            }\n        }\n\n        // Swap the current row with the maxRow\n        var temp = augmented.data[i];\n        augmented.data[i] = augmented.data[maxRow];\n        augmented.data[maxRow] = temp;\n\n        // Scale pivot row so diagonal element becomes 1\n        VectorUtil.scale(augmented.data[i], 1 / augmented.get(i, i));\n\n        // Eliminate all other entries in this column\n        for (int k = 0; k < N; k++) {\n            if (k != i) {\n                float factor = augmented.get(k, i);\n                for (int j = 0; j < 2 * N; j++) {\n                    augmented.addTo(k, j, -factor * augmented.get(i, j));\n                }\n            }\n        }\n    }\n\n    // Extract inverse matrix (right half of augmented matrix)\n    var inverse = new Matrix(N, N);\n    for (int i = 0; i < N; i++) {\n        inverse.data[i].copyFrom(augmented.data[i], N, 0, N);\n    }\n\n    return inverse;\n}", "test": "@Test\npublic void testInvert() {\n    // Test case that benefits from partial pivoting\n    // Matrix where second row has larger absolute value in first column (2 > 1)\n    // Partial pivoting should swap rows to use 2 as pivot instead of 1\n    var matrix = Matrix.from(new float[][] {{1f, 3f}, {2f, 4f}});\n    var expected = Matrix.from(new float[][] {{-2f, 1.5f}, {1f, -0.5f}});\n    assertEquals(expected, matrix.invert());\n}"}, {"turn_num": 4, "instruction": "Add a check during pivoting to detect if the matrix is singular. If the largest absolute value in the current column is zero (or effectively zero due to floating-point precision), throw an IllegalArgumentException with the message 'matrix is singular and cannot be inverted'.", "solution": "public Matrix invert() {\n    if (data.length == 0 || data.length != data[0].length()) {\n        throw new IllegalArgumentException(\"matrix must be square\");\n    }\n    \n    int N = data.length;\n\n    // Initialize augmented matrix (original matrix on the left, identity matrix on the right)\n    var augmented = new Matrix(N, 2 * N);\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            augmented.set(i, j, get(i, j));\n            augmented.set(i, j + N, (i == j) ? 1 : 0);\n        }\n    }\n\n    // Perform Gaussian elimination with partial pivoting\n    for (int i = 0; i < N; i++) {\n        // Partial pivoting: Find the row with the largest absolute value in column i\n        int maxRow = i;\n        for (int k = i + 1; k < N; k++) {\n            if (Math.abs(augmented.get(k, i)) > Math.abs(augmented.get(maxRow, i))) {\n                maxRow = k;\n            }\n        }\n\n        // Check for singularity\n        if (Math.abs(augmented.get(maxRow, i)) == 0) {\n            throw new IllegalArgumentException(\"matrix is singular and cannot be inverted\");\n        }\n\n        // Swap the current row with the maxRow\n        var temp = augmented.data[i];\n        augmented.data[i] = augmented.data[maxRow];\n        augmented.data[maxRow] = temp;\n\n        // Scale pivot row so diagonal element becomes 1\n        VectorUtil.scale(augmented.data[i], 1 / augmented.get(i, i));\n\n        // Eliminate all other entries in this column\n        for (int k = 0; k < N; k++) {\n            if (k != i) {\n                float factor = augmented.get(k, i);\n                for (int j = 0; j < 2 * N; j++) {\n                    augmented.addTo(k, j, -factor * augmented.get(i, j));\n                }\n            }\n        }\n    }\n\n    // Extract inverse matrix (right half of augmented matrix)\n    var inverse = new Matrix(N, N);\n    for (int i = 0; i < N; i++) {\n        inverse.data[i].copyFrom(augmented.data[i], N, 0, N);\n    }\n\n    return inverse;\n}", "test": "@Test\npublic void testInvert() {\n    var matrix = Matrix.from(new float[][] {{4, 7}, {2, 6}});\n    var expected = Matrix.from(new float[][] {{0.6f, -0.7f}, {-0.2f, 0.4f}});\n    assertEquals(expected, matrix.invert());\n    \n    // Test singular matrix detection\n    var singularMatrix = Matrix.from(new float[][] {{1, 2}, {2, 4}});\n    boolean exceptionThrown = false;\n    try {\n        singularMatrix.invert();\n    } catch (IllegalArgumentException e) {\n        exceptionThrown = true;\n        assertEquals(\"matrix is singular and cannot be inverted\", e.getMessage());\n    }\n    assertEquals(true, exceptionThrown);\n}"}], "metadata": {"source": "mrgbench", "func": "    public Matrix invert() {\n        if (data.length == 0 || data.length != data[0].length()) {\n            throw new IllegalArgumentException(\"matrix must be square\");\n        }\n\n        int N = data.length;\n\n        // Initialize augmented matrix (original matrix on the left, identity matrix on the right)\n        var augmented = new Matrix(N, 2 * N);\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                augmented.set(i, j, get(i, j));\n                augmented.set(i, j + N, (i == j) ? 1 : 0);\n            }\n        }\n\n        // Perform Gaussian elimination with pivoting\n        for (int i = 0; i < N; i++) {\n            // Pivot: Find the row with the largest absolute value in column i to promote numerical stability\n            int maxRow = i;\n            for (int k = i + 1; k < N; k++) {\n                if (abs(augmented.get(k, i)) > abs(augmented.get(maxRow, i))) {\n                    maxRow = k;\n                }\n            }\n\n            // Swap the current row with the maxRow\n            var temp = augmented.data[i];\n            augmented.data[i] = augmented.data[maxRow];\n            augmented.data[maxRow] = temp;\n\n            // Scale pivot row\n            VectorUtil.scale(augmented.data[i], 1 / augmented.get(i, i));\n\n            // Eliminate below and above pivot\n            for (int k = 0; k < N; k++) {\n                if (k != i) {\n                    float factor = augmented.get(k, i);\n                    for (int j = 0; j < 2 * N; j++) {\n                        augmented.addTo(k, j, -factor * augmented.get(i, j));\n                    }\n                }\n            }\n        }\n\n        // Extract inverse matrix\n        var inverse = new Matrix(N, N);\n        for (int i = 0; i < N; i++) {\n            inverse.data[i].copyFrom(augmented.data[i], N, 0, N);\n        }\n\n        return inverse;\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/vector/Matrix.java", "func_name": "invert", "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.vector;\n\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\nimport static java.lang.Math.abs;\n\n/**\n * Matrix object where each row is a VectorFloat; this makes multiplication of a matrix by a vector\n * a series of efficient dot products.\n */\npublic class Matrix {\n    private static final VectorTypeSupport vts = VectorizationProvider.getInstance().getVectorTypeSupport();\n\n    VectorFloat<?>[] data;\n\n    public Matrix(int m, int n) {\n        this(m, n, true);\n    }\n\n    public Matrix(int m, int n, boolean allocateZeroed) {\n        data = new VectorFloat[m];\n        if (allocateZeroed) {\n            for (int i = 0; i < m; i++) {\n                data[i] = vts.createFloatVector(n);\n            }\n        }\n    }\n\n    public float get(int i, int j) {\n        return data[i].get(j);\n    }\n\n    public void set(int i, int j, float value) {\n        data[i].set(j, value);\n    }\n\n    public boolean isIsomorphicWith(Matrix other) {\n        return data.length == other.data.length && data[0].length() == other.data[0].length();\n    }\n\n    public String toString() {\n        StringBuilder sb = new StringBuilder();\n        for (VectorFloat<?> row : data) {\n            sb.append(row.toString());\n            sb.append(\"\\n\");\n        }\n        return sb.toString();\n    }", "func_start": 71, "func_end": 123, "body_len": 52, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/vector/TestMatrixUtil.java", "test_start": 33, "test_end": 38, "test_code": "    @Test\n    public void testInvert() {\n        var matrix = Matrix.from(new float[][] {{4, 7}, {2, 6}});\n        var expected = Matrix.from(new float[][] {{0.6f, -0.7f}, {-0.2f, 0.4f}});\n        assertEquals(expected, matrix.invert());\n    }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.vector.TestMatrixUtil#testInvert\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1042}
{"hash_id": "697f7351aaff39abd8dd4696", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `rerank` that takes an integer topK, a scoring function for reranking, a score threshold (rerankFloor), and two output containers: one for reranked results and one for unused results. The function should compute exact scores for all items, keep the topK items with the highest exact scores in the reranked container, and place the rest in the unused container. Finally, return the worst (lowest) approximate score among the topK reranked items, or positive infinity if fewer than topK items are reranked. You should write self-contained code starting with: ```java\n<code>```", "solution": "public float rerank(int topK, ScoreFunction.ExactScoreFunction reranker, float rerankFloor, NodeQueue reranked, NodesUnsorted unused) {\n    // Compute exact scores for all items\n    int[] ids = new int[size()];\n    float[] exactScores = new float[size()];\n    float[] approximateScores = new float[size()];\n    \n    for (int i = 0; i < size(); i++) {\n        long heapValue = heap.get(i + 1);\n        ids[i] = decodeNodeId(heapValue);\n        approximateScores[i] = decodeScore(heapValue);\n        exactScores[i] = reranker.similarityTo(ids[i]);\n    }\n    \n    // Find topK items with highest exact scores using selection approach\n    boolean[] selected = new boolean[size()];\n    int selectedCount = 0;\n    \n    // Select topK items with highest exact scores\n    for (int k = 0; k < topK && k < size(); k++) {\n        int bestIdx = -1;\n        float bestScore = Float.NEGATIVE_INFINITY;\n        \n        for (int i = 0; i < size(); i++) {\n            if (!selected[i] && exactScores[i] > bestScore) {\n                bestScore = exactScores[i];\n                bestIdx = i;\n            }\n        }\n        \n        if (bestIdx >= 0) {\n            selected[bestIdx] = true;\n            reranked.push(ids[bestIdx], exactScores[bestIdx]);\n            selectedCount++;\n        }\n    }\n    \n    // Add remaining items to unused\n    for (int i = 0; i < size(); i++) {\n        if (!selected[i]) {\n            unused.add(ids[i], approximateScores[i]);\n        }\n    }\n    \n    // Return worst approximate score among topK\n    if (selectedCount < topK) {\n        return Float.POSITIVE_INFINITY;\n    }\n    \n    float worstApproximate = Float.POSITIVE_INFINITY;\n    for (int i = 0; i < size(); i++) {\n        if (selected[i]) {\n            worstApproximate = Math.min(worstApproximate, approximateScores[i]);\n        }\n    }\n    \n    return worstApproximate;\n}", "test": "@Test\npublic void testTopMinHeap() {\n    NodeQueue nn = new NodeQueue(new GrowableLongHeap(2), NodeQueue.Order.MIN_HEAP);\n    nn.push(1, 0.5f);\n    nn.push(2, -0.5f);\n    // higher scores are better; lowest score on top\n    assertEquals(-0.5f, nn.topScore(), 0);\n    assertEquals(2, nn.topNode());\n}"}, {"turn_num": 2, "instruction": "Modify the function to only compute exact scores for items whose approximate scores meet or exceed the rerankFloor. For items below the floor, do not compute exact scores and add them directly to the unused container with their approximate scores.", "solution": "public float rerank(int topK, ScoreFunction.ExactScoreFunction reranker, float rerankFloor, NodeQueue reranked, NodesUnsorted unused) {\n    // First pass: identify items above floor and compute their exact scores\n    int[] aboveFloorIds = new int[size()];\n    float[] exactScores = new float[size()];\n    float[] approximateScores = new float[size()];\n    boolean[] isAboveFloor = new boolean[size()];\n    int aboveFloorCount = 0;\n    \n    for (int i = 0; i < size(); i++) {\n        long heapValue = heap.get(i + 1);\n        int nodeId = decodeNodeId(heapValue);\n        float approxScore = decodeScore(heapValue);\n        \n        aboveFloorIds[i] = nodeId;\n        approximateScores[i] = approxScore;\n        \n        if (approxScore >= rerankFloor) {\n            // Compute exact score only for items above floor\n            exactScores[i] = reranker.similarityTo(nodeId);\n            isAboveFloor[i] = true;\n            aboveFloorCount++;\n        } else {\n            // Add items below floor directly to unused\n            unused.add(nodeId, approxScore);\n            isAboveFloor[i] = false;\n        }\n    }\n    \n    // Select topK from items above floor using their exact scores\n    boolean[] selected = new boolean[size()];\n    int selectedCount = 0;\n    \n    for (int k = 0; k < topK && selectedCount < aboveFloorCount; k++) {\n        int bestIdx = -1;\n        float bestScore = Float.NEGATIVE_INFINITY;\n        \n        for (int i = 0; i < size(); i++) {\n            if (isAboveFloor[i] && !selected[i] && exactScores[i] > bestScore) {\n                bestScore = exactScores[i];\n                bestIdx = i;\n            }\n        }\n        \n        if (bestIdx >= 0) {\n            selected[bestIdx] = true;\n            reranked.push(aboveFloorIds[bestIdx], exactScores[bestIdx]);\n            selectedCount++;\n        }\n    }\n    \n    // Add remaining above-floor items to unused with their approximate scores\n    for (int i = 0; i < size(); i++) {\n        if (isAboveFloor[i] && !selected[i]) {\n            unused.add(aboveFloorIds[i], approximateScores[i]);\n        }\n    }\n    \n    // Return worst approximate score among selected topK\n    if (selectedCount < topK) {\n        return Float.POSITIVE_INFINITY;\n    }\n    \n    float worstApproximate = Float.POSITIVE_INFINITY;\n    for (int i = 0; i < size(); i++) {\n        if (selected[i]) {\n            worstApproximate = Math.min(worstApproximate, approximateScores[i]);\n        }\n    }\n    \n    return worstApproximate;\n}", "test": "@Test\npublic void testTopMinHeap() {\n    NodeQueue nn = new NodeQueue(new GrowableLongHeap(2), NodeQueue.Order.MIN_HEAP);\n    nn.push(1, 0.5f);\n    nn.push(2, -0.5f);\n    // higher scores are better; lowest score on top\n    assertEquals(-0.5f, nn.topScore(), 0);\n    assertEquals(2, nn.topNode());\n}"}, {"turn_num": 3, "instruction": "Ensure that if no item meets the rerankFloor, the single item with the highest approximate score is still reranked with its exact score, and all other items are placed in the unused container with their approximate scores.", "solution": "public float rerank(int topK, ScoreFunction.ExactScoreFunction reranker, float rerankFloor, NodeQueue reranked, NodesUnsorted unused) {\n    // First pass: identify items above floor and find best item\n    int[] nodeIds = new int[size()];\n    float[] approximateScores = new float[size()];\n    boolean[] isAboveFloor = new boolean[size()];\n    int aboveFloorCount = 0;\n    int bestItemIndex = -1;\n    float bestApproximateScore = Float.NEGATIVE_INFINITY;\n    \n    for (int i = 0; i < size(); i++) {\n        long heapValue = heap.get(i + 1);\n        int nodeId = decodeNodeId(heapValue);\n        float approxScore = decodeScore(heapValue);\n        \n        nodeIds[i] = nodeId;\n        approximateScores[i] = approxScore;\n        \n        // Track best item for fallback case\n        if (approxScore > bestApproximateScore) {\n            bestApproximateScore = approxScore;\n            bestItemIndex = i;\n        }\n        \n        if (approxScore >= rerankFloor) {\n            isAboveFloor[i] = true;\n            aboveFloorCount++;\n        } else {\n            isAboveFloor[i] = false;\n        }\n    }\n    \n    // Handle case where no items meet rerankFloor\n    if (aboveFloorCount == 0 && bestItemIndex >= 0) {\n        // Rerank the single best item\n        float exactScore = reranker.similarityTo(nodeIds[bestItemIndex]);\n        reranked.push(nodeIds[bestItemIndex], exactScore);\n        \n        // Add all other items to unused\n        for (int i = 0; i < size(); i++) {\n            if (i != bestItemIndex) {\n                unused.add(nodeIds[i], approximateScores[i]);\n            }\n        }\n        \n        return bestApproximateScore;\n    }\n    \n    // Normal case: process items above floor\n    float[] exactScores = new float[size()];\n    \n    // Compute exact scores for items above floor\n    for (int i = 0; i < size(); i++) {\n        if (isAboveFloor[i]) {\n            exactScores[i] = reranker.similarityTo(nodeIds[i]);\n        } else {\n            // Add items below floor directly to unused\n            unused.add(nodeIds[i], approximateScores[i]);\n        }\n    }\n    \n    // Select topK from items above floor using their exact scores\n    boolean[] selected = new boolean[size()];\n    int selectedCount = 0;\n    \n    for (int k = 0; k < topK && selectedCount < aboveFloorCount; k++) {\n        int bestIdx = -1;\n        float bestScore = Float.NEGATIVE_INFINITY;\n        \n        for (int i = 0; i < size(); i++) {\n            if (isAboveFloor[i] && !selected[i] && exactScores[i] > bestScore) {\n                bestScore = exactScores[i];\n                bestIdx = i;\n            }\n        }\n        \n        if (bestIdx >= 0) {\n            selected[bestIdx] = true;\n            reranked.push(nodeIds[bestIdx], exactScores[bestIdx]);\n            selectedCount++;\n        }\n    }\n    \n    // Add remaining above-floor items to unused with their approximate scores\n    for (int i = 0; i < size(); i++) {\n        if (isAboveFloor[i] && !selected[i]) {\n            unused.add(nodeIds[i], approximateScores[i]);\n        }\n    }\n    \n    // Return worst approximate score among selected topK\n    if (selectedCount < topK) {\n        return Float.POSITIVE_INFINITY;\n    }\n    \n    float worstApproximate = Float.POSITIVE_INFINITY;\n    for (int i = 0; i < size(); i++) {\n        if (selected[i]) {\n            worstApproximate = Math.min(worstApproximate, approximateScores[i]);\n        }\n    }\n    \n    return worstApproximate;\n}", "test": "@Test\npublic void testTopMinHeap() {\n    NodeQueue nn = new NodeQueue(new GrowableLongHeap(2), NodeQueue.Order.MIN_HEAP);\n    nn.push(1, 0.5f);\n    nn.push(2, -0.5f);\n    // higher scores are better; lowest score on top\n    assertEquals(-0.5f, nn.topScore(), 0);\n    assertEquals(2, nn.topNode());\n}"}, {"turn_num": 4, "instruction": "Maintain the approximate scores of reranked items in a map so that after selecting the final topK items based on exact scores, you can correctly retrieve their original approximate scores for determining the worst approximate score in the final topK.", "solution": "public float rerank(int topK, ScoreFunction.ExactScoreFunction reranker, float rerankFloor, NodeQueue reranked, NodesUnsorted unused) {\n    // Map to maintain approximate scores by node ID\n    var approximateScoresById = new Int2ObjectHashMap<Float>();\n    \n    // First pass: identify items above floor and find best item\n    int[] nodeIds = new int[size()];\n    boolean[] isAboveFloor = new boolean[size()];\n    int aboveFloorCount = 0;\n    int bestItemIndex = -1;\n    float bestApproximateScore = Float.NEGATIVE_INFINITY;\n    \n    for (int i = 0; i < size(); i++) {\n        long heapValue = heap.get(i + 1);\n        int nodeId = decodeNodeId(heapValue);\n        float approxScore = decodeScore(heapValue);\n        \n        nodeIds[i] = nodeId;\n        approximateScoresById.put(nodeId, Float.valueOf(approxScore));\n        \n        // Track best item for fallback case\n        if (approxScore > bestApproximateScore) {\n            bestApproximateScore = approxScore;\n            bestItemIndex = i;\n        }\n        \n        if (approxScore >= rerankFloor) {\n            isAboveFloor[i] = true;\n            aboveFloorCount++;\n        } else {\n            isAboveFloor[i] = false;\n        }\n    }\n    \n    // Handle case where no items meet rerankFloor\n    if (aboveFloorCount == 0 && bestItemIndex >= 0) {\n        // Rerank the single best item\n        float exactScore = reranker.similarityTo(nodeIds[bestItemIndex]);\n        reranked.push(nodeIds[bestItemIndex], exactScore);\n        \n        // Add all other items to unused\n        for (int i = 0; i < size(); i++) {\n            if (i != bestItemIndex) {\n                unused.add(nodeIds[i], approximateScoresById.get(nodeIds[i]));\n            }\n        }\n        \n        return bestApproximateScore;\n    }\n    \n    // Normal case: process items above floor\n    float[] exactScores = new float[size()];\n    \n    // Compute exact scores for items above floor\n    for (int i = 0; i < size(); i++) {\n        if (isAboveFloor[i]) {\n            exactScores[i] = reranker.similarityTo(nodeIds[i]);\n        } else {\n            // Add items below floor directly to unused\n            unused.add(nodeIds[i], approximateScoresById.get(nodeIds[i]));\n        }\n    }\n    \n    // Select topK from items above floor using their exact scores\n    boolean[] selected = new boolean[size()];\n    int selectedCount = 0;\n    \n    for (int k = 0; k < topK && selectedCount < aboveFloorCount; k++) {\n        int bestIdx = -1;\n        float bestScore = Float.NEGATIVE_INFINITY;\n        \n        for (int i = 0; i < size(); i++) {\n            if (isAboveFloor[i] && !selected[i] && exactScores[i] > bestScore) {\n                bestScore = exactScores[i];\n                bestIdx = i;\n            }\n        }\n        \n        if (bestIdx >= 0) {\n            selected[bestIdx] = true;\n            reranked.push(nodeIds[bestIdx], exactScores[bestIdx]);\n            selectedCount++;\n        }\n    }\n    \n    // Add remaining above-floor items to unused with their approximate scores\n    for (int i = 0; i < size(); i++) {\n        if (isAboveFloor[i] && !selected[i]) {\n            unused.add(nodeIds[i], approximateScoresById.get(nodeIds[i]));\n        }\n    }\n    \n    // Return worst approximate score among selected topK using the map\n    if (selectedCount < topK) {\n        return Float.POSITIVE_INFINITY;\n    }\n    \n    float worstApproximate = Float.POSITIVE_INFINITY;\n    for (int i = 0; i < size(); i++) {\n        if (selected[i]) {\n            worstApproximate = Math.min(worstApproximate, approximateScoresById.get(nodeIds[i]));\n        }\n    }\n    \n    return worstApproximate;\n}", "test": "@Test\npublic void testTopMinHeap() {\n    NodeQueue nn = new NodeQueue(new GrowableLongHeap(2), NodeQueue.Order.MIN_HEAP);\n    nn.push(1, 0.5f);\n    nn.push(2, -0.5f);\n    // higher scores are better; lowest score on top\n    assertEquals(-0.5f, nn.topScore(), 0);\n    assertEquals(2, nn.topNode());\n}"}], "metadata": {"source": "mrgbench", "func": "    public float rerank(int topK, ScoreFunction.ExactScoreFunction reranker, float rerankFloor, NodeQueue reranked, NodesUnsorted unused) {\n        // Rescore the nodes whose approximate score meets the floor.  Nodes that do not will be marked as -1\n        int[] ids = new int[size()];\n        float[] exactScores = new float[size()];\n        var approximateScoresById = new Int2ObjectHashMap<Float>();\n        float bestScore = Float.NEGATIVE_INFINITY;\n        int bestIndex = -1;\n        int scoresAboveFloor = 0;\n        for (int i = 0; i < size(); i++) {\n            long heapValue = heap.get(i + 1);\n            float score = decodeScore(heapValue);\n            var nodeId = decodeNodeId(heapValue);\n            // track the best score found so far in case nothing is above the floor\n            if (score > bestScore) {\n                bestScore = score;\n                bestIndex = i;\n            }\n\n            if (score >= rerankFloor) {\n                // rerank this one\n                ids[i] = nodeId;\n                exactScores[i] = reranker.similarityTo(ids[i]);\n                approximateScoresById.put(ids[i], Float.valueOf(score));\n                scoresAboveFloor++;\n            } else {\n                // mark it unranked\n                ids[i] = -1;\n            }\n        }\n\n        if (scoresAboveFloor == 0 && bestIndex >= 0) {\n            // if nothing was above the floor, then rerank the best one found\n            ids[bestIndex] = decodeNodeId(heap.get(bestIndex + 1));\n            exactScores[bestIndex] = reranker.similarityTo(ids[bestIndex]);\n            approximateScoresById.put(ids[bestIndex], Float.valueOf(bestScore));\n        }\n\n        // go through the entries and add to the appropriate collection\n        for (int i = 0; i < ids.length; i++) {\n            if (ids[i] == -1) {\n                unused.add(decodeNodeId(heap.get(i + 1)), decodeScore(heap.get(i + 1)));\n                continue;\n            }\n\n            // if the reranked queue is full, then either this node, or the one it replaces on the heap, will be added\n            // to the unused pile, but push() can't tell us what node was evicted when the queue was already full, so\n            // we examine that manually\n            if (reranked.size() < topK) {\n                reranked.push(ids[i], exactScores[i]);\n            } else if (exactScores[i] > reranked.topScore()) {\n                int evictedNode = reranked.topNode();\n                unused.add(evictedNode, approximateScoresById.get(evictedNode));\n                reranked.push(ids[i], exactScores[i]);\n            } else {\n                unused.add(ids[i], decodeScore(heap.get(i + 1)));\n            }\n        }\n\n        // final pass to find the worst approximate score in the topK\n        // (we can't do this as part of the earlier loops because we don't know which nodes will be in the final topK)\n        float worstApproximateInTopK = Float.POSITIVE_INFINITY;\n        if (reranked.size() < topK) {\n            return worstApproximateInTopK;\n        }\n        for (int i = 0; i < reranked.size(); i++) {\n            int nodeId = decodeNodeId(reranked.heap.get(i + 1));\n            worstApproximateInTopK = min(worstApproximateInTopK, approximateScoresById.get(nodeId));\n        }\n\n        return worstApproximateInTopK;\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeQueue.java", "func_name": "rerank", "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.graph.similarity.ScoreFunction;\nimport io.github.jbellis.jvector.util.AbstractLongHeap;\nimport io.github.jbellis.jvector.util.BoundedLongHeap;\nimport io.github.jbellis.jvector.util.NumericUtils;\nimport java.util.PrimitiveIterator;\nimport org.agrona.collections.Int2ObjectHashMap;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeQueue uses a {@link io.github.jbellis.jvector.util.AbstractLongHeap} to store lists of nodes in a graph,\n * represented as a node id with an associated score packed together as a sortable long, which is sorted\n * primarily by score. The queue {@link #push(int, float)} operation provides either fixed-size\n * or unbounded operations, depending on the implementation subclasses, and either maxheap or minheap behavior.\n */\npublic class NodeQueue {\n    public enum Order {\n        /** Smallest values at the top of the heap */\n        MIN_HEAP {\n            @Override\n            long apply(long v) {\n                return v;\n            }\n        },\n        /** Largest values at the top of the heap */\n        MAX_HEAP {\n            @Override\n            long apply(long v) {\n                // This cannot be just `-v` since Long.MIN_VALUE doesn't have a positive counterpart. It\n                // needs a function that returns MAX_VALUE for MIN_VALUE and vice-versa.\n                return -1 - v;\n            }\n        };\n\n        abstract long apply(long v);\n    }\n\n    private final AbstractLongHeap heap;\n    private final Order order;\n\n    public NodeQueue(AbstractLongHeap heap, Order order) {\n        this.heap = heap;\n        this.order = order;\n    }\n\n    /**\n     * @return the number of elements in the heap\n     */\n    public int size() {\n        return heap.size();\n    }\n\n    /**\n     * Adds a new graph node to the heap.  Will extend storage or replace the worst element\n     * depending on the type of heap it is.\n     *\n     * @param newNode  the node id\n     * @param newScore the relative similarity score to the node of the owner\n     *\n     * @return true if the new value was added.\n     */\n    public boolean push(int newNode, float newScore) {\n        return heap.push(encode(newNode, newScore));\n    }\n\n    /**\n     * Encodes then adds elements from the given iterator to this heap until elementsSize elements have been added or\n     * the iterator is exhausted. The heap then re-heapifies in O(n) time (Floyd's build-heap).\n     *\n     * @param nodeScoreIterator the node/score pairs to add\n     * @param count             the maximum number of elements to pull from the nodeScoreIterator\n     */\n    public void pushMany(NodeScoreIterator nodeScoreIterator, int count) {\n        heap.pushMany(new NodeScoreIteratorConverter(nodeScoreIterator, this), count);\n    }\n\n    /**\n     * Encodes the node ID and its similarity score as long.  If two scores are equals,\n     * the smaller node ID wins.\n     *\n     * <p>The most significant 32 bits represent the float score, encoded as a sortable int.\n     *\n     * <p>The less significant 32 bits represent the node ID.\n     *\n     * <p>The bits representing the node ID are complemented to guarantee the win for the smaller node\n     * ID.\n     *\n     * <p>The AND with 0xFFFFFFFFL (a long with first 32 bit as 1) is necessary to obtain a long that\n     * has\n     *\n     * <p>The most significant 32 bits to 0\n     *\n     * <p>The less significant 32 bits represent the node ID.\n     *\n     * @param node  the node ID\n     * @param score the node score\n     * @return the encoded score, node ID\n     */\n    private long encode(int node, float score) {\n        assert node >= 0 : node;\n        return order.apply(\n                (((long) NumericUtils.floatToSortableInt(score)) << 32) | (0xFFFFFFFFL & ~node));\n    }\n\n    private float decodeScore(long heapValue) {\n        return NumericUtils.sortableIntToFloat((int) (order.apply(heapValue) >> 32));\n    }\n\n    private int decodeNodeId(long heapValue) {\n        return (int) ~(order.apply(heapValue));\n    }\n\n    /** Removes the top element and returns its node id. */\n    public int pop() {\n        return decodeNodeId(heap.pop());\n    }\n\n    /** Returns a copy of the internal nodes array. Not sorted by score! */\n    public int[] nodesCopy() {\n        int size = size();\n        int[] nodes = new int[size];\n        for (int i = 0; i < size; i++) {\n            nodes[i] = decodeNodeId(heap.get(i + 1));\n        }\n        return nodes;\n    }", "func_start": 160, "func_end": 230, "body_len": 70, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java", "test_start": 69, "test_end": 77, "test_code": "  @Test\n  public void testTopMinHeap() {\n    NodeQueue nn = new NodeQueue(new GrowableLongHeap(2), NodeQueue.Order.MIN_HEAP);\n    nn.push(1, 0.5f);\n    nn.push(2, -0.5f);\n    // higher scores are better; lowest score on top\n    assertEquals(-0.5f, nn.topScore(), 0);\n    assertEquals(2, nn.topNode());\n  }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeQueue#testTopMinHeap\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1043}
{"hash_id": "29066589401870d85f4e87b3", "total_turn": 5, "turn_datas": [{"turn_num": 1, "instruction": "Write a static method `merge` that takes two `NodeArray` objects, `a1` and `a2`, and returns a new `NodeArray` containing all nodes from both arrays, merged in descending order by their scores. You should write self-contained code starting with: ```java\nstatic NodeArray merge(NodeArray a1, NodeArray a2) {```", "solution": "static NodeArray merge(NodeArray a1, NodeArray a2) {\n    NodeArray merged = new NodeArray(a1.size() + a2.size());\n    int i = 0, j = 0;\n\n    // Merge in descending order by score\n    while (i < a1.size() && j < a2.size()) {\n        if (a1.scores[i] > a2.scores[j]) {\n            merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            i++;\n        } else if (a1.scores[i] < a2.scores[j]) {\n            merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            j++;\n        } else {\n            // Same score - add both\n            merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            i++;\n            j++;\n        }\n    }\n\n    // Add remaining elements from a1\n    while (i < a1.size()) {\n        merged.addInOrder(a1.nodes[i], a1.scores[i]);\n        i++;\n    }\n\n    // Add remaining elements from a2\n    while (j < a2.size()) {\n        merged.addInOrder(a2.nodes[j], a2.scores[j]);\n        j++;\n    }\n\n    return merged;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1] in descending order by score\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Test with multiple elements\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(2);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1] in descending order by score\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n}"}, {"turn_num": 2, "instruction": "Modify the method to eliminate duplicate nodes (based on node identity) when they share the same score, ensuring that only one instance of each node is included per distinct score level.", "solution": "static NodeArray merge(NodeArray a1, NodeArray a2) {\n    NodeArray merged = new NodeArray(a1.size() + a2.size());\n    int i = 0, j = 0;\n\n    // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n    // we need to remember all the nodes with the current score to avoid adding duplicates\n    var nodesWithLastScore = new IntHashSet();\n    float lastAddedScore = Float.NaN;\n\n    // Merge in descending order by score\n    while (i < a1.size() && j < a2.size()) {\n        if (a1.scores[i] > a2.scores[j]) {\n            // add from a1\n            if (a1.scores[i] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a1.scores[i];\n            }\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            i++;\n        } else if (a1.scores[i] < a2.scores[j]) {\n            // add from a2\n            if (a2.scores[j] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a2.scores[j];\n            }\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            j++;\n        } else {\n            // same score -- add both if not duplicates\n            if (a1.scores[i] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a1.scores[i];\n            }\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            i++;\n            j++;\n        }\n    }\n\n    // Add remaining elements from a1\n    while (i < a1.size()) {\n        if (a1.scores[i] != lastAddedScore) {\n            nodesWithLastScore.clear();\n            lastAddedScore = a1.scores[i];\n        }\n        if (nodesWithLastScore.add(a1.nodes[i])) {\n            merged.addInOrder(a1.nodes[i], a1.scores[i]);\n        }\n        i++;\n    }\n\n    // Add remaining elements from a2\n    while (j < a2.size()) {\n        if (a2.scores[j] != lastAddedScore) {\n            nodesWithLastScore.clear();\n            lastAddedScore = a2.scores[j];\n        }\n        if (nodesWithLastScore.add(a2.nodes[j])) {\n            merged.addInOrder(a2.nodes[j], a2.scores[j]);\n        }\n        j++;\n    }\n\n    return merged;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1] - duplicates eliminated\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing duplicate elimination with same score\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2] - duplicate node 2 eliminated\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n\n    // Test multiple duplicates with same score\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(5, 5.0f);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(1, 3.0f);\n\n    arr2 = new NodeArray(2);\n    arr2.addInOrder(3, 3.0f);\n    arr2.addInOrder(2, 3.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [5, 3, 1, 2] - only one instance of node 3\n    assertArrayEquals(new int[] {5, 3, 1, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {5.0f, 3.0f, 3.0f, 3.0f}, merged.copyDenseScores(), 0.0f);\n}"}, {"turn_num": 3, "instruction": "Handle ties in score by grouping nodes with the same score together and deduplicating within that group, using a set to track node identities for the current score level.", "solution": "static NodeArray merge(NodeArray a1, NodeArray a2) {\n    NodeArray merged = new NodeArray(a1.size() + a2.size());\n    int i = 0, j = 0;\n\n    // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n    // we need to remember all the nodes with the current score to avoid adding duplicates\n    var nodesWithLastScore = new IntHashSet();\n    float lastAddedScore = Float.NaN;\n\n    // loop through both source arrays, adding the highest score element to the merged array,\n    // until we reach the end of one of the sources\n    while (i < a1.size() && j < a2.size()) {\n        if (a1.scores[i] < a2.scores[j]) {\n            // add from a2\n            if (a2.scores[j] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a2.scores[j];\n            }\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            j++;\n        } else if (a1.scores[i] > a2.scores[j]) {\n            // add from a1\n            if (a1.scores[i] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a1.scores[i];\n            }\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            i++;\n        } else {\n            // same score -- process a1 first, then a2 to maintain consistent ordering\n            if (a1.scores[i] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a1.scores[i];\n            }\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            i++;\n            j++;\n        }\n    }\n\n    // If elements remain in a1, add them\n    if (i < a1.size()) {\n        // avoid duplicates while adding nodes with the same score\n        while (i < a1.size() && a1.scores[i] == lastAddedScore) {\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            i++;\n        }\n        // the remaining nodes have a different score, so we can bulk-add them\n        if (i < a1.size()) {\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size(), a1.size() - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size(), a1.size() - i);\n            merged.size += a1.size() - i;\n        }\n    }\n\n    // If elements remain in a2, add them\n    if (j < a2.size()) {\n        // avoid duplicates while adding nodes with the same score\n        while (j < a2.size() && a2.scores[j] == lastAddedScore) {\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            j++;\n        }\n        // the remaining nodes have a different score, so we can bulk-add them\n        if (j < a2.size()) {\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size(), a2.size() - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size(), a2.size() - j);\n            merged.size += a2.size() - j;\n        }\n    }\n\n    return merged;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    // Test set-based deduplication with simpler, more predictable ordering\n    var arr1 = new NodeArray(4);\n    arr1.addInOrder(100, 5.0f);  // unique high score\n    arr1.addInOrder(200, 3.0f);  // tie group\n    arr1.addInOrder(300, 3.0f);  // tie group\n    arr1.addInOrder(400, 1.0f);  // low score\n\n    var arr2 = new NodeArray(4);\n    arr2.addInOrder(200, 3.0f);  // duplicate in tie group\n    arr2.addInOrder(500, 3.0f);  // new node in tie group\n    arr2.addInOrder(600, 3.0f);  // new node in tie group\n    arr2.addInOrder(700, 1.0f);  // low score\n\n    var merged = NodeArray.merge(arr1, arr2);\n    \n    // Expected: deduplication within score groups using set\n    // Score 5.0f: [100]\n    // Score 3.0f: [200, 300, 500, 600] (200 deduplicated)\n    // Score 1.0f: [400, 700]\n    assertArrayEquals(new int[] {100, 200, 300, 500, 600, 400, 700}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {5.0f, 3.0f, 3.0f, 3.0f, 3.0f, 1.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n    \n    // Test that set clears between different score levels\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(1, 10.0f);\n    arr1.addInOrder(2, 10.0f);\n    arr1.addInOrder(1, 5.0f);  // same node ID, different score - should appear\n\n    arr2 = new NodeArray(2);\n    arr2.addInOrder(2, 10.0f);  // duplicate at score 10.0f\n    arr2.addInOrder(3, 5.0f);   // new node at score 5.0f\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Node 1 appears twice (different scores), node 2 deduplicated at score 10.0f\n    assertArrayEquals(new int[] {1, 2, 1, 3}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {10.0f, 10.0f, 5.0f, 5.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n}"}, {"turn_num": 4, "instruction": "Ensure that even if one of the input arrays is empty, a new `NodeArray` is always returned with the non-empty array's elements, properly deduplicated and sorted in descending order.", "solution": "static NodeArray merge(NodeArray a1, NodeArray a2) {\n    // Handle empty array cases - always return a new NodeArray\n    if (a1.size() == 0 && a2.size() == 0) {\n        return new NodeArray(0);\n    }\n    if (a1.size() == 0) {\n        // Create a new NodeArray with a2's elements\n        NodeArray result = new NodeArray(a2.size());\n        System.arraycopy(a2.nodes, 0, result.nodes, 0, a2.size());\n        System.arraycopy(a2.scores, 0, result.scores, 0, a2.size());\n        result.size = a2.size();\n        return result;\n    }\n    if (a2.size() == 0) {\n        // Create a new NodeArray with a1's elements\n        NodeArray result = new NodeArray(a1.size());\n        System.arraycopy(a1.nodes, 0, result.nodes, 0, a1.size());\n        System.arraycopy(a1.scores, 0, result.scores, 0, a1.size());\n        result.size = a1.size();\n        return result;\n    }\n\n    NodeArray merged = new NodeArray(a1.size() + a2.size());\n    int i = 0, j = 0;\n\n    // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n    // we need to remember all the nodes with the current score to avoid adding duplicates\n    var nodesWithLastScore = new IntHashSet();\n    float lastAddedScore = Float.NaN;\n\n    // loop through both source arrays, adding the highest score element to the merged array,\n    // until we reach the end of one of the sources\n    while (i < a1.size() && j < a2.size()) {\n        if (a1.scores[i] < a2.scores[j]) {\n            // add from a2\n            if (a2.scores[j] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a2.scores[j];\n            }\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            j++;\n        } else if (a1.scores[i] > a2.scores[j]) {\n            // add from a1\n            if (a1.scores[i] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a1.scores[i];\n            }\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            i++;\n        } else {\n            // same score -- process a1 first, then a2 to maintain consistent ordering\n            if (a1.scores[i] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a1.scores[i];\n            }\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            i++;\n            j++;\n        }\n    }\n\n    // If elements remain in a1, add them\n    if (i < a1.size()) {\n        // avoid duplicates while adding nodes with the same score\n        while (i < a1.size() && a1.scores[i] == lastAddedScore) {\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            i++;\n        }\n        // the remaining nodes have a different score, so we can bulk-add them\n        if (i < a1.size()) {\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size(), a1.size() - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size(), a1.size() - i);\n            merged.size += a1.size() - i;\n        }\n    }\n\n    // If elements remain in a2, add them\n    if (j < a2.size()) {\n        // avoid duplicates while adding nodes with the same score\n        while (j < a2.size() && a2.scores[j] == lastAddedScore) {\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            j++;\n        }\n        // the remaining nodes have a different score, so we can bulk-add them\n        if (j < a2.size()) {\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size(), a2.size() - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size(), a2.size() - j);\n            merged.size += a2.size() - j;\n        }\n    }\n\n    return merged;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n\n    // Test empty array cases - verify NEW arrays are always returned\n    var emptyArr = new NodeArray(0);\n    var nonEmptyArr = new NodeArray(2);\n    nonEmptyArr.addInOrder(5, 5.0f);\n    nonEmptyArr.addInOrder(3, 3.0f);\n\n    // Empty first array - must return NEW array with non-empty array's elements\n    merged = NodeArray.merge(emptyArr, nonEmptyArr);\n    assertTrue(merged != nonEmptyArr);\n    assertTrue(merged != emptyArr);\n    assertArrayEquals(new int[] {5, 3}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {5.0f, 3.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Empty second array - must return NEW array with non-empty array's elements\n    merged = NodeArray.merge(nonEmptyArr, emptyArr);\n    assertTrue(merged != nonEmptyArr);\n    assertTrue(merged != emptyArr);\n    assertArrayEquals(new int[] {5, 3}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {5.0f, 3.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Both empty - should return new empty array\n    var anotherEmptyArr = new NodeArray(0);\n    merged = NodeArray.merge(emptyArr, anotherEmptyArr);\n    assertTrue(merged != emptyArr);\n    assertTrue(merged != anotherEmptyArr);\n    assertEquals(0, merged.size());\n}"}, {"turn_num": 5, "instruction": "After merging the overlapping portion, efficiently add remaining elements from either array by avoiding duplicates for nodes with the last processed score and bulk-copying the rest using System.arraycopy for performance.", "solution": "static NodeArray merge(NodeArray a1, NodeArray a2) {\n    // Handle empty array cases - always return a new NodeArray\n    if (a1.size() == 0 && a2.size() == 0) {\n        return new NodeArray(0);\n    }\n    if (a1.size() == 0) {\n        // Create a new NodeArray with a2's elements\n        NodeArray result = new NodeArray(a2.size());\n        System.arraycopy(a2.nodes, 0, result.nodes, 0, a2.size());\n        System.arraycopy(a2.scores, 0, result.scores, 0, a2.size());\n        result.size = a2.size();\n        return result;\n    }\n    if (a2.size() == 0) {\n        // Create a new NodeArray with a1's elements\n        NodeArray result = new NodeArray(a1.size());\n        System.arraycopy(a1.nodes, 0, result.nodes, 0, a1.size());\n        System.arraycopy(a1.scores, 0, result.scores, 0, a1.size());\n        result.size = a1.size();\n        return result;\n    }\n\n    NodeArray merged = new NodeArray(a1.size() + a2.size());\n    int i = 0, j = 0;\n\n    // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n    // we need to remember all the nodes with the current score to avoid adding duplicates\n    var nodesWithLastScore = new IntHashSet();\n    float lastAddedScore = Float.NaN;\n\n    // loop through both source arrays, adding the highest score element to the merged array,\n    // until we reach the end of one of the sources\n    while (i < a1.size() && j < a2.size()) {\n        if (a1.scores[i] < a2.scores[j]) {\n            // add from a2\n            if (a2.scores[j] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a2.scores[j];\n            }\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            j++;\n        } else if (a1.scores[i] > a2.scores[j]) {\n            // add from a1\n            if (a1.scores[i] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a1.scores[i];\n            }\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            i++;\n        } else {\n            // same score -- add from a2 first, then a1\n            if (a1.scores[i] != lastAddedScore) {\n                nodesWithLastScore.clear();\n                lastAddedScore = a1.scores[i];\n            }\n            if (nodesWithLastScore.add(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            if (nodesWithLastScore.add(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            i++;\n            j++;\n        }\n    }\n\n    // If elements remain in a1, add them\n    if (i < a1.size()) {\n        // avoid duplicates while adding nodes with the same score as lastAddedScore\n        while (i < a1.size() && a1.scores[i] == lastAddedScore) {\n            if (!nodesWithLastScore.contains(a1.nodes[i])) {\n                merged.addInOrder(a1.nodes[i], a1.scores[i]);\n            }\n            i++;\n        }\n        // the remaining nodes have a different score, so we can bulk-add them\n        if (i < a1.size()) {\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size, a1.size() - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size, a1.size() - i);\n            merged.size += a1.size() - i;\n        }\n    }\n\n    // If elements remain in a2, add them\n    if (j < a2.size()) {\n        // avoid duplicates while adding nodes with the same score as lastAddedScore\n        while (j < a2.size() && a2.scores[j] == lastAddedScore) {\n            if (!nodesWithLastScore.contains(a2.nodes[j])) {\n                merged.addInOrder(a2.nodes[j], a2.scores[j]);\n            }\n            j++;\n        }\n        // the remaining nodes have a different score, so we can bulk-add them\n        if (j < a2.size()) {\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size, a2.size() - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size, a2.size() - j);\n            merged.size += a2.size() - j;\n        }\n    }\n\n    return merged;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    // Test the specific optimization: bulk-copy performance after duplicate handling\n    // Create a scenario where one array is exhausted, leaving many elements in the other\n    // with both duplicates at the last processed score AND many different-score elements\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(100, 10.0f);\n\n    var arr2 = new NodeArray(10);\n    arr2.addInOrder(200, 10.0f); // duplicate with last processed score\n    arr2.addInOrder(300, 10.0f); // another duplicate with last processed score\n    // These should be bulk-copied using System.arraycopy for performance\n    arr2.addInOrder(400, 9.0f);\n    arr2.addInOrder(500, 8.0f);\n    arr2.addInOrder(600, 7.0f);\n    arr2.addInOrder(700, 6.0f);\n    arr2.addInOrder(800, 5.0f);\n    arr2.addInOrder(900, 4.0f);\n    arr2.addInOrder(1000, 3.0f);\n    arr2.addInOrder(1100, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected: [200, 100, 300, 400, 500, 600, 700, 800, 900, 1000, 1100]\n    // The key test: duplicates (200, 300) are handled individually, \n    // but elements 400-1100 should be bulk-copied for performance\n    assertArrayEquals(new int[] {200, 100, 300, 400, 500, 600, 700, 800, 900, 1000, 1100}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {10.0f, 10.0f, 10.0f, 9.0f, 8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    \n    // Test bulk copy optimization when arr1 has remaining elements\n    arr1 = new NodeArray(5);\n    arr1.addInOrder(50, 5.0f); // will match last processed score\n    arr1.addInOrder(60, 4.0f); // different score - should be bulk copied\n    arr1.addInOrder(70, 3.0f); // different score - should be bulk copied\n    arr1.addInOrder(80, 2.0f); // different score - should be bulk copied\n    arr1.addInOrder(90, 1.0f); // different score - should be bulk copied\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(20, 5.0f); // will be last processed score\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected: [20, 50, 60, 70, 80, 90]\n    // After main loop: [20, 50] (lastAddedScore = 5.0f, nodesWithLastScore = {20, 50})\n    // Processing remaining arr1: 60,70,80,90 bulk copied\n    assertArrayEquals(new int[] {20, 50, 60, 70, 80, 90}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {5.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n}"}], "metadata": {"source": "mrgbench", "func": "    static NodeArray merge(NodeArray a1, NodeArray a2) {\n        NodeArray merged = new NodeArray(a1.size() + a2.size());\n        int i = 0, j = 0;\n\n        // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n        // we need to remember all the nodes with the current score to avoid adding duplicates\n        var nodesWithLastScore = new IntHashSet();\n        float lastAddedScore = Float.NaN;\n\n        // loop through both source arrays, adding the highest score element to the merged array,\n        // until we reach the end of one of the sources\n        while (i < a1.size() && j < a2.size()) {\n            if (a1.scores[i] < a2.scores[j]) {\n                // add from a2\n                if (a2.scores[j] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a2.scores[j];\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            } else if (a1.scores[i] > a2.scores[j]) {\n                // add from a1\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            } else {\n                // same score -- add both\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                i++;\n                j++;\n            }\n        }\n\n        // If elements remain in a1, add them\n        if (i < a1.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (i < a1.size && a1.scores[i] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size, a1.size - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size, a1.size - i);\n            merged.size += a1.size - i;\n        }\n\n        // If elements remain in a2, add them\n        if (j < a2.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (j < a2.size && a2.scores[j] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size, a2.size - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size, a2.size - j);\n            merged.size += a2.size - j;\n        }\n\n        return merged;\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java", "func_name": "merge", "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.util.ArrayUtil;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.RamUsageEstimator;\nimport org.agrona.collections.IntHashSet;\n\nimport java.util.Arrays;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeArray encodes nodeids and their scores relative to some other element \n * (a query vector, or another graph node) as a pair of growable arrays. \n * Nodes are arranged in the sorted order of their scores in descending order,\n * i.e. the most-similar nodes are first.\n */\npublic class NodeArray {\n    public static final NodeArray EMPTY = new NodeArray(0);\n\n    private int size;\n    private float[] scores;\n    private int[] nodes;\n\n    public NodeArray(int initialSize) {\n        nodes = new int[initialSize];\n        scores = new float[initialSize];\n    }\n\n    // this idiosyncratic constructor exists for the benefit of subclass ConcurrentNeighborMap\n    protected NodeArray(NodeArray nodeArray) {\n        this.size = nodeArray.size();\n        this.nodes = nodeArray.nodes;\n        this.scores = nodeArray.scores;\n    }", "func_start": 63, "func_end": 143, "body_len": 80, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java", "test_start": 189, "test_end": 229, "test_code": "  @Test\n  public void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n  }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeArray#testMergeCandidatesSimple\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1044}
{"hash_id": "892c3ff660b30a2824182053", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `addInOrder(int newNode, float newScore)` that adds a new node and its corresponding score to the end of internal arrays, and increments the size of the collection. You should write self-contained code starting with: ```java\npublic void addInOrder(int newNode, float newScore) {\n}```", "solution": "public void addInOrder(int newNode, float newScore) {\n    if (size == nodes.length) {\n        growArrays();\n    }\n    nodes[size] = newNode;\n    scores[size] = newScore;\n    ++size;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n    \n    // Verify that the node and score were added correctly\n    assertEquals(1, arr1.size());\n    assertArrayEquals(new int[] {1}, arr1.copyDenseNodes());\n    assertArrayEquals(new float[] {1.0f}, arr1.copyDenseScores(), 0.0f);\n    \n    // Test adding multiple nodes\n    var arr2 = new NodeArray(3);\n    arr2.addInOrder(3, 3.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n    \n    assertEquals(3, arr2.size());\n    assertArrayEquals(new int[] {3, 2, 1}, arr2.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f, 1.0f}, arr2.copyDenseScores(), 0.0f);\n}"}, {"turn_num": 2, "instruction": "Modify the method to first check if there is enough capacity in the arrays. If the current size equals the length of the arrays, call the method `growArrays()` to resize them before adding the new elements.", "solution": "public void addInOrder(int newNode, float newScore) {\n    if (size == nodes.length) {\n        growArrays();\n    }\n    if (size > 0) {\n        float previousScore = scores[size - 1];\n        assert ((previousScore >= newScore))\n                : \"Nodes are added in the incorrect order! Comparing \"\n                + newScore\n                + \" to \"\n                + Arrays.toString(ArrayUtil.copyOfSubArray(scores, 0, size));\n    }\n    nodes[size] = newNode;\n    scores[size] = newScore;\n    ++size;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    // Create a NodeArray with exactly 1 capacity to force immediate growth\n    var arr = new NodeArray(1);\n    \n    // Fill the initial capacity\n    arr.addInOrder(3, 3.0f);\n    assertEquals(1, arr.size());\n    \n    // This second addition MUST trigger growArrays() call\n    // Previous round's code (without capacity check) would fail with ArrayIndexOutOfBoundsException\n    // Current round's code should succeed by calling growArrays()\n    arr.addInOrder(2, 2.0f);\n    assertEquals(2, arr.size());\n    \n    // Verify both elements are correctly stored after growth\n    assertArrayEquals(new int[] {3, 2}, arr.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, arr.copyDenseScores(), 0.0f);\n    \n    // Test multiple consecutive growths to ensure robustness\n    arr.addInOrder(1, 1.0f);\n    assertEquals(3, arr.size());\n    assertArrayEquals(new int[] {3, 2, 1}, arr.copyDenseNodes());\n}"}, {"turn_num": 3, "instruction": "Ensure that each new node has a score that is less than or equal to the score of the previously added node. Before inserting the new elements, if the collection is not empty, assert that the newScore is less than or equal to the score of the last added node.", "solution": "public void addInOrder(int newNode, float newScore) {\n    if (size == nodes.length) {\n        growArrays();\n    }\n    if (size > 0) {\n        float previousScore = scores[size - 1];\n        assert ((previousScore >= newScore))\n                : \"Nodes are added in the incorrect order! Comparing \"\n                + newScore\n                + \" to \"\n                + Arrays.toString(ArrayUtil.copyOfSubArray(scores, 0, size));\n    }\n    nodes[size] = newNode;\n    scores[size] = newScore;\n    ++size;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    // Test the specific assertion behavior for Round 3\n    var arr1 = new NodeArray(3);\n    arr1.addInOrder(1, 5.0f);\n    \n    // Test that assertion message includes the specific format required in Round 3\n    boolean correctAssertionMessage = false;\n    try {\n        arr1.addInOrder(2, 6.0f);  // This violates the order constraint\n    } catch (AssertionError e) {\n        String message = e.getMessage();\n        // Verify the assertion message contains the exact format from Round 3\n        correctAssertionMessage = message.contains(\"Nodes are added in the incorrect order!\") &&\n                                message.contains(\"Comparing 6.0 to\") &&\n                                message.contains(\"[5.0]\");\n    }\n    assertTrue(\"Assertion message should match Round 3 format\", correctAssertionMessage);\n    \n    // Test that the assertion specifically checks previousScore >= newScore\n    var arr2 = new NodeArray(2);\n    arr2.addInOrder(10, 10.0f);\n    \n    // This should pass because 10.0 >= 9.0 (equal scores allowed)\n    arr2.addInOrder(20, 10.0f);\n    assertEquals(2, arr2.size());\n    \n    // This should pass because 10.0 >= 8.0\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(10, 10.0f);\n    arr2.addInOrder(20, 8.0f);\n    assertEquals(2, arr2.size());\n}"}], "metadata": {"source": "mrgbench", "func": "    public void addInOrder(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        if (size > 0) {\n            float previousScore = scores[size - 1];\n            assert ((previousScore >= newScore))\n                    : \"Nodes are added in the incorrect order! Comparing \"\n                    + newScore\n                    + \" to \"\n                    + Arrays.toString(ArrayUtil.copyOfSubArray(scores, 0, size));\n        }\n        nodes[size] = newNode;\n        scores[size] = newScore;\n        ++size;\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java", "func_name": "addInOrder", "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.util.ArrayUtil;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.RamUsageEstimator;\nimport org.agrona.collections.IntHashSet;\n\nimport java.util.Arrays;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeArray encodes nodeids and their scores relative to some other element \n * (a query vector, or another graph node) as a pair of growable arrays. \n * Nodes are arranged in the sorted order of their scores in descending order,\n * i.e. the most-similar nodes are first.\n */\npublic class NodeArray {\n    public static final NodeArray EMPTY = new NodeArray(0);\n\n    private int size;\n    private float[] scores;\n    private int[] nodes;\n\n    public NodeArray(int initialSize) {\n        nodes = new int[initialSize];\n        scores = new float[initialSize];\n    }\n\n    // this idiosyncratic constructor exists for the benefit of subclass ConcurrentNeighborMap\n    protected NodeArray(NodeArray nodeArray) {\n        this.size = nodeArray.size();\n        this.nodes = nodeArray.nodes;\n        this.scores = nodeArray.scores;\n    }\n\n    /** always creates a new NodeArray to return, even when a1 or a2 is empty */\n    static NodeArray merge(NodeArray a1, NodeArray a2) {\n        NodeArray merged = new NodeArray(a1.size() + a2.size());\n        int i = 0, j = 0;\n\n        // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n        // we need to remember all the nodes with the current score to avoid adding duplicates\n        var nodesWithLastScore = new IntHashSet();\n        float lastAddedScore = Float.NaN;\n\n        // loop through both source arrays, adding the highest score element to the merged array,\n        // until we reach the end of one of the sources\n        while (i < a1.size() && j < a2.size()) {\n            if (a1.scores[i] < a2.scores[j]) {\n                // add from a2\n                if (a2.scores[j] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a2.scores[j];\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            } else if (a1.scores[i] > a2.scores[j]) {\n                // add from a1\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            } else {\n                // same score -- add both\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                i++;\n                j++;\n            }\n        }\n\n        // If elements remain in a1, add them\n        if (i < a1.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (i < a1.size && a1.scores[i] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size, a1.size - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size, a1.size - i);\n            merged.size += a1.size - i;\n        }\n\n        // If elements remain in a2, add them\n        if (j < a2.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (j < a2.size && a2.scores[j] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size, a2.size - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size, a2.size - j);\n            merged.size += a2.size - j;\n        }\n\n        return merged;\n    }", "func_start": 149, "func_end": 164, "body_len": 15, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java", "test_start": 189, "test_end": 229, "test_code": "  @Test\n  public void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n  }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeArray#testMergeCandidatesSimple\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1045}
{"hash_id": "065b76257acf1ce27409b5aa", "total_turn": 2, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `insertSorted` that takes an integer `newNode` and a float `newScore`, then inserts the node into a sorted array structure based on the score in ascending order while maintaining sorted order. You should write self-contained code starting with: ```java\npublic int insertSorted(int newNode, float newScore) {```", "solution": "public int insertSorted(int newNode, float newScore) {\n    if (size == nodes.length) {\n        growArrays();\n    }\n    \n    // Find insertion point for ascending order\n    int insertPos = 0;\n    while (insertPos < size && scores[insertPos] < newScore) {\n        insertPos++;\n    }\n    \n    // Check if node already exists at this position\n    if (insertPos < size && nodes[insertPos] == newNode && scores[insertPos] == newScore) {\n        return -1; // Node already exists\n    }\n    \n    // Shift elements to make room\n    for (int i = size; i > insertPos; i--) {\n        nodes[i] = nodes[i - 1];\n        scores[i] = scores[i - 1];\n    }\n    \n    // Insert new node and score\n    nodes[insertPos] = newNode;\n    scores[insertPos] = newScore;\n    size++;\n    \n    return insertPos;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    var arr = new NodeArray(3);\n    \n    // Test basic insertion in ascending order\n    int result1 = arr.insertSorted(1, 1.0f);\n    assertEquals(0, result1);\n    assertEquals(1, arr.size());\n    \n    int result2 = arr.insertSorted(2, 2.0f);\n    assertEquals(1, result2);\n    assertEquals(2, arr.size());\n    \n    int result3 = arr.insertSorted(0, 0.5f);\n    assertEquals(0, result3);\n    assertEquals(3, arr.size());\n    \n    // Verify ascending order: 0.5f, 1.0f, 2.0f\n    assertArrayEquals(new int[] {0, 1, 2}, arr.copyDenseNodes());\n    assertArrayEquals(new float[] {0.5f, 1.0f, 2.0f}, arr.copyDenseScores(), 0.0f);\n    \n    // Test duplicate insertion\n    int result4 = arr.insertSorted(1, 1.0f);\n    assertEquals(-1, result4); // Should return -1 for duplicate\n}"}, {"turn_num": 2, "instruction": "If a node with the same score already exists, the method should ignore the duplicate and return -1. If the array is full, it should first resize the internal arrays to accommodate more elements by calling a helper method `growArrays()`. Use a helper method `insertionPoint(newNode, newScore)` to determine the correct index for insertion, returning -1 if the score already exists. Use another helper method `insertInternal(insertionPoint, newNode, newScore)` to perform the actual shift and insertion at the determined index, and return the insertion index.", "solution": "public int insertSorted(int newNode, float newScore) {\n    if (size == nodes.length) {\n        growArrays();\n    }\n    int insertionPoint = insertionPoint(newNode, newScore);\n    if (insertionPoint == -1) {\n        return -1;\n    }\n\n    return insertInternal(insertionPoint, newNode, newScore);\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    var arr = new NodeArray(2);\n    \n    // Test normal insertion using helper methods\n    int result1 = arr.insertSorted(1, 1.0f);\n    assertTrue(result1 >= 0);\n    \n    // Test duplicate insertion - should call insertionPoint and return -1\n    int result2 = arr.insertSorted(1, 1.0f);\n    assertEquals(-1, result2);\n    \n    // Test array growth - should call growArrays() when full\n    int result3 = arr.insertSorted(2, 2.0f);\n    assertTrue(result3 >= 0);\n    \n    // Fill array to capacity to force growth\n    arr.insertSorted(3, 3.0f);\n    \n    // This insertion should trigger growArrays() and use insertInternal\n    int result4 = arr.insertSorted(4, 4.0f);\n    assertTrue(result4 >= 0);\n    \n    // Verify that insertionPoint helper correctly identifies duplicates\n    int result5 = arr.insertSorted(2, 2.0f);\n    assertEquals(-1, result5);\n    \n    // Verify insertInternal was used for actual insertions\n    assertTrue(arr.size() > 2); // Should have grown beyond initial capacity\n}"}], "metadata": {"source": "mrgbench", "func": "    public int insertSorted(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        int insertionPoint = insertionPoint(newNode, newScore);\n        if (insertionPoint == -1) {\n            return -1;\n        }\n\n        return insertInternal(insertionPoint, newNode, newScore);\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java", "func_name": "insertSorted", "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.util.ArrayUtil;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.RamUsageEstimator;\nimport org.agrona.collections.IntHashSet;\n\nimport java.util.Arrays;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeArray encodes nodeids and their scores relative to some other element \n * (a query vector, or another graph node) as a pair of growable arrays. \n * Nodes are arranged in the sorted order of their scores in descending order,\n * i.e. the most-similar nodes are first.\n */\npublic class NodeArray {\n    public static final NodeArray EMPTY = new NodeArray(0);\n\n    private int size;\n    private float[] scores;\n    private int[] nodes;\n\n    public NodeArray(int initialSize) {\n        nodes = new int[initialSize];\n        scores = new float[initialSize];\n    }\n\n    // this idiosyncratic constructor exists for the benefit of subclass ConcurrentNeighborMap\n    protected NodeArray(NodeArray nodeArray) {\n        this.size = nodeArray.size();\n        this.nodes = nodeArray.nodes;\n        this.scores = nodeArray.scores;\n    }\n\n    /** always creates a new NodeArray to return, even when a1 or a2 is empty */\n    static NodeArray merge(NodeArray a1, NodeArray a2) {\n        NodeArray merged = new NodeArray(a1.size() + a2.size());\n        int i = 0, j = 0;\n\n        // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n        // we need to remember all the nodes with the current score to avoid adding duplicates\n        var nodesWithLastScore = new IntHashSet();\n        float lastAddedScore = Float.NaN;\n\n        // loop through both source arrays, adding the highest score element to the merged array,\n        // until we reach the end of one of the sources\n        while (i < a1.size() && j < a2.size()) {\n            if (a1.scores[i] < a2.scores[j]) {\n                // add from a2\n                if (a2.scores[j] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a2.scores[j];\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            } else if (a1.scores[i] > a2.scores[j]) {\n                // add from a1\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            } else {\n                // same score -- add both\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                i++;\n                j++;\n            }\n        }\n\n        // If elements remain in a1, add them\n        if (i < a1.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (i < a1.size && a1.scores[i] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size, a1.size - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size, a1.size - i);\n            merged.size += a1.size - i;\n        }\n\n        // If elements remain in a2, add them\n        if (j < a2.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (j < a2.size && a2.scores[j] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size, a2.size - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size, a2.size - j);\n            merged.size += a2.size - j;\n        }\n\n        return merged;\n    }\n\n    /**\n     * Add a new node to the NodeArray. The new node must be worse than all previously stored\n     * nodes.\n     */\n    public void addInOrder(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        if (size > 0) {\n            float previousScore = scores[size - 1];\n            assert ((previousScore >= newScore))\n                    : \"Nodes are added in the incorrect order! Comparing \"\n                    + newScore\n                    + \" to \"\n                    + Arrays.toString(ArrayUtil.copyOfSubArray(scores, 0, size));\n        }\n        nodes[size] = newNode;\n        scores[size] = newScore;\n        ++size;\n    }\n\n    /**\n     * Returns the index at which the given node should be inserted to maintain sorted order,\n     * or -1 if the node already exists in the array (with the same score).\n     */\n    int insertionPoint(int newNode, float newScore) {\n        int insertionPoint = descSortFindRightMostInsertionPoint(newScore);\n        return duplicateExistsNear(insertionPoint, newNode, newScore) ? -1 : insertionPoint;\n    }", "func_start": 181, "func_end": 191, "body_len": 10, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java", "test_start": 189, "test_end": 229, "test_code": "  @Test\n  public void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n  }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeArray#testMergeCandidatesSimple\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1046}
{"hash_id": "eb78c124cc256ca338de74e5", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `retain(Bits selected)` that modifies the current NodeArray in place by keeping only the elements whose indices are marked in the provided BitSet. The method should preserve the original order of the retained elements and update the size of the array to reflect the number of retained elements. You should write self-contained code starting with: ```java\npublic void retain(Bits selected) {\n    // Your code here\n}```", "solution": "public void retain(Bits selected) {\n    int writeIdx = 0;\n    \n    for (int readIdx = 0; readIdx < size; readIdx++) {\n        if (selected.get(readIdx)) {\n            nodes[writeIdx] = nodes[readIdx];\n            scores[writeIdx] = scores[readIdx];\n            writeIdx++;\n        }\n    }\n    \n    size = writeIdx;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    var arr = new NodeArray(5);\n    arr.addInOrder(10, 5.0f);\n    arr.addInOrder(20, 4.0f);\n    arr.addInOrder(30, 3.0f);\n    arr.addInOrder(40, 2.0f);\n    arr.addInOrder(50, 1.0f);\n    \n    // Create a Bits implementation that selects indices 0, 2, 4\n    var selected = new io.github.jbellis.jvector.util.Bits() {\n        public boolean get(int index) {\n            return index == 0 || index == 2 || index == 4;\n        }\n        \n        public int length() {\n            return 5;\n        }\n    };\n    \n    arr.retain(selected);\n    \n    // Should retain elements at indices 0, 2, 4 (nodes 10, 30, 50)\n    assertEquals(3, arr.size());\n    assertArrayEquals(new int[] {10, 30, 50}, arr.copyDenseNodes());\n    assertArrayEquals(new float[] {5.0f, 3.0f, 1.0f}, arr.copyDenseScores(), 0.0f);\n}"}, {"turn_num": 2, "instruction": "Ensure the method does not use any additional array or data structure for storage; perform the element retention in-place by shifting elements forward within the existing arrays. Optimize the implementation to avoid unnecessary element assignments when the read and write indices are the same, i.e., skip copying an element if it would be moved to the same position.", "solution": "public void retain(Bits selected) {\n    int writeIdx = 0; // index for where to write the next retained element\n\n    for (int readIdx = 0; readIdx < size; readIdx++) {\n        if (selected.get(readIdx)) {\n            if (writeIdx != readIdx) {\n                // Move the selected entries to the front while maintaining their relative order\n                nodes[writeIdx] = nodes[readIdx];\n                scores[writeIdx] = scores[readIdx];\n            }\n            // else { we haven't created any gaps in the backing arrays yet, so we don't need to move anything }\n            writeIdx++;\n        }\n    }\n\n    size = writeIdx;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n}"}, {"turn_num": 3, "instruction": "Update the size of the array after processing to exactly match the number of retained elements, ensuring any leftover elements beyond the new size are effectively discarded and not accessible in subsequent operations.", "solution": "public void retain(Bits selected) {\n    int writeIdx = 0; // index for where to write the next retained element\n\n    for (int readIdx = 0; readIdx < size; readIdx++) {\n        if (selected.get(readIdx)) {\n            if (writeIdx != readIdx) {\n                // Move the selected entries to the front while maintaining their relative order\n                nodes[writeIdx] = nodes[readIdx];\n                scores[writeIdx] = scores[readIdx];\n            }\n            // else { we haven't created any gaps in the backing arrays yet, so we don't need to move anything }\n            writeIdx++;\n        }\n    }\n\n    // Clear leftover elements beyond the new size\n    for (int i = writeIdx; i < size; i++) {\n        nodes[i] = 0;\n        scores[i] = 0.0f;\n    }\n\n    size = writeIdx;\n}", "test": "@Test\npublic void testMergeCandidatesSimple() {\n  // Test that verifies size is updated correctly after retain operation\n  var nodeArray = new NodeArray(5);\n  nodeArray.addInOrder(5, 5.0f);\n  nodeArray.addInOrder(4, 4.0f);\n  nodeArray.addInOrder(3, 3.0f);\n  nodeArray.addInOrder(2, 2.0f);\n  nodeArray.addInOrder(1, 1.0f);\n  \n  // Create a Bits object that selects only the first 2 elements\n  var selected = new FixedBitSet(5);\n  selected.set(0);\n  selected.set(1);\n  \n  // Retain only first 2 elements\n  nodeArray.retain(selected);\n  \n  // Verify size is updated correctly to match retained elements\n  assertEquals(2, nodeArray.size());\n  \n  // Verify retained elements are correct\n  assertArrayEquals(new int[] {5, 4}, nodeArray.copyDenseNodes());\n  assertArrayEquals(new float[] {5.0f, 4.0f}, nodeArray.copyDenseScores(), 0.0f);\n  \n  // Verify that copyDenseNodes and copyDenseScores only return the retained elements\n  // This indirectly tests that leftover elements are not accessible\n  assertEquals(2, nodeArray.copyDenseNodes().length);\n  assertEquals(2, nodeArray.copyDenseScores().length);\n}"}], "metadata": {"source": "mrgbench", "func": "    public void retain(Bits selected) {\n        int writeIdx = 0; // index for where to write the next retained element\n\n        for (int readIdx = 0; readIdx < size; readIdx++) {\n            if (selected.get(readIdx)) {\n                if (writeIdx != readIdx) {\n                    // Move the selected entries to the front while maintaining their relative order\n                    nodes[writeIdx] = nodes[readIdx];\n                    scores[writeIdx] = scores[readIdx];\n                }\n                // else { we haven't created any gaps in the backing arrays yet, so we don't need to move anything }\n                writeIdx++;\n            }\n        }\n\n        size = writeIdx;\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java", "func_name": "retain", "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.util.ArrayUtil;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.RamUsageEstimator;\nimport org.agrona.collections.IntHashSet;\n\nimport java.util.Arrays;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeArray encodes nodeids and their scores relative to some other element \n * (a query vector, or another graph node) as a pair of growable arrays. \n * Nodes are arranged in the sorted order of their scores in descending order,\n * i.e. the most-similar nodes are first.\n */\npublic class NodeArray {\n    public static final NodeArray EMPTY = new NodeArray(0);\n\n    private int size;\n    private float[] scores;\n    private int[] nodes;\n\n    public NodeArray(int initialSize) {\n        nodes = new int[initialSize];\n        scores = new float[initialSize];\n    }\n\n    // this idiosyncratic constructor exists for the benefit of subclass ConcurrentNeighborMap\n    protected NodeArray(NodeArray nodeArray) {\n        this.size = nodeArray.size();\n        this.nodes = nodeArray.nodes;\n        this.scores = nodeArray.scores;\n    }\n\n    /** always creates a new NodeArray to return, even when a1 or a2 is empty */\n    static NodeArray merge(NodeArray a1, NodeArray a2) {\n        NodeArray merged = new NodeArray(a1.size() + a2.size());\n        int i = 0, j = 0;\n\n        // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n        // we need to remember all the nodes with the current score to avoid adding duplicates\n        var nodesWithLastScore = new IntHashSet();\n        float lastAddedScore = Float.NaN;\n\n        // loop through both source arrays, adding the highest score element to the merged array,\n        // until we reach the end of one of the sources\n        while (i < a1.size() && j < a2.size()) {\n            if (a1.scores[i] < a2.scores[j]) {\n                // add from a2\n                if (a2.scores[j] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a2.scores[j];\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            } else if (a1.scores[i] > a2.scores[j]) {\n                // add from a1\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            } else {\n                // same score -- add both\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                i++;\n                j++;\n            }\n        }\n\n        // If elements remain in a1, add them\n        if (i < a1.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (i < a1.size && a1.scores[i] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size, a1.size - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size, a1.size - i);\n            merged.size += a1.size - i;\n        }\n\n        // If elements remain in a2, add them\n        if (j < a2.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (j < a2.size && a2.scores[j] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size, a2.size - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size, a2.size - j);\n            merged.size += a2.size - j;\n        }\n\n        return merged;\n    }\n\n    /**\n     * Add a new node to the NodeArray. The new node must be worse than all previously stored\n     * nodes.\n     */\n    public void addInOrder(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        if (size > 0) {\n            float previousScore = scores[size - 1];\n            assert ((previousScore >= newScore))\n                    : \"Nodes are added in the incorrect order! Comparing \"\n                    + newScore\n                    + \" to \"\n                    + Arrays.toString(ArrayUtil.copyOfSubArray(scores, 0, size));\n        }\n        nodes[size] = newNode;\n        scores[size] = newScore;\n        ++size;\n    }\n\n    /**\n     * Returns the index at which the given node should be inserted to maintain sorted order,\n     * or -1 if the node already exists in the array (with the same score).\n     */\n    int insertionPoint(int newNode, float newScore) {\n        int insertionPoint = descSortFindRightMostInsertionPoint(newScore);\n        return duplicateExistsNear(insertionPoint, newNode, newScore) ? -1 : insertionPoint;\n    }\n\n    /**\n     * Add a new node to the NodeArray into a correct sort position according to its score.\n     * Duplicate node + score pairs are ignored.\n     *\n     * @return the insertion point of the new node, or -1 if it already existed\n     */\n    public int insertSorted(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        int insertionPoint = insertionPoint(newNode, newScore);\n        if (insertionPoint == -1) {\n            return -1;\n        }\n\n        return insertInternal(insertionPoint, newNode, newScore);\n    }\n\n    /**\n     * Add a new node to the NodeArray into the specified insertion point.\n     */\n    void insertAt(int insertionPoint, int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        insertInternal(insertionPoint, newNode, newScore);\n    }\n\n    private int insertInternal(int insertionPoint, int newNode, float newScore) {\n        System.arraycopy(nodes, insertionPoint, nodes, insertionPoint + 1, size - insertionPoint);\n        System.arraycopy(scores, insertionPoint, scores, insertionPoint + 1, size - insertionPoint);\n        nodes[insertionPoint] = newNode;\n        scores[insertionPoint] = newScore;\n        ++size;\n        return insertionPoint;\n    }\n\n    private boolean duplicateExistsNear(int insertionPoint, int newNode, float newScore) {\n        // Check to the left\n        for (int i = insertionPoint - 1; i >= 0 && scores[i] == newScore; i--) {\n            if (nodes[i] == newNode) {\n                return true;\n            }\n        }\n\n        // Check to the right\n        for (int i = insertionPoint; i < size && scores[i] == newScore; i++) {\n            if (nodes[i] == newNode) {\n                return true;\n            }\n        }\n\n        return false;\n    }", "func_start": 240, "func_end": 256, "body_len": 16, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java", "test_start": 189, "test_end": 229, "test_code": "  @Test\n  public void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n  }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeArray#testMergeCandidatesSimple\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1047}
{"hash_id": "fa45139dab549d317045f5c4", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `refine` in the `ProductQuantization` class that takes a `RandomAccessVectorValues` object, an integer `lloydsRounds`, a float `anisotropicThreshold`, and two `ForkJoinPool` executors (`simdExecutor` and `parallelExecutor`). The method should validate that `lloydsRounds` is non-negative, compute subvector sizes and offsets using `getSubvectorSizesAndOffsets` based on the input dimension and number of subquantizers `M`, extract training vectors from the input using `extractTrainingVectors`, and return a new `ProductQuantization` instance with updated codebooks generated by applying K-means clustering on subvectors using `KMeansPlusPlusClusterer`, preserving all other parameters like `clusterCount` and `globalCentroid`. You should write self-contained code starting with: ```java\npublic ProductQuantization refine(RandomAccessVectorValues ravv, int lloydsRounds, float anisotropicThreshold, ForkJoinPool simdExecutor, ForkJoinPool parallelExecutor)```", "solution": "public ProductQuantization refine(RandomAccessVectorValues ravv, int lloydsRounds, float anisotropicThreshold, ForkJoinPool simdExecutor, ForkJoinPool parallelExecutor) {\n    if (lloydsRounds < 0) {\n        throw new IllegalArgumentException(\"lloydsRounds must be non-negative\");\n    }\n\n    var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n    var vectors = extractTrainingVectors(ravv, parallelExecutor);\n    \n    var refinedCodebooks = new VectorFloat<?>[M];\n    for (int m = 0; m < M; m++) {\n        VectorFloat<?>[] subvectors = extractSubvectors(vectors, m, subvectorSizesAndOffsets);\n        var clusterer = new KMeansPlusPlusClusterer(subvectors, codebooks[m], anisotropicThreshold);\n        refinedCodebooks[m] = clusterer.cluster(lloydsRounds, 0);\n    }\n\n    return new ProductQuantization(refinedCodebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n}", "test": "@Test\npublic void testSaveLoad() throws Exception {\n    // Generate a PQ for random 2D vectors\n    var vectors = createRandomVectors(512, 2);\n    var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n    \n    // Test refine method with basic parameters\n    var refinedPq = pq.refine(new ListRandomAccessVectorValues(vectors, 2), 5, 0.2f, ForkJoinPool.commonPool(), ForkJoinPool.commonPool());\n    \n    // Verify that refine returns a new ProductQuantization instance\n    Assertions.assertNotNull(refinedPq);\n    Assertions.assertNotSame(pq, refinedPq);\n    \n    // Test that negative lloydsRounds throws exception\n    Assertions.assertThrows(IllegalArgumentException.class, () -> {\n        pq.refine(new ListRandomAccessVectorValues(vectors, 2), -1, 0.2f, ForkJoinPool.commonPool(), ForkJoinPool.commonPool());\n    });\n}"}, {"turn_num": 2, "instruction": "Modify the method to center the training vectors by subtracting the `globalCentroid` from each vector if `globalCentroid` is not null, using the `simdExecutor` for parallel execution via a submitted task, and ensure the centered vectors are used in subsequent clustering steps.", "solution": "public ProductQuantization refine(RandomAccessVectorValues ravv, int lloydsRounds, float anisotropicThreshold, ForkJoinPool simdExecutor, ForkJoinPool parallelExecutor) {\n    if (lloydsRounds < 0) {\n        throw new IllegalArgumentException(\"lloydsRounds must be non-negative\");\n    }\n\n    var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n    var vectorsMutable = extractTrainingVectors(ravv, parallelExecutor);\n    \n    if (globalCentroid != null) {\n        var vectors = vectorsMutable;\n        vectorsMutable = simdExecutor.submit(() -> vectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n    }\n    var vectors = vectorsMutable;\n    \n    var refinedCodebooks = new VectorFloat<?>[M];\n    for (int m = 0; m < M; m++) {\n        VectorFloat<?>[] subvectors = extractSubvectors(vectors, m, subvectorSizesAndOffsets);\n        var clusterer = new KMeansPlusPlusClusterer(subvectors, codebooks[m], anisotropicThreshold);\n        refinedCodebooks[m] = clusterer.cluster(lloydsRounds, 0);\n    }\n\n    return new ProductQuantization(refinedCodebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n}", "test": "@Test\npublic void testSaveLoad() throws Exception {\n    // Generate a PQ for random 2D vectors\n    var vectors = createRandomVectors(512, 2);\n    var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n\n    // Write\n    var file = File.createTempFile(\"pqtest\", \".pq\");\n    try (var out = new DataOutputStream(new FileOutputStream(file))) {\n        pq.write(out);\n    }\n    // Read\n    try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n        var pq2 = ProductQuantization.load(readerSupplier.get());\n        Assertions.assertEquals(pq, pq2);\n    }\n}"}, {"turn_num": 3, "instruction": "Ensure that the subvectors are extracted for each subquantizer using a helper method `extractSubvectors` that takes the full vectors, the subquantizer index `m`, and the precomputed `subvectorSizesAndOffsets`, and apply K-means clustering independently per subquantizer in parallel using `simdExecutor` and `IntStream.range(0, M).parallel()`.", "solution": "public ProductQuantization refine(RandomAccessVectorValues ravv, int lloydsRounds, float anisotropicThreshold, ForkJoinPool simdExecutor, ForkJoinPool parallelExecutor) {\n    if (lloydsRounds < 0) {\n        throw new IllegalArgumentException(\"lloydsRounds must be non-negative\");\n    }\n\n    var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n    var vectorsMutable = extractTrainingVectors(ravv, parallelExecutor);\n    \n    if (globalCentroid != null) {\n        var vectors = vectorsMutable;\n        vectorsMutable = simdExecutor.submit(() -> vectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n    }\n    var vectors = vectorsMutable;\n    \n    var refinedCodebooks = simdExecutor.submit(() -> IntStream.range(0, M).parallel().mapToObj(m -> {\n        VectorFloat<?>[] subvectors = extractSubvectors(vectors, m, subvectorSizesAndOffsets);\n        var clusterer = new KMeansPlusPlusClusterer(subvectors, codebooks[m], anisotropicThreshold);\n        return clusterer.cluster(anisotropicThreshold == UNWEIGHTED ? lloydsRounds : 0,\n                                 anisotropicThreshold == UNWEIGHTED ? 0 : lloydsRounds);\n    }).toArray(VectorFloat<?>[]::new)).join();\n\n    return new ProductQuantization(refinedCodebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n}", "test": "@Test\npublic void testSaveLoad() throws Exception {\n    // Generate vectors for testing parallel refine functionality\n    var vectors = createRandomVectors(100, 8);\n    var ravv = new ListRandomAccessVectorValues(vectors, 8);\n    \n    // Create initial PQ with multiple subquantizers to test parallel execution\n    var pq = ProductQuantization.compute(ravv, 2, 4, false, 0.2f);\n    \n    // Create executors for parallel processing\n    var simdExecutor = ForkJoinPool.commonPool();\n    var parallelExecutor = ForkJoinPool.commonPool();\n    \n    // Test that refine works with parallel execution and extractSubvectors\n    var refinedPq = pq.refine(ravv, 1, 0.2f, simdExecutor, parallelExecutor);\n    \n    // Verify the refined PQ has the same structure\n    Assertions.assertEquals(pq.M, refinedPq.M);\n    Assertions.assertNotNull(refinedPq.codebooks);\n    Assertions.assertEquals(pq.codebooks.length, refinedPq.codebooks.length);\n    \n    // Verify each codebook is not null and has proper structure\n    for (int m = 0; m < refinedPq.M; m++) {\n        Assertions.assertNotNull(refinedPq.codebooks[m]);\n        Assertions.assertTrue(refinedPq.codebooks[m].length() > 0);\n    }\n}"}, {"turn_num": 4, "instruction": "Conditionally apply Lloyd's algorithm iterations during clustering: if `anisotropicThreshold` is equal to the constant `UNWEIGHTED`, perform all `lloydsRounds` iterations in the refinement phase; otherwise, skip Lloyd's iterations in the initial phase and perform them only in the second phase by passing `lloydsRounds` accordingly to the `cluster` method of `KMeansPlusPlusClusterer`.", "solution": "public ProductQuantization refine(RandomAccessVectorValues ravv,\n                                  int lloydsRounds,\n                                  float anisotropicThreshold,\n                                  ForkJoinPool simdExecutor,\n                                  ForkJoinPool parallelExecutor)\n{\n    if (lloydsRounds < 0) {\n        throw new IllegalArgumentException(\"lloydsRounds must be non-negative\");\n    }\n\n    var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n    var vectorsMutable = extractTrainingVectors(ravv, parallelExecutor);\n    if (globalCentroid != null) {\n        var vectors = vectorsMutable;\n        vectorsMutable = simdExecutor.submit(() -> vectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n    }\n    var vectors = vectorsMutable; // \"effectively final\" to make the closure happy\n\n    var refinedCodebooks = simdExecutor.submit(() -> IntStream.range(0, M).parallel().mapToObj(m -> {\n        VectorFloat<?>[] subvectors = extractSubvectors(vectors, m, subvectorSizesAndOffsets);\n        var clusterer = new KMeansPlusPlusClusterer(subvectors, codebooks[m], anisotropicThreshold);\n        if (anisotropicThreshold == UNWEIGHTED) {\n            return clusterer.cluster(lloydsRounds, 0);\n        } else {\n            return clusterer.cluster(0, lloydsRounds);\n        }\n    }).toArray(VectorFloat<?>[]::new)).join();\n\n    return new ProductQuantization(refinedCodebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n}", "test": "@Test\npublic void testSaveLoad() throws Exception {\n    // Generate a PQ for random 2D vectors\n    var vectors = createRandomVectors(512, 2);\n    var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n    \n    // Test Lloyd's algorithm behavior with different thresholds\n    var refinedWeighted = pq.refine(new ListRandomAccessVectorValues(vectors, 2), 3, 0.5f, ForkJoinPool.commonPool(), ForkJoinPool.commonPool());\n    \n    // Verify that refinement produces valid results\n    Assertions.assertNotNull(refinedWeighted);\n    \n    // Write\n    var file = File.createTempFile(\"pqtest\", \".pq\");\n    try (var out = new DataOutputStream(new FileOutputStream(file))) {\n        refinedWeighted.write(out);\n    }\n    // Read\n    try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n        var pq2 = ProductQuantization.load(readerSupplier.get());\n        Assertions.assertEquals(refinedWeighted, pq2);\n    }\n}"}], "metadata": {"source": "mrgbench", "func": "    public ProductQuantization refine(RandomAccessVectorValues ravv,\n                                      int lloydsRounds,\n                                      float anisotropicThreshold,\n                                      ForkJoinPool simdExecutor,\n                                      ForkJoinPool parallelExecutor)\n    {\n        if (lloydsRounds < 0) {\n            throw new IllegalArgumentException(\"lloydsRounds must be non-negative\");\n        }\n\n        var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n        var vectorsMutable = extractTrainingVectors(ravv, parallelExecutor);\n        if (globalCentroid != null) {\n            var vectors = vectorsMutable;\n            vectorsMutable = simdExecutor.submit(() -> vectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n        }\n        var vectors = vectorsMutable; // \"effectively final\" to make the closure happy\n\n        var refinedCodebooks = simdExecutor.submit(() -> IntStream.range(0, M).parallel().mapToObj(m -> {\n            VectorFloat<?>[] subvectors = extractSubvectors(vectors, m, subvectorSizesAndOffsets);\n            var clusterer = new KMeansPlusPlusClusterer(subvectors, codebooks[m], anisotropicThreshold);\n            return clusterer.cluster(anisotropicThreshold == UNWEIGHTED ? lloydsRounds : 0,\n                                     anisotropicThreshold == UNWEIGHTED ? 0 : lloydsRounds);\n        }).toArray(VectorFloat<?>[]::new)).join();\n\n        return new ProductQuantization(refinedCodebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java", "func_name": "refine", "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.quantization;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.disk.RandomAccessReader;\nimport io.github.jbellis.jvector.graph.RandomAccessVectorValues;\nimport io.github.jbellis.jvector.graph.disk.OnDiskGraphIndex;\nimport io.github.jbellis.jvector.util.Accountable;\nimport io.github.jbellis.jvector.util.PhysicalCoreExecutor;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.VectorUtil;\nimport io.github.jbellis.jvector.vector.VectorizationProvider;\nimport io.github.jbellis.jvector.vector.types.ByteSequence;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\nimport java.io.DataOutput;\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Objects;\nimport java.util.concurrent.ForkJoinPool;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Supplier;\nimport java.util.logging.Logger;\nimport java.util.stream.Collectors;\nimport java.util.stream.IntStream;\n\nimport static io.github.jbellis.jvector.quantization.KMeansPlusPlusClusterer.UNWEIGHTED;\nimport static io.github.jbellis.jvector.util.MathUtil.square;\nimport static io.github.jbellis.jvector.vector.VectorUtil.dotProduct;\nimport static io.github.jbellis.jvector.vector.VectorUtil.sub;\nimport static java.lang.Math.min;\nimport static java.lang.Math.sqrt;\n\n/**\n * Product Quantization for float vectors.  Supports arbitrary source and target dimensionality;\n * in particular, the source does not need to be evenly divisible by the target.\n */\npublic class ProductQuantization implements VectorCompressor<ByteSequence<?>>, Accountable {\n    private static final int MAGIC = 0x75EC4012; // JVECTOR, with some imagination\n\n    protected static final Logger LOG = Logger.getLogger(ProductQuantization.class.getName());\n\n    private static final VectorTypeSupport vectorTypeSupport = VectorizationProvider.getInstance().getVectorTypeSupport();\n    static final int DEFAULT_CLUSTERS = 256; // number of clusters per subspace = one byte's worth\n    static final int K_MEANS_ITERATIONS = 6;\n    public static final int MAX_PQ_TRAINING_SET_SIZE = 128000;\n\n    final VectorFloat<?>[] codebooks; // array of codebooks, where each codebook is a VectorFloat consisting of k contiguous subvectors each of length M\n    final int M; // codebooks.length, redundantly reproduced for convenience\n    private final int clusterCount; // codebooks[0].length, redundantly reproduced for convenience\n    final int originalDimension;\n    final VectorFloat<?> globalCentroid;\n    final int[][] subvectorSizesAndOffsets;\n    final float anisotropicThreshold; // parallel cost multiplier\n    private final float[][] centroidNormsSquared; // precomputed norms of the centroids, for encoding\n    private final ThreadLocal<VectorFloat<?>> partialSums; // for dot product, euclidean, and cosine partials\n    private final ThreadLocal<VectorFloat<?>> partialBestDistances; // for partial best distances during fused ADC\n    private final ThreadLocal<ByteSequence<?>> partialQuantizedSums; // for quantized sums during fused ADC\n    private final AtomicReference<VectorFloat<?>> partialSquaredMagnitudes; // for cosine partials\n    private final AtomicReference<ByteSequence<?>> partialQuantizedSquaredMagnitudes; // for quantized squared magnitude partials during cosine fused ADC\n    protected volatile float squaredMagnitudeDelta = 0; // for cosine fused ADC squared magnitude quantization delta (since this is invariant for a given PQ)\n    protected volatile float minSquaredMagnitude = 0; // for cosine fused ADC minimum squared magnitude (invariant for a given PQ)\n\n    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     */\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter) {\n        return compute(ravv, M, clusterCount, globallyCenter, UNWEIGHTED, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter, float anisotropicThreshold) {\n        return compute(ravv, M, clusterCount, globallyCenter, anisotropicThreshold, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param clusterCount number of clusters per subspace\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     * @param anisotropicThreshold the threshold of relevance for anisotropic angular distance shaping, giving\n     *        higher priority to parallel error.  Anisotropic shaping requires that your dataset be normalized\n     *        to unit length.  Use a threshold of UNWEIGHTED for isotropic distance\n     *        (i.e. normal, unweighted L2 distance).\n     * @param simdExecutor     ForkJoinPool instance for SIMD operations, best is to use a pool with the size of\n     *                         the number of physical cores.\n     * @param parallelExecutor ForkJoinPool instance for parallel stream operations\n     */\n    public static ProductQuantization compute(RandomAccessVectorValues ravv,\n                                              int M,\n                                              int clusterCount,\n                                              boolean globallyCenter,\n                                              float anisotropicThreshold,\n                                              ForkJoinPool simdExecutor,\n                                              ForkJoinPool parallelExecutor)\n    {\n        checkClusterCount(clusterCount);\n\n        var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n        var vectors = extractTrainingVectors(ravv, parallelExecutor);\n\n        // subtract the centroid from each training vector\n        VectorFloat<?> globalCentroid;\n        if (globallyCenter) {\n            globalCentroid = KMeansPlusPlusClusterer.centroidOf(vectors);\n            // subtract the centroid from each vector\n            List<VectorFloat<?>> finalVectors = vectors;\n            vectors = simdExecutor.submit(() -> finalVectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n        } else {\n            globalCentroid = null;\n        }\n\n        // derive the codebooks\n        var codebooks = createCodebooks(vectors, subvectorSizesAndOffsets, clusterCount, anisotropicThreshold, simdExecutor);\n        return new ProductQuantization(codebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n    }\n\n    static List<VectorFloat<?>> extractTrainingVectors(RandomAccessVectorValues ravv, ForkJoinPool parallelExecutor) {\n        // limit the number of vectors we train on\n        var P = min(1.0f, MAX_PQ_TRAINING_SET_SIZE / (float) ravv.size());\n        var ravvCopy = ravv.threadLocalSupplier();\n        return parallelExecutor.submit(() -> IntStream.range(0, ravv.size()).parallel()\n                        .filter(i -> ThreadLocalRandom.current().nextFloat() < P)\n                        .mapToObj(targetOrd -> {\n                            var localRavv = ravvCopy.get();\n                            VectorFloat<?> v = localRavv.getVector(targetOrd);\n                            return localRavv.isValueShared() ? v.copy() : v;\n                        })\n                        .collect(Collectors.toList()))\n                .join();\n    }\n\n    /**\n     * Create a new PQ by fine-tuning this one with the data in `ravv`\n     */\n    public ProductQuantization refine(RandomAccessVectorValues ravv) {\n        return refine(ravv, 1, UNWEIGHTED, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }", "func_start": 171, "func_end": 197, "body_len": 26, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java", "test_start": 198, "test_end": 214, "test_code": "    @Test\n    public void testSaveLoad() throws Exception {\n        // Generate a PQ for random 2D vectors\n        var vectors = createRandomVectors(512, 2);\n        var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n\n        // Write\n        var file = File.createTempFile(\"pqtest\", \".pq\");\n        try (var out = new DataOutputStream(new FileOutputStream(file))) {\n            pq.write(out);\n        }\n        // Read\n        try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n            var pq2 = ProductQuantization.load(readerSupplier.get());\n            Assertions.assertEquals(pq, pq2);\n        }\n    }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.quantization.TestProductQuantization#testSaveLoad\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1049}
{"hash_id": "de0fedd9a18a6ee04611cd53", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a private method `encodeAnisotropic` that takes a floating-point vector and a byte sequence as input, and stores an initial encoding of the vector into the byte sequence by selecting, for each subvector, the codebook centroid that minimizes the residual norm. You should write self-contained code starting with: ```java\nprivate void encodeAnisotropic(VectorFloat<?> vector, ByteSequence<?> result) {\n}```", "solution": "private void encodeAnisotropic(VectorFloat<?> vector, ByteSequence<?> result) {\n    // compute the residuals from each subvector to each corresponding codebook centroid\n    Residual[][] residuals = computeResiduals(vector);\n    assert residuals.length == M : \"Residuals length mismatch \" + residuals.length + \" != \" + M;\n    // start with centroids that minimize the residual norms\n    initializeToMinResidualNorms(residuals, result);\n}", "test": "@Test\npublic void testSaveLoad() throws Exception {\n    // Generate a PQ for random 2D vectors\n    var vectors = createRandomVectors(512, 2);\n    var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n\n    // Write\n    var file = File.createTempFile(\"pqtest\", \".pq\");\n    try (var out = new DataOutputStream(new FileOutputStream(file))) {\n        pq.write(out);\n    }\n    // Read\n    try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n        var pq2 = ProductQuantization.load(readerSupplier.get());\n        Assertions.assertEquals(pq, pq2);\n    }\n}"}, {"turn_num": 2, "instruction": "Modify the method to compute and maintain a cumulative sum of the parallel residual component from the selected centroids after the initial encoding.", "solution": "private void encodeAnisotropic(VectorFloat<?> vector, ByteSequence<?> result) {\n    // compute the residuals from each subvector to each corresponding codebook centroid\n    Residual[][] residuals = computeResiduals(vector);\n    assert residuals.length == M : \"Residuals length mismatch \" + residuals.length + \" != \" + M;\n    // start with centroids that minimize the residual norms\n    initializeToMinResidualNorms(residuals, result);\n    // sum the initial parallel residual component\n    float parallelResidualComponentSum = 0;\n    for (int i = 0; i < result.length(); i++) {\n        int centroidIdx = Byte.toUnsignedInt(result.get(i));\n        parallelResidualComponentSum += residuals[i][centroidIdx].parallelResidualComponent;\n    }\n}", "test": "@Test\npublic void testSaveLoad() throws Exception {\n    // Generate a PQ for random 2D vectors to test cumulative sum computation\n    var vectors = createRandomVectors(512, 2);\n    var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n    \n    // Test that encoding works with cumulative sum computation\n    var testVector = vectors.get(0);\n    var encoded = pq.encode(testVector);\n    Assertions.assertNotNull(encoded, \"Encoding should succeed with cumulative sum computation\");\n    Assertions.assertEquals(1, encoded.length(), \"Encoded result should have correct length\");\n    \n    // Write and read to verify the PQ structure is maintained\n    var file = File.createTempFile(\"pqtest\", \".pq\");\n    try (var out = new DataOutputStream(new FileOutputStream(file))) {\n        pq.write(out);\n    }\n    try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n        var pq2 = ProductQuantization.load(readerSupplier.get());\n        Assertions.assertEquals(pq, pq2);\n    }\n}"}, {"turn_num": 3, "instruction": "Add an iterative refinement loop that runs for a maximum of 10 iterations, where in each iteration, every subspace is optimized by considering how changing the centroid affects the total parallel residual component, using a helper method `optimizeSingleSubspace` that returns the new centroid index and updated parallel residual sum.", "solution": "private void encodeAnisotropic(VectorFloat<?> vector, ByteSequence<?> result) {\n    // compute the residuals from each subvector to each corresponding codebook centroid\n    Residual[][] residuals = computeResiduals(vector);\n    assert residuals.length == M : \"Residuals length mismatch \" + residuals.length + \" != \" + M;\n    // start with centroids that minimize the residual norms\n    initializeToMinResidualNorms(residuals, result);\n    // sum the initial parallel residual component\n    float parallelResidualComponentSum = 0;\n    for (int i = 0; i < result.length(); i++) {\n        int centroidIdx = Byte.toUnsignedInt(result.get(i));\n        parallelResidualComponentSum += residuals[i][centroidIdx].parallelResidualComponent;\n    }\n\n    // Iterative refinement loop for maximum 10 iterations\n    int MAX_ITERATIONS = 10;\n    for (int iter = 0; iter < MAX_ITERATIONS; iter++) {\n        boolean changed = false;\n        for (int i = 0; i < residuals.length; i++) {\n            int oldIdx = Byte.toUnsignedInt(result.get(i));\n            CoordinateDescentResult cdr = optimizeSingleSubspace(residuals[i], oldIdx, parallelResidualComponentSum);\n            if (cdr.newCenterIdx != oldIdx) {\n                parallelResidualComponentSum = cdr.newParallelResidualComponent;\n                result.set(i, (byte) cdr.newCenterIdx);\n                changed = true;\n            }\n        }\n        if (!changed) {\n            break;\n        }\n    }\n}", "test": "@Test\npublic void testSaveLoad() throws Exception {\n    // Create larger, more diverse vectors to ensure iterative refinement has opportunity to optimize\n    var vectors = createRandomVectors(100, 8);\n    var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 8), 4, 16, false, 0.2f);\n    \n    // Test that the iterative refinement loop executes by encoding multiple different vectors\n    // and verifying that the encoding process completes successfully\n    var testVector1 = vectors.get(0);\n    var testVector2 = vectors.get(1);\n    \n    // Encode different vectors - the iterative refinement should handle both\n    var result1 = pq.encode(testVector1);\n    var result2 = pq.encode(testVector2);\n    \n    // Verify that encoding produces valid results (correct length)\n    Assertions.assertEquals(4, result1.length(), \"Result should have correct number of subspaces\");\n    Assertions.assertEquals(4, result2.length(), \"Result should have correct number of subspaces\");\n    \n    // Verify that the iterative refinement produces deterministic results for the same input\n    var result1_repeat = pq.encode(testVector1);\n    Assertions.assertEquals(result1, result1_repeat, \"Iterative refinement should be deterministic\");\n}"}, {"turn_num": 4, "instruction": "Update the method to track whether any centroid index changes during an iteration, and terminate the loop early if no changes occur, indicating convergence.", "solution": "private void encodeAnisotropic(VectorFloat<?> vector, ByteSequence<?> result) {\n    // compute the residuals from each subvector to each corresponding codebook centroid\n    Residual[][] residuals = computeResiduals(vector);\n    assert residuals.length == M : \"Residuals length mismatch \" + residuals.length + \" != \" + M;\n    // start with centroids that minimize the residual norms\n    initializeToMinResidualNorms(residuals, result);\n    // sum the initial parallel residual component\n    float parallelResidualComponentSum = 0;\n    for (int i = 0; i < result.length(); i++) {\n        int centroidIdx = Byte.toUnsignedInt(result.get(i));\n        parallelResidualComponentSum += residuals[i][centroidIdx].parallelResidualComponent;\n    }\n\n    // Optimize until convergence\n    int MAX_ITERATIONS = 10;\n    for (int iter = 0; iter < MAX_ITERATIONS; iter++) {\n        // Track whether any centroid index changes during this iteration\n        boolean changed = false;\n        for (int i = 0; i < residuals.length; i++) {\n            int oldIdx = Byte.toUnsignedInt(result.get(i));\n            CoordinateDescentResult cdr = optimizeSingleSubspace(residuals[i], oldIdx, parallelResidualComponentSum);\n            if (cdr.newCenterIdx != oldIdx) {\n                parallelResidualComponentSum = cdr.newParallelResidualComponent;\n                result.set(i, (byte) cdr.newCenterIdx);\n                changed = true;\n            }\n        }\n        // Terminate early if no changes occur, indicating convergence\n        if (!changed) {\n            break;\n        }\n    }\n}", "test": "@Test\npublic void testSaveLoad() throws Exception {\n    // Generate a PQ for random 2D vectors\n    var vectors = createRandomVectors(512, 2);\n    var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n\n    // Write\n    var file = File.createTempFile(\"pqtest\", \".pq\");\n    try (var out = new DataOutputStream(new FileOutputStream(file))) {\n        pq.write(out);\n    }\n    // Read\n    try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n        var pq2 = ProductQuantization.load(readerSupplier.get());\n        Assertions.assertEquals(pq, pq2);\n    }\n}"}], "metadata": {"source": "mrgbench", "func": "    private void encodeAnisotropic(VectorFloat<?> vector, ByteSequence<?> result) {\n        // compute the residuals from each subvector to each corresponding codebook centroid\n        Residual[][] residuals = computeResiduals(vector);\n        assert residuals.length == M : \"Residuals length mismatch \" + residuals.length + \" != \" + M;\n        // start with centroids that minimize the residual norms\n        initializeToMinResidualNorms(residuals, result);\n        // sum the initial parallel residual component\n        float parallelResidualComponentSum = 0;\n        for (int i = 0; i < result.length(); i++) {\n            int centroidIdx = Byte.toUnsignedInt(result.get(i));\n            parallelResidualComponentSum += residuals[i][centroidIdx].parallelResidualComponent;\n        }\n\n        // SCANN sorts the subspaces by residual norm here (and adds a sorted->original subspace index map),\n        // presumably with the intent to help this converge faster, but profiling shows that almost 90% of the\n        // cost of this method is computeResiduals + initializeToMinResidualNorms, so we're not going to bother.\n\n        // Optimize until convergence\n        int MAX_ITERATIONS = 10; // borrowed from SCANN code without experimenting w/ other values\n        for (int iter = 0; iter < MAX_ITERATIONS; iter++) {\n            // loop over each subspace\n            boolean changed = false;\n            for (int i = 0; i < residuals.length; i++) {\n                int oldIdx = Byte.toUnsignedInt(result.get(i));\n                CoordinateDescentResult cdr = optimizeSingleSubspace(residuals[i], oldIdx, parallelResidualComponentSum);\n                if (cdr.newCenterIdx != oldIdx) {\n                    parallelResidualComponentSum = cdr.newParallelResidualComponent;\n                    result.set(i, (byte) cdr.newCenterIdx);\n                    changed = true;\n                }\n            }\n            // Done if nothing changed this iteration\n            if (!changed) {\n                break;\n            }\n        }\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java", "func_name": "encodeAnisotropic", "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.quantization;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.disk.RandomAccessReader;\nimport io.github.jbellis.jvector.graph.RandomAccessVectorValues;\nimport io.github.jbellis.jvector.graph.disk.OnDiskGraphIndex;\nimport io.github.jbellis.jvector.util.Accountable;\nimport io.github.jbellis.jvector.util.PhysicalCoreExecutor;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.VectorUtil;\nimport io.github.jbellis.jvector.vector.VectorizationProvider;\nimport io.github.jbellis.jvector.vector.types.ByteSequence;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\nimport java.io.DataOutput;\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Objects;\nimport java.util.concurrent.ForkJoinPool;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Supplier;\nimport java.util.logging.Logger;\nimport java.util.stream.Collectors;\nimport java.util.stream.IntStream;\n\nimport static io.github.jbellis.jvector.quantization.KMeansPlusPlusClusterer.UNWEIGHTED;\nimport static io.github.jbellis.jvector.util.MathUtil.square;\nimport static io.github.jbellis.jvector.vector.VectorUtil.dotProduct;\nimport static io.github.jbellis.jvector.vector.VectorUtil.sub;\nimport static java.lang.Math.min;\nimport static java.lang.Math.sqrt;\n\n/**\n * Product Quantization for float vectors.  Supports arbitrary source and target dimensionality;\n * in particular, the source does not need to be evenly divisible by the target.\n */\npublic class ProductQuantization implements VectorCompressor<ByteSequence<?>>, Accountable {\n    private static final int MAGIC = 0x75EC4012; // JVECTOR, with some imagination\n\n    protected static final Logger LOG = Logger.getLogger(ProductQuantization.class.getName());\n\n    private static final VectorTypeSupport vectorTypeSupport = VectorizationProvider.getInstance().getVectorTypeSupport();\n    static final int DEFAULT_CLUSTERS = 256; // number of clusters per subspace = one byte's worth\n    static final int K_MEANS_ITERATIONS = 6;\n    public static final int MAX_PQ_TRAINING_SET_SIZE = 128000;\n\n    final VectorFloat<?>[] codebooks; // array of codebooks, where each codebook is a VectorFloat consisting of k contiguous subvectors each of length M\n    final int M; // codebooks.length, redundantly reproduced for convenience\n    private final int clusterCount; // codebooks[0].length, redundantly reproduced for convenience\n    final int originalDimension;\n    final VectorFloat<?> globalCentroid;\n    final int[][] subvectorSizesAndOffsets;\n    final float anisotropicThreshold; // parallel cost multiplier\n    private final float[][] centroidNormsSquared; // precomputed norms of the centroids, for encoding\n    private final ThreadLocal<VectorFloat<?>> partialSums; // for dot product, euclidean, and cosine partials\n    private final ThreadLocal<VectorFloat<?>> partialBestDistances; // for partial best distances during fused ADC\n    private final ThreadLocal<ByteSequence<?>> partialQuantizedSums; // for quantized sums during fused ADC\n    private final AtomicReference<VectorFloat<?>> partialSquaredMagnitudes; // for cosine partials\n    private final AtomicReference<ByteSequence<?>> partialQuantizedSquaredMagnitudes; // for quantized squared magnitude partials during cosine fused ADC\n    protected volatile float squaredMagnitudeDelta = 0; // for cosine fused ADC squared magnitude quantization delta (since this is invariant for a given PQ)\n    protected volatile float minSquaredMagnitude = 0; // for cosine fused ADC minimum squared magnitude (invariant for a given PQ)\n\n    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     */\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter) {\n        return compute(ravv, M, clusterCount, globallyCenter, UNWEIGHTED, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter, float anisotropicThreshold) {\n        return compute(ravv, M, clusterCount, globallyCenter, anisotropicThreshold, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param clusterCount number of clusters per subspace\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     * @param anisotropicThreshold the threshold of relevance for anisotropic angular distance shaping, giving\n     *        higher priority to parallel error.  Anisotropic shaping requires that your dataset be normalized\n     *        to unit length.  Use a threshold of UNWEIGHTED for isotropic distance\n     *        (i.e. normal, unweighted L2 distance).\n     * @param simdExecutor     ForkJoinPool instance for SIMD operations, best is to use a pool with the size of\n     *                         the number of physical cores.\n     * @param parallelExecutor ForkJoinPool instance for parallel stream operations\n     */\n    public static ProductQuantization compute(RandomAccessVectorValues ravv,\n                                              int M,\n                                              int clusterCount,\n                                              boolean globallyCenter,\n                                              float anisotropicThreshold,\n                                              ForkJoinPool simdExecutor,\n                                              ForkJoinPool parallelExecutor)\n    {\n        checkClusterCount(clusterCount);\n\n        var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n        var vectors = extractTrainingVectors(ravv, parallelExecutor);\n\n        // subtract the centroid from each training vector\n        VectorFloat<?> globalCentroid;\n        if (globallyCenter) {\n            globalCentroid = KMeansPlusPlusClusterer.centroidOf(vectors);\n            // subtract the centroid from each vector\n            List<VectorFloat<?>> finalVectors = vectors;\n            vectors = simdExecutor.submit(() -> finalVectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n        } else {\n            globalCentroid = null;\n        }\n\n        // derive the codebooks\n        var codebooks = createCodebooks(vectors, subvectorSizesAndOffsets, clusterCount, anisotropicThreshold, simdExecutor);\n        return new ProductQuantization(codebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n    }\n\n    static List<VectorFloat<?>> extractTrainingVectors(RandomAccessVectorValues ravv, ForkJoinPool parallelExecutor) {\n        // limit the number of vectors we train on\n        var P = min(1.0f, MAX_PQ_TRAINING_SET_SIZE / (float) ravv.size());\n        var ravvCopy = ravv.threadLocalSupplier();\n        return parallelExecutor.submit(() -> IntStream.range(0, ravv.size()).parallel()\n                        .filter(i -> ThreadLocalRandom.current().nextFloat() < P)\n                        .mapToObj(targetOrd -> {\n                            var localRavv = ravvCopy.get();\n                            VectorFloat<?> v = localRavv.getVector(targetOrd);\n                            return localRavv.isValueShared() ? v.copy() : v;\n                        })\n                        .collect(Collectors.toList()))\n                .join();\n    }\n\n    /**\n     * Create a new PQ by fine-tuning this one with the data in `ravv`\n     */\n    public ProductQuantization refine(RandomAccessVectorValues ravv) {\n        return refine(ravv, 1, UNWEIGHTED, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Create a new PQ by fine-tuning this one with the data in `ravv`\n     *\n     * @param lloydsRounds number of Lloyd's iterations to run against\n     *                     the new data.  Suggested values are 1 or 2.\n     */\n    public ProductQuantization refine(RandomAccessVectorValues ravv,\n                                      int lloydsRounds,\n                                      float anisotropicThreshold,\n                                      ForkJoinPool simdExecutor,\n                                      ForkJoinPool parallelExecutor)\n    {\n        if (lloydsRounds < 0) {\n            throw new IllegalArgumentException(\"lloydsRounds must be non-negative\");\n        }\n\n        var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n        var vectorsMutable = extractTrainingVectors(ravv, parallelExecutor);\n        if (globalCentroid != null) {\n            var vectors = vectorsMutable;\n            vectorsMutable = simdExecutor.submit(() -> vectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n        }\n        var vectors = vectorsMutable; // \"effectively final\" to make the closure happy\n\n        var refinedCodebooks = simdExecutor.submit(() -> IntStream.range(0, M).parallel().mapToObj(m -> {\n            VectorFloat<?>[] subvectors = extractSubvectors(vectors, m, subvectorSizesAndOffsets);\n            var clusterer = new KMeansPlusPlusClusterer(subvectors, codebooks[m], anisotropicThreshold);\n            return clusterer.cluster(anisotropicThreshold == UNWEIGHTED ? lloydsRounds : 0,\n                                     anisotropicThreshold == UNWEIGHTED ? 0 : lloydsRounds);\n        }).toArray(VectorFloat<?>[]::new)).join();\n\n        return new ProductQuantization(refinedCodebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n    }\n\n    ProductQuantization(VectorFloat<?>[] codebooks, int clusterCount, int[][] subvectorSizesAndOffsets, VectorFloat<?> globalCentroid, float anisotropicThreshold) {\n        checkClusterCount(clusterCount);\n\n        this.codebooks = codebooks;\n        this.globalCentroid = globalCentroid;\n        this.M = codebooks.length;\n        this.clusterCount = clusterCount;\n        this.subvectorSizesAndOffsets = subvectorSizesAndOffsets;\n        this.originalDimension = Arrays.stream(subvectorSizesAndOffsets).mapToInt(m -> m[0]).sum();\n        if (globalCentroid != null && globalCentroid.length() != originalDimension) {\n            var msg = String.format(\"Global centroid length %d does not match vector dimensionality %d\", globalCentroid.length(), originalDimension);\n            throw new IllegalArgumentException(msg);\n        }\n        this.anisotropicThreshold = anisotropicThreshold;\n        this.partialSums = ThreadLocal.withInitial(() -> vectorTypeSupport.createFloatVector(getSubspaceCount() * getClusterCount()));\n        this.partialBestDistances = ThreadLocal.withInitial(() -> vectorTypeSupport.createFloatVector(getSubspaceCount()));\n        this.partialQuantizedSums = ThreadLocal.withInitial(() -> vectorTypeSupport.createByteSequence(getSubspaceCount() * getClusterCount() * 2));\n        this.partialSquaredMagnitudes = new AtomicReference<>(null);\n        this.partialQuantizedSquaredMagnitudes= new AtomicReference<>(null);\n\n\n        centroidNormsSquared = new float[M][clusterCount];\n        for (int i = 0; i < M; i++) {\n            for (int j = 0; j < clusterCount; j++) {\n                centroidNormsSquared[i][j] = dotProduct(codebooks[i], j * subvectorSizesAndOffsets[i][0],\n                                                        codebooks[i], j * subvectorSizesAndOffsets[i][0],\n                                                        subvectorSizesAndOffsets[i][0]);\n            }\n        }\n    }\n\n    @Override\n    public ImmutablePQVectors createCompressedVectors(Object[] compressedVectors) {\n        return new ImmutablePQVectors(this, (ByteSequence<?>[]) compressedVectors, compressedVectors.length, 1);\n    }\n\n    /**\n     * Encodes the given vectors in parallel using the PQ codebooks. If a vector is missing (null), it will be encoded\n     * as a zero vector.\n     */\n    @Override\n    public PQVectors encodeAll(RandomAccessVectorValues ravv, ForkJoinPool simdExecutor) {\n        return PQVectors.encodeAndBuild(this, ravv.size(), ravv, simdExecutor);\n    }", "func_start": 248, "func_end": 284, "body_len": 36, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java", "test_start": 198, "test_end": 214, "test_code": "    @Test\n    public void testSaveLoad() throws Exception {\n        // Generate a PQ for random 2D vectors\n        var vectors = createRandomVectors(512, 2);\n        var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n\n        // Write\n        var file = File.createTempFile(\"pqtest\", \".pq\");\n        try (var out = new DataOutputStream(new FileOutputStream(file))) {\n            pq.write(out);\n        }\n        // Read\n        try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n            var pq2 = ProductQuantization.load(readerSupplier.get());\n            Assertions.assertEquals(pq, pq2);\n        }\n    }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.quantization.TestProductQuantization#testSaveLoad\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1050}
{"hash_id": "21dcaa4767c22bb361a2b693", "total_turn": 5, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `removeDeletedNodes()` that removes nodes marked for deletion from a graph structure and returns the count of removed nodes. You should write self-contained code starting with: ```java\npublic long removeDeletedNodes() {```", "solution": "public long removeDeletedNodes() {\n    var toDelete = graph.getDeletedNodes().copy();\n    var nRemoved = toDelete.cardinality();\n    if (nRemoved == 0) {\n        return 0;\n    }\n\n    // Reconstruct edges to maintain connectivity when nodes are deleted\n    for (int currentLevel = 0; currentLevel < graph.layers.size(); currentLevel++) {\n        final int level = currentLevel;\n        var newEdges = new ConcurrentHashMap<Integer, Set<Integer>>();\n        \n        // Find new edges to add when deleted nodes are removed\n        parallelExecutor.submit(() -> {\n            IntStream.range(0, graph.getIdUpperBound()).parallel().forEach(i -> {\n                if (toDelete.get(i)) {\n                    return;\n                }\n                for (var it = graph.getNeighborsIterator(level, i); it.hasNext(); ) {\n                    var j = it.nextInt();\n                    if (toDelete.get(j)) {\n                        var newEdgesForI = newEdges.computeIfAbsent(i, __ -> ConcurrentHashMap.newKeySet());\n                        for (var jt = graph.getNeighborsIterator(level, j); jt.hasNext(); ) {\n                            int k = jt.nextInt();\n                            if (i != k && !toDelete.get(k)) {\n                                newEdgesForI.add(k);\n                            }\n                        }\n                    }\n                }\n            });\n        }).join();\n\n        // Apply the edge changes\n        simdExecutor.submit(() -> {\n            newEdges.entrySet().stream().parallel().forEach(e -> {\n                int node = e.getKey();\n                var sf = scoreProvider.searchProviderFor(node).scoreFunction();\n                var candidates = new NodeArray(graph.getDegree(level));\n                for (var k : e.getValue()) {\n                    candidates.insertSorted(k, sf.similarityTo(k));\n                }\n\n                // Add random edges if no candidates found\n                if (candidates.size() == 0) {\n                    var R = ThreadLocalRandom.current();\n                    for (int i = 0; i < 2 * graph.getDegree(level); i++) {\n                        int randomNode = R.nextInt(graph.getIdUpperBound());\n                        while(toDelete.get(randomNode)) {\n                            randomNode = R.nextInt(graph.getIdUpperBound());\n                        }\n                        if (randomNode != node && !candidates.contains(randomNode) && graph.layers.get(level).contains(randomNode)) {\n                            float score = sf.similarityTo(randomNode);\n                            candidates.insertSorted(randomNode, score);\n                        }\n                        if (candidates.size() == graph.getDegree(level)) {\n                            break;\n                        }\n                    }\n                }\n\n                graph.layers.get(level).replaceDeletedNeighbors(node, toDelete, candidates);\n            });\n        }).join();\n    }\n\n    // Handle entry point update if it was deleted\n    if (toDelete.get(graph.entry().node)) {\n        int newLevel = graph.getMaxLevel();\n        int newEntry = -1;\n        outer:\n        while (newLevel >= 0) {\n            for (var it = graph.getNodes(newLevel); it.hasNext(); ){\n                int i = it.nextInt();\n                if (!toDelete.get(i)) {\n                    newEntry = i;\n                    break outer;\n                }\n            }\n            newLevel--;\n        }\n        graph.updateEntryNode(newEntry >= 0 ? new NodeAtLevel(newLevel, newEntry) : null);\n    }\n\n    long memorySize = 0;\n    // Remove the deleted nodes from the graph\n    for (int i = toDelete.nextSetBit(0); i != NO_MORE_DOCS; i = toDelete.nextSetBit(i + 1)) {\n        int nDeletions = graph.removeNode(i);\n        for (var iLayer = 0; iLayer < nDeletions; iLayer++) {\n            memorySize += graph.ramBytesUsedOneNode(iLayer);\n        }\n    }\n    \n    return memorySize;\n}", "test": "@Test\npublic void testMarkingAllNodesAsDeleted() {\n    // Test that removeDeletedNodes properly handles node removal and returns memory size\n    testMarkingAllNodesAsDeleted(false);\n    testMarkingAllNodesAsDeleted(true);\n}"}, {"turn_num": 2, "instruction": "Modify the method to preserve graph connectivity by adding new edges between neighbors of deleted nodes using the edge promotion strategy from 'FreshDiskANN': if node j is deleted and there are edges (i, j) and (j, k), add a new edge (i, k). Implement this using parallel streams for performance.", "solution": "public synchronized long removeDeletedNodes() {\n    // Take a snapshot of the nodes to delete\n    var toDelete = graph.getDeletedNodes().copy();\n    var nRemoved = toDelete.cardinality();\n    if (nRemoved == 0) {\n        return 0;\n    }\n\n    for (int currentLevel = 0; currentLevel < graph.layers.size(); currentLevel++) {\n        final int level = currentLevel;  // Create effectively final copy for lambda\n        // Compute new edges to insert.  If node j is deleted, we add edges (i, k)\n        // whenever (i, j) and (j, k) are directed edges in the current graph.  This\n        // strategy is proposed in \"FreshDiskANN: A Fast and Accurate Graph-Based\n        // ANN Index for Streaming Similarity Search\" section 4.2.\n        var newEdges = new ConcurrentHashMap<Integer, Set<Integer>>(); // new edges for key k are values v\n        parallelExecutor.submit(() -> {\n            IntStream.range(0, graph.getIdUpperBound()).parallel().forEach(i -> {\n                if (toDelete.get(i)) {\n                    return;\n                }\n                for (var it = graph.getNeighborsIterator(level, i); it.hasNext(); ) {\n                    var j = it.nextInt();\n                    if (toDelete.get(j)) {\n                        var newEdgesForI = newEdges.computeIfAbsent(i, __ -> ConcurrentHashMap.newKeySet());\n                        for (var jt = graph.getNeighborsIterator(level, j); jt.hasNext(); ) {\n                            int k = jt.nextInt();\n                            if (i != k && !toDelete.get(k)) {\n                                newEdgesForI.add(k);\n                            }\n                        }\n                    }\n                }\n            });\n        }).join();\n\n        // Remove deleted nodes from neighbors lists;\n        // Score the new edges, and connect the most diverse ones as neighbors\n        simdExecutor.submit(() -> {\n            newEdges.entrySet().stream().parallel().forEach(e -> {\n                // turn the new edges into a NodeArray\n                int node = e.getKey();\n                // each deleted node has ALL of its neighbors added as candidates, so using approximate\n                // scoring and then re-scoring only the best options later makes sense here\n                var sf = scoreProvider.searchProviderFor(node).scoreFunction();\n                var candidates = new NodeArray(graph.getDegree(level));\n                for (var k : e.getValue()) {\n                    candidates.insertSorted(k, sf.similarityTo(k));\n                }\n\n                // it's unlikely, but possible, that all the potential replacement edges were to nodes that have also\n                // been deleted.  if that happens, keep the graph connected by adding random edges.\n                // (this is overly conservative -- really what we care about is that the end result of\n                // replaceDeletedNeighbors not be empty -- but we want to avoid having the node temporarily\n                // neighborless while concurrent searches run.  empirically, this only results in a little extra work.)\n                if (candidates.size() == 0) {\n                    var R = ThreadLocalRandom.current();\n                    // doing actual sampling-without-replacement is expensive so we'll loop a fixed number of times instead\n                    for (int i = 0; i < 2 * graph.getDegree(level); i++) {\n                        int randomNode = R.nextInt(graph.getIdUpperBound());\n                        while(toDelete.get(randomNode)) {\n                            randomNode = R.nextInt(graph.getIdUpperBound());\n                        }\n                        if (randomNode != node && !candidates.contains(randomNode) && graph.layers.get(level).contains(randomNode)) {\n                            float score = sf.similarityTo(randomNode);\n                            candidates.insertSorted(randomNode, score);\n                        }\n                        if (candidates.size() == graph.getDegree(level)) {\n                            break;\n                        }\n                    }\n                }\n\n                // remove edges to deleted nodes and add the new connections, maintaining diversity\n                graph.layers.get(level).replaceDeletedNeighbors(node, toDelete, candidates);\n            });\n        }).join();\n    }\n\n    // Generally we want to keep entryPoint update and node removal distinct, because both can be expensive,\n    // but if the entry point was deleted then we have no choice\n    if (toDelete.get(graph.entry().node)) {\n        // pick a random node at the top layer\n        int newLevel = graph.getMaxLevel();\n        int newEntry = -1;\n        outer:\n        while (newLevel >= 0) {\n            for (var it = graph.getNodes(newLevel); it.hasNext(); ){\n                int i = it.nextInt();\n                if (!toDelete.get(i)) {\n                    newEntry = i;\n                    break outer;\n                }\n            }\n            newLevel--;\n        }\n\n        graph.updateEntryNode(newEntry >= 0 ? new NodeAtLevel(newLevel, newEntry) : null);\n    }\n\n    long memorySize = 0;\n\n    // Remove the deleted nodes from the graph\n    assert toDelete.cardinality() == nRemoved : \"cardinality changed\";\n    for (int i = toDelete.nextSetBit(0); i != NO_MORE_DOCS; i = toDelete.nextSetBit(i + 1)) {\n        int nDeletions = graph.removeNode(i);\n        for (var iLayer = 0; iLayer < nDeletions; iLayer++) {\n            memorySize += graph.ramBytesUsedOneNode(iLayer);\n        }\n    }\n    return memorySize;\n}", "test": "@Test\npublic void testMarkingAllNodesAsDeleted() {\n    testMarkingAllNodesAsDeleted(false);\n    testMarkingAllNodesAsDeleted(true);\n}"}, {"turn_num": 3, "instruction": "Update the method to use a scoring function to evaluate and prioritize new edges, maintaining diversity by selecting the highest-scoring neighbors. If no valid replacement edges exist, add random valid edges to ensure the node retains neighbors and the graph remains connected.", "solution": "public synchronized long removeDeletedNodes() {\n    // Take a snapshot of the nodes to delete\n    var toDelete = graph.getDeletedNodes().copy();\n    var nRemoved = toDelete.cardinality();\n    if (nRemoved == 0) {\n        return 0;\n    }\n\n    for (int currentLevel = 0; currentLevel < graph.layers.size(); currentLevel++) {\n        final int level = currentLevel;  // Create effectively final copy for lambda\n        // Compute new edges to insert.  If node j is deleted, we add edges (i, k)\n        // whenever (i, j) and (j, k) are directed edges in the current graph.  This\n        // strategy is proposed in \"FreshDiskANN: A Fast and Accurate Graph-Based\n        // ANN Index for Streaming Similarity Search\" section 4.2.\n        var newEdges = new ConcurrentHashMap<Integer, Set<Integer>>(); // new edges for key k are values v\n        parallelExecutor.submit(() -> {\n            IntStream.range(0, graph.getIdUpperBound()).parallel().forEach(i -> {\n                if (toDelete.get(i)) {\n                    return;\n                }\n                for (var it = graph.getNeighborsIterator(level, i); it.hasNext(); ) {\n                    var j = it.nextInt();\n                    if (toDelete.get(j)) {\n                        var newEdgesForI = newEdges.computeIfAbsent(i, __ -> ConcurrentHashMap.newKeySet());\n                        for (var jt = graph.getNeighborsIterator(level, j); jt.hasNext(); ) {\n                            int k = jt.nextInt();\n                            if (i != k && !toDelete.get(k)) {\n                                newEdgesForI.add(k);\n                            }\n                        }\n                    }\n                }\n            });\n        }).join();\n\n        // Remove deleted nodes from neighbors lists;\n        // Score the new edges, and connect the most diverse ones as neighbors\n        simdExecutor.submit(() -> {\n            newEdges.entrySet().stream().parallel().forEach(e -> {\n                // turn the new edges into a NodeArray\n                int node = e.getKey();\n                // Use scoring function to evaluate and prioritize new edges, maintaining diversity\n                var sf = scoreProvider.searchProviderFor(node).scoreFunction();\n                var candidates = new NodeArray(graph.getDegree(level));\n                \n                // Score each candidate edge and select highest-scoring neighbors for diversity\n                for (var k : e.getValue()) {\n                    float score = sf.similarityTo(k);\n                    candidates.insertSorted(k, score);\n                }\n\n                // If no valid replacement edges exist, add random valid edges to ensure connectivity\n                if (candidates.size() == 0) {\n                    var R = ThreadLocalRandom.current();\n                    // doing actual sampling-without-replacement is expensive so we'll loop a fixed number of times instead\n                    for (int i = 0; i < 2 * graph.getDegree(level); i++) {\n                        int randomNode = R.nextInt(graph.getIdUpperBound());\n                        while(toDelete.get(randomNode)) {\n                            randomNode = R.nextInt(graph.getIdUpperBound());\n                        }\n                        if (randomNode != node && !candidates.contains(randomNode) && graph.layers.get(level).contains(randomNode)) {\n                            float score = sf.similarityTo(randomNode);\n                            candidates.insertSorted(randomNode, score);\n                        }\n                        if (candidates.size() == graph.getDegree(level)) {\n                            break;\n                        }\n                    }\n                }\n\n                // remove edges to deleted nodes and add the new connections, maintaining diversity\n                graph.layers.get(level).replaceDeletedNeighbors(node, toDelete, candidates);\n            });\n        }).join();\n    }\n\n    // Generally we want to keep entryPoint update and node removal distinct, because both can be expensive,\n    // but if the entry point was deleted then we have no choice\n    if (toDelete.get(graph.entry().node)) {\n        // pick a random node at the top layer\n        int newLevel = graph.getMaxLevel();\n        int newEntry = -1;\n        outer:\n        while (newLevel >= 0) {\n            for (var it = graph.getNodes(newLevel); it.hasNext(); ){\n                int i = it.nextInt();\n                if (!toDelete.get(i)) {\n                    newEntry = i;\n                    break outer;\n                }\n            }\n            newLevel--;\n        }\n\n        graph.updateEntryNode(newEntry >= 0 ? new NodeAtLevel(newLevel, newEntry) : null);\n    }\n\n    long memorySize = 0;\n\n    // Remove the deleted nodes from the graph\n    assert toDelete.cardinality() == nRemoved : \"cardinality changed\";\n    for (int i = toDelete.nextSetBit(0); i != NO_MORE_DOCS; i = toDelete.nextSetBit(i + 1)) {\n        int nDeletions = graph.removeNode(i);\n        for (var iLayer = 0; iLayer < nDeletions; iLayer++) {\n            memorySize += graph.ramBytesUsedOneNode(iLayer);\n        }\n    }\n    return memorySize;\n}", "test": "@Test\npublic void testMarkingAllNodesAsDeleted() {\n    testMarkingAllNodesAsDeleted(false);\n    testMarkingAllNodesAsDeleted(true);\n}"}, {"turn_num": 4, "instruction": "Ensure thread safety by making the method synchronized, and coordinate with the graph's layer structure: apply edge updates per layer, update the graph's entry point if it was deleted by selecting a valid node from the highest available layer, and maintain consistency with neighbor lists and memory tracking systems.", "solution": "public synchronized long removeDeletedNodes() {\n    // Take a snapshot of the nodes to delete\n    var toDelete = graph.getDeletedNodes().copy();\n    var nRemoved = toDelete.cardinality();\n    if (nRemoved == 0) {\n        return 0;\n    }\n\n    // Apply edge updates per layer, coordinating with the graph's layer structure\n    for (int currentLevel = 0; currentLevel < graph.layers.size(); currentLevel++) {\n        final int level = currentLevel;  // Create effectively final copy for lambda\n        // Compute new edges to insert.  If node j is deleted, we add edges (i, k)\n        // whenever (i, j) and (j, k) are directed edges in the current graph.  This\n        // strategy is proposed in \"FreshDiskANN: A Fast and Accurate Graph-Based\n        // ANN Index for Streaming Similarity Search\" section 4.2.\n        var newEdges = new ConcurrentHashMap<Integer, Set<Integer>>(); // new edges for key k are values v\n        parallelExecutor.submit(() -> {\n            IntStream.range(0, graph.getIdUpperBound()).parallel().forEach(i -> {\n                if (toDelete.get(i)) {\n                    return;\n                }\n                for (var it = graph.getNeighborsIterator(level, i); it.hasNext(); ) {\n                    var j = it.nextInt();\n                    if (toDelete.get(j)) {\n                        var newEdgesForI = newEdges.computeIfAbsent(i, __ -> ConcurrentHashMap.newKeySet());\n                        for (var jt = graph.getNeighborsIterator(level, j); jt.hasNext(); ) {\n                            int k = jt.nextInt();\n                            if (i != k && !toDelete.get(k)) {\n                                newEdgesForI.add(k);\n                            }\n                        }\n                    }\n                }\n            });\n        }).join();\n\n        // Remove deleted nodes from neighbors lists;\n        // Score the new edges, and connect the most diverse ones as neighbors\n        simdExecutor.submit(() -> {\n            newEdges.entrySet().stream().parallel().forEach(e -> {\n                // turn the new edges into a NodeArray\n                int node = e.getKey();\n                // each deleted node has ALL of its neighbors added as candidates, so using approximate\n                // scoring and then re-scoring only the best options later makes sense here\n                var sf = scoreProvider.searchProviderFor(node).scoreFunction();\n                var candidates = new NodeArray(graph.getDegree(level));\n                for (var k : e.getValue()) {\n                    candidates.insertSorted(k, sf.similarityTo(k));\n                }\n\n                // it's unlikely, but possible, that all the potential replacement edges were to nodes that have also\n                // been deleted.  if that happens, keep the graph connected by adding random edges.\n                // (this is overly conservative -- really what we care about is that the end result of\n                // replaceDeletedNeighbors not be empty -- but we want to avoid having the node temporarily\n                // neighborless while concurrent searches run.  empirically, this only results in a little extra work.)\n                if (candidates.size() == 0) {\n                    var R = ThreadLocalRandom.current();\n                    // doing actual sampling-without-replacement is expensive so we'll loop a fixed number of times instead\n                    for (int i = 0; i < 2 * graph.getDegree(level); i++) {\n                        int randomNode = R.nextInt(graph.getIdUpperBound());\n                        while(toDelete.get(randomNode)) {\n                            randomNode = R.nextInt(graph.getIdUpperBound());\n                        }\n                        if (randomNode != node && !candidates.contains(randomNode) && graph.layers.get(level).contains(randomNode)) {\n                            float score = sf.similarityTo(randomNode);\n                            candidates.insertSorted(randomNode, score);\n                        }\n                        if (candidates.size() == graph.getDegree(level)) {\n                            break;\n                        }\n                    }\n                }\n\n                // remove edges to deleted nodes and add the new connections, maintaining diversity\n                graph.layers.get(level).replaceDeletedNeighbors(node, toDelete, candidates);\n            });\n        }).join();\n    }\n\n    // Update the graph's entry point if it was deleted by selecting a valid node from the highest available layer\n    if (toDelete.get(graph.entry().node)) {\n        // pick a valid node from the highest available layer\n        int newLevel = graph.getMaxLevel();\n        int newEntry = -1;\n        outer:\n        while (newLevel >= 0) {\n            for (var it = graph.getNodes(newLevel); it.hasNext(); ){\n                int i = it.nextInt();\n                if (!toDelete.get(i)) {\n                    newEntry = i;\n                    break outer;\n                }\n            }\n            newLevel--;\n        }\n\n        graph.updateEntryNode(newEntry >= 0 ? new NodeAtLevel(newLevel, newEntry) : null);\n    }\n\n    long memorySize = 0;\n\n    // Remove the deleted nodes from the graph, maintaining consistency with neighbor lists and memory tracking\n    assert toDelete.cardinality() == nRemoved : \"cardinality changed\";\n    for (int i = toDelete.nextSetBit(0); i != NO_MORE_DOCS; i = toDelete.nextSetBit(i + 1)) {\n        int nDeletions = graph.removeNode(i);\n        for (var iLayer = 0; iLayer < nDeletions; iLayer++) {\n            memorySize += graph.ramBytesUsedOneNode(iLayer);\n        }\n    }\n    return memorySize;\n}", "test": "@Test\npublic void testMarkingAllNodesAsDeleted() {\n    // Test thread safety by verifying synchronized method behavior\n    // and layer coordination with entry point selection from highest available layer\n    testMarkingAllNodesAsDeleted(false);\n    testMarkingAllNodesAsDeleted(true);\n    \n    // Verify that the synchronized method maintains thread safety\n    // by testing that concurrent access doesn't cause issues\n    Thread t1 = new Thread(() -> testMarkingAllNodesAsDeleted(false));\n    Thread t2 = new Thread(() -> testMarkingAllNodesAsDeleted(true));\n    \n    t1.start();\n    t2.start();\n    \n    try {\n        t1.join();\n        t2.join();\n    } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        throw new RuntimeException(\"Test interrupted\", e);\n    }\n}"}, {"turn_num": 5, "instruction": "Extend the method to return an approximate count of the memory (in bytes) freed by removing the deleted nodes, calculated using the graph's per-node memory usage function across all layers from which the nodes were removed.", "solution": "public synchronized long removeDeletedNodes() {\n    // Take a snapshot of the nodes to delete\n    var toDelete = graph.getDeletedNodes().copy();\n    var nRemoved = toDelete.cardinality();\n    if (nRemoved == 0) {\n        return 0;\n    }\n\n    for (int currentLevel = 0; currentLevel < graph.layers.size(); currentLevel++) {\n        final int level = currentLevel;  // Create effectively final copy for lambda\n        // Compute new edges to insert.  If node j is deleted, we add edges (i, k)\n        // whenever (i, j) and (j, k) are directed edges in the current graph.  This\n        // strategy is proposed in \"FreshDiskANN: A Fast and Accurate Graph-Based\n        // ANN Index for Streaming Similarity Search\" section 4.2.\n        var newEdges = new ConcurrentHashMap<Integer, Set<Integer>>(); // new edges for key k are values v\n        parallelExecutor.submit(() -> {\n            IntStream.range(0, graph.getIdUpperBound()).parallel().forEach(i -> {\n                if (toDelete.get(i)) {\n                    return;\n                }\n                for (var it = graph.getNeighborsIterator(level, i); it.hasNext(); ) {\n                    var j = it.nextInt();\n                    if (toDelete.get(j)) {\n                        var newEdgesForI = newEdges.computeIfAbsent(i, __ -> ConcurrentHashMap.newKeySet());\n                        for (var jt = graph.getNeighborsIterator(level, j); jt.hasNext(); ) {\n                            int k = jt.nextInt();\n                            if (i != k && !toDelete.get(k)) {\n                                newEdgesForI.add(k);\n                            }\n                        }\n                    }\n                }\n            });\n        }).join();\n\n        // Remove deleted nodes from neighbors lists;\n        // Score the new edges, and connect the most diverse ones as neighbors\n        simdExecutor.submit(() -> {\n            newEdges.entrySet().stream().parallel().forEach(e -> {\n                // turn the new edges into a NodeArray\n                int node = e.getKey();\n                // each deleted node has ALL of its neighbors added as candidates, so using approximate\n                // scoring and then re-scoring only the best options later makes sense here\n                var sf = scoreProvider.searchProviderFor(node).scoreFunction();\n                var candidates = new NodeArray(graph.getDegree(level));\n                for (var k : e.getValue()) {\n                    candidates.insertSorted(k, sf.similarityTo(k));\n                }\n\n                // it's unlikely, but possible, that all the potential replacement edges were to nodes that have also\n                // been deleted.  if that happens, keep the graph connected by adding random edges.\n                // (this is overly conservative -- really what we care about is that the end result of\n                // replaceDeletedNeighbors not be empty -- but we want to avoid having the node temporarily\n                // neighborless while concurrent searches run.  empirically, this only results in a little extra work.)\n                if (candidates.size() == 0) {\n                    var R = ThreadLocalRandom.current();\n                    // doing actual sampling-without-replacement is expensive so we'll loop a fixed number of times instead\n                    for (int i = 0; i < 2 * graph.getDegree(level); i++) {\n                        int randomNode = R.nextInt(graph.getIdUpperBound());\n                        while(toDelete.get(randomNode)) {\n                            randomNode = R.nextInt(graph.getIdUpperBound());\n                        }\n                        if (randomNode != node && !candidates.contains(randomNode) && graph.layers.get(level).contains(randomNode)) {\n                            float score = sf.similarityTo(randomNode);\n                            candidates.insertSorted(randomNode, score);\n                        }\n                        if (candidates.size() == graph.getDegree(level)) {\n                            break;\n                        }\n                    }\n                }\n\n                // remove edges to deleted nodes and add the new connections, maintaining diversity\n                graph.layers.get(level).replaceDeletedNeighbors(node, toDelete, candidates);\n            });\n        }).join();\n    }\n\n    // Generally we want to keep entryPoint update and node removal distinct, because both can be expensive,\n    // but if the entry point was deleted then we have no choice\n    if (toDelete.get(graph.entry().node)) {\n        // pick a random node at the top layer\n        int newLevel = graph.getMaxLevel();\n        int newEntry = -1;\n        outer:\n        while (newLevel >= 0) {\n            for (var it = graph.getNodes(newLevel); it.hasNext(); ){\n                int i = it.nextInt();\n                if (!toDelete.get(i)) {\n                    newEntry = i;\n                    break outer;\n                }\n            }\n            newLevel--;\n        }\n\n        graph.updateEntryNode(newEntry >= 0 ? new NodeAtLevel(newLevel, newEntry) : null);\n    }\n\n    long memorySize = 0;\n\n    // Remove the deleted nodes from the graph and calculate memory freed\n    assert toDelete.cardinality() == nRemoved : \"cardinality changed\";\n    for (int i = toDelete.nextSetBit(0); i != NO_MORE_DOCS; i = toDelete.nextSetBit(i + 1)) {\n        // Calculate memory for this node across all layers it exists in\n        for (int layer = 0; layer < graph.layers.size(); layer++) {\n            if (graph.layers.get(layer).contains(i)) {\n                memorySize += graph.ramBytesUsedOneNode(layer);\n            }\n        }\n        graph.removeNode(i);\n    }\n    return memorySize;\n}", "test": "@Test\npublic void testMarkingAllNodesAsDeleted() {\n    testMarkingAllNodesAsDeleted(false);\n    testMarkingAllNodesAsDeleted(true);\n}"}], "metadata": {"source": "mrgbench", "func": "    public synchronized long removeDeletedNodes() {\n        // Take a snapshot of the nodes to delete\n        var toDelete = graph.getDeletedNodes().copy();\n        var nRemoved = toDelete.cardinality();\n        if (nRemoved == 0) {\n            return 0;\n        }\n\n        for (int currentLevel = 0; currentLevel < graph.layers.size(); currentLevel++) {\n            final int level = currentLevel;  // Create effectively final copy for lambda\n            // Compute new edges to insert.  If node j is deleted, we add edges (i, k)\n            // whenever (i, j) and (j, k) are directed edges in the current graph.  This\n            // strategy is proposed in \"FreshDiskANN: A Fast and Accurate Graph-Based\n            // ANN Index for Streaming Similarity Search\" section 4.2.\n            var newEdges = new ConcurrentHashMap<Integer, Set<Integer>>(); // new edges for key k are values v\n            parallelExecutor.submit(() -> {\n                IntStream.range(0, graph.getIdUpperBound()).parallel().forEach(i -> {\n                    if (toDelete.get(i)) {\n                        return;\n                    }\n                    for (var it = graph.getNeighborsIterator(level, i); it.hasNext(); ) {\n                        var j = it.nextInt();\n                        if (toDelete.get(j)) {\n                            var newEdgesForI = newEdges.computeIfAbsent(i, __ -> ConcurrentHashMap.newKeySet());\n                            for (var jt = graph.getNeighborsIterator(level, j); jt.hasNext(); ) {\n                                int k = jt.nextInt();\n                                if (i != k && !toDelete.get(k)) {\n                                    newEdgesForI.add(k);\n                                }\n                            }\n                        }\n                    }\n                });\n            }).join();\n\n            // Remove deleted nodes from neighbors lists;\n            // Score the new edges, and connect the most diverse ones as neighbors\n            simdExecutor.submit(() -> {\n                newEdges.entrySet().stream().parallel().forEach(e -> {\n                    // turn the new edges into a NodeArray\n                    int node = e.getKey();\n                    // each deleted node has ALL of its neighbors added as candidates, so using approximate\n                    // scoring and then re-scoring only the best options later makes sense here\n                    var sf = scoreProvider.searchProviderFor(node).scoreFunction();\n                    var candidates = new NodeArray(graph.getDegree(level));\n                    for (var k : e.getValue()) {\n                        candidates.insertSorted(k, sf.similarityTo(k));\n                    }\n\n                    // it's unlikely, but possible, that all the potential replacement edges were to nodes that have also\n                    // been deleted.  if that happens, keep the graph connected by adding random edges.\n                    // (this is overly conservative -- really what we care about is that the end result of\n                    // replaceDeletedNeighbors not be empty -- but we want to avoid having the node temporarily\n                    // neighborless while concurrent searches run.  empirically, this only results in a little extra work.)\n                    if (candidates.size() == 0) {\n                        var R = ThreadLocalRandom.current();\n                        // doing actual sampling-without-replacement is expensive so we'll loop a fixed number of times instead\n                        for (int i = 0; i < 2 * graph.getDegree(level); i++) {\n                            int randomNode = R.nextInt(graph.getIdUpperBound());\n                            while(toDelete.get(randomNode)) {\n                                randomNode = R.nextInt(graph.getIdUpperBound());\n                            }\n                            if (randomNode != node && !candidates.contains(randomNode) && graph.layers.get(level).contains(randomNode)) {\n                                float score = sf.similarityTo(randomNode);\n                                candidates.insertSorted(randomNode, score);\n                            }\n                            if (candidates.size() == graph.getDegree(level)) {\n                                break;\n                            }\n                        }\n                    }\n\n                    // remove edges to deleted nodes and add the new connections, maintaining diversity\n                    graph.layers.get(level).replaceDeletedNeighbors(node, toDelete, candidates);\n                });\n            }).join();\n        }\n\n        // Generally we want to keep entryPoint update and node removal distinct, because both can be expensive,\n        // but if the entry point was deleted then we have no choice\n        if (toDelete.get(graph.entry().node)) {\n            // pick a random node at the top layer\n            int newLevel = graph.getMaxLevel();\n            int newEntry = -1;\n            outer:\n            while (newLevel >= 0) {\n                for (var it = graph.getNodes(newLevel); it.hasNext(); ){\n                    int i = it.nextInt();\n                    if (!toDelete.get(i)) {\n                        newEntry = i;\n                        break outer;\n                    }\n                }\n                newLevel--;\n            }\n\n            graph.updateEntryNode(newEntry >= 0 ? new NodeAtLevel(newLevel, newEntry) : null);\n        }\n\n        long memorySize = 0;\n\n        // Remove the deleted nodes from the graph\n        assert toDelete.cardinality() == nRemoved : \"cardinality changed\";\n        for (int i = toDelete.nextSetBit(0); i != NO_MORE_DOCS; i = toDelete.nextSetBit(i + 1)) {\n            int nDeletions = graph.removeNode(i);\n            for (var iLayer = 0; iLayer < nDeletions; iLayer++) {\n                memorySize += graph.ramBytesUsedOneNode(iLayer);\n            }\n        }\n        return memorySize;\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/GraphIndexBuilder.java", "func_name": "removeDeletedNodes", "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.disk.RandomAccessReader;\nimport io.github.jbellis.jvector.graph.GraphIndex.NodeAtLevel;\nimport io.github.jbellis.jvector.graph.SearchResult.NodeScore;\nimport io.github.jbellis.jvector.graph.diversity.VamanaDiversityProvider;\nimport io.github.jbellis.jvector.graph.similarity.BuildScoreProvider;\nimport io.github.jbellis.jvector.graph.similarity.ScoreFunction;\nimport io.github.jbellis.jvector.graph.similarity.SearchScoreProvider;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.ExceptionUtils;\nimport io.github.jbellis.jvector.util.ExplicitThreadLocal;\nimport io.github.jbellis.jvector.util.PhysicalCoreExecutor;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport org.agrona.collections.IntArrayList;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.Closeable;\nimport java.io.IOException;\nimport java.io.UncheckedIOException;\nimport java.util.*;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentSkipListSet;\nimport java.util.concurrent.ForkJoinPool;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.stream.IntStream;\n\nimport static io.github.jbellis.jvector.util.DocIdSetIterator.NO_MORE_DOCS;\nimport static java.lang.Math.*;\n\n/**\n * Builder for Concurrent GraphIndex. See {@link GraphIndex} for a high level overview, and the\n * comments to `addGraphNode` for details on the concurrent building approach.\n * <p>\n * GIB allocates scratch space and copies of the RandomAccessVectorValues for each thread\n * that calls `addGraphNode`.  These allocations are retained until the GIB itself is no longer referenced.\n * Under most conditions this is not something you need to worry about, but it does mean\n * that spawning a new Thread per call is not advisable.  This includes virtual threads.\n */\npublic class GraphIndexBuilder implements Closeable {\n    private static final int BUILD_BATCH_SIZE = 50;\n\n    private static final Logger logger = LoggerFactory.getLogger(GraphIndexBuilder.class);\n\n    private final int beamWidth;\n    private final ExplicitThreadLocal<NodeArray> naturalScratch;\n    private final ExplicitThreadLocal<NodeArray> concurrentScratch;\n\n    private final int dimension;\n    private final float neighborOverflow;\n    private final float alpha;\n    private final boolean addHierarchy;\n    private final boolean refineFinalGraph;\n\n    @VisibleForTesting\n    final OnHeapGraphIndex graph;\n\n    private final ConcurrentSkipListSet<NodeAtLevel> insertionsInProgress = new ConcurrentSkipListSet<>();\n\n    private final BuildScoreProvider scoreProvider;\n\n    private final ForkJoinPool simdExecutor;\n    private final ForkJoinPool parallelExecutor;\n\n    private final ExplicitThreadLocal<GraphSearcher> searchers;\n\n    private final Random rng;\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     * By default, refineFinalGraph = true.\n     *\n     * @param vectorValues     the vectors whose relations are represented by the graph - must provide a\n     *                         different view over those vectors than the one used to add via addGraphNode.\n     * @param M                – the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     */\n    public GraphIndexBuilder(RandomAccessVectorValues vectorValues,\n                             VectorSimilarityFunction similarityFunction,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy)\n    {\n        this(BuildScoreProvider.randomAccessScoreProvider(vectorValues, similarityFunction),\n                vectorValues.dimension(),\n                M,\n                beamWidth,\n                neighborOverflow,\n                alpha,\n                addHierarchy,\n                true);\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     *\n     * @param vectorValues     the vectors whose relations are represented by the graph - must provide a\n     *                         different view over those vectors than the one used to add via addGraphNode.\n     * @param M                – the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     */\n    public GraphIndexBuilder(RandomAccessVectorValues vectorValues,\n                             VectorSimilarityFunction similarityFunction,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph)\n    {\n        this(BuildScoreProvider.randomAccessScoreProvider(vectorValues, similarityFunction),\n                vectorValues.dimension(),\n                M,\n                beamWidth,\n                neighborOverflow,\n                alpha,\n                addHierarchy,\n                refineFinalGraph);\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     * Default executor pools are used.\n     * By default, refineFinalGraph = true.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param M                the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy)\n    {\n        this(scoreProvider, dimension, M, beamWidth, neighborOverflow, alpha, addHierarchy, true, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     * Default executor pools are used.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param M                the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph)\n    {\n        this(scoreProvider, dimension, M, beamWidth, neighborOverflow, alpha, addHierarchy, refineFinalGraph, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param M                the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     * @param simdExecutor     ForkJoinPool instance for SIMD operations, best is to use a pool with the size of\n     *                         the number of physical cores.\n     * @param parallelExecutor ForkJoinPool instance for parallel stream operations\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph,\n                             ForkJoinPool simdExecutor,\n                             ForkJoinPool parallelExecutor)\n    {\n        this(scoreProvider, dimension, List.of(M), beamWidth, neighborOverflow, alpha, addHierarchy, refineFinalGraph, simdExecutor, parallelExecutor);\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     * Default executor pools are used.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param maxDegrees       the maximum number of connections a node can have in each layer; if fewer entries\n     *      *                  are specified than the number of layers, the last entry is used for all remaining layers.\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             List<Integer> maxDegrees,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph)\n    {\n        this(scoreProvider, dimension, maxDegrees, beamWidth, neighborOverflow, alpha, addHierarchy, refineFinalGraph, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param maxDegrees       the maximum number of connections a node can have in each layer; if fewer entries\n     *                         are specified than the number of layers, the last entry is used for all remaining layers.\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     * @param simdExecutor     ForkJoinPool instance for SIMD operations, best is to use a pool with the size of\n     *                         the number of physical cores.\n     * @param parallelExecutor ForkJoinPool instance for parallel stream operations\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             List<Integer> maxDegrees,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph,\n                             ForkJoinPool simdExecutor,\n                             ForkJoinPool parallelExecutor)\n    {\n        if (maxDegrees.stream().anyMatch(i -> i <= 0)) {\n            throw new IllegalArgumentException(\"layer degrees must be positive\");\n        }\n        if (maxDegrees.size() > 1 && !addHierarchy) {\n            throw new IllegalArgumentException(\"Cannot specify multiple max degrees with addHierarchy=False\");\n        }\n        if (beamWidth <= 0) {\n            throw new IllegalArgumentException(\"beamWidth must be positive\");\n        }\n        if (neighborOverflow < 1.0f) {\n            throw new IllegalArgumentException(\"neighborOverflow must be >= 1.0\");\n        }\n        if (alpha <= 0) {\n            throw new IllegalArgumentException(\"alpha must be positive\");\n        }\n\n        this.scoreProvider = scoreProvider;\n        this.dimension = dimension;\n        this.neighborOverflow = neighborOverflow;\n        this.alpha = alpha;\n        this.addHierarchy = addHierarchy;\n        this.refineFinalGraph = refineFinalGraph;\n        this.beamWidth = beamWidth;\n        this.simdExecutor = simdExecutor;\n        this.parallelExecutor = parallelExecutor;\n\n        this.graph = new OnHeapGraphIndex(maxDegrees, neighborOverflow, new VamanaDiversityProvider(scoreProvider, alpha), BUILD_BATCH_SIZE);\n        this.searchers = ExplicitThreadLocal.withInitial(() -> {\n            var gs = new GraphSearcher(graph);\n            gs.usePruning(false);\n            return gs;\n        });\n\n        // in scratch we store candidates in reverse order: worse candidates are first\n        this.naturalScratch = ExplicitThreadLocal.withInitial(() -> new NodeArray(max(beamWidth, graph.maxDegree() + 1)));\n        this.concurrentScratch = ExplicitThreadLocal.withInitial(() -> new NodeArray(max(beamWidth, graph.maxDegree() + 1)));\n\n        this.rng = new Random(0);\n    }\n\n    // used by Cassandra when it fine-tunes the PQ codebook\n    public static GraphIndexBuilder rescore(GraphIndexBuilder other, BuildScoreProvider newProvider) {\n        var newBuilder = new GraphIndexBuilder(newProvider,\n                other.dimension,\n                other.graph.maxDegrees,\n                other.beamWidth,\n                other.neighborOverflow,\n                other.alpha,\n                other.addHierarchy,\n                other.refineFinalGraph,\n                other.simdExecutor,\n                other.parallelExecutor);\n\n        // Copy each node and its neighbors from the old graph to the new one\n        other.parallelExecutor.submit(() -> {\n            IntStream.range(0, other.graph.getIdUpperBound()).parallel().forEach(i -> {\n                // Find the highest layer this node exists in\n                int maxLayer = -1;\n                for (int lvl = 0; lvl < other.graph.layers.size(); lvl++) {\n                    if (other.graph.getNeighbors(lvl, i) == null) {\n                        break;\n                    }\n                    maxLayer = lvl;\n                }\n                if (maxLayer < 0) {\n                    return;\n                }\n\n                // Loop over 0..maxLayer, re-score neighbors for each layer\n                var sf = newProvider.searchProviderFor(i).scoreFunction();\n                for (int lvl = 0; lvl <= maxLayer; lvl++) {\n                    var oldNeighborsIt = other.graph.getNeighborsIterator(lvl, i);\n                    // Copy edges, compute new scores\n                    var newNeighbors = new NodeArray(oldNeighborsIt.size());\n                    while (oldNeighborsIt.hasNext()) {\n                        int neighbor = oldNeighborsIt.nextInt();\n                        // since we're using a different score provider, use insertSorted instead of addInOrder\n                        newNeighbors.insertSorted(neighbor, sf.similarityTo(neighbor));\n                    }\n                    newBuilder.graph.addNode(lvl, i, newNeighbors);\n                }\n            });\n        }).join();\n\n        // Set the entry node\n        newBuilder.graph.updateEntryNode(other.graph.entry());\n\n        return newBuilder;\n    }\n\n    public OnHeapGraphIndex build(RandomAccessVectorValues ravv) {\n        var vv = ravv.threadLocalSupplier();\n        int size = ravv.size();\n\n        simdExecutor.submit(() -> {\n            IntStream.range(0, size).parallel().forEach(node -> {\n                addGraphNode(node, vv.get().getVector(node));\n            });\n        }).join();\n\n        cleanup();\n        return graph;\n    }\n\n    /**\n     * Cleanup the graph by completing removal of marked-for-delete nodes, trimming\n     * neighbor sets to the advertised degree, and updating the entry node.\n     * <p>\n     * Uses default threadpool to process nodes in parallel.  There is currently no way to restrict this to a single thread.\n     * <p>\n     * Must be called before writing to disk.\n     * <p>\n     * May be called multiple times, but should not be called during concurrent modifications to the graph.\n     */\n    public void cleanup() {\n        if (graph.size(0) == 0) {\n            return;\n        }\n        graph.validateEntryNode(); // sanity check before we start\n\n        // purge deleted nodes.\n        // backlinks can cause neighbors to soft-overflow, so do this before neighbors cleanup\n        removeDeletedNodes();\n\n        if (graph.size(0) == 0) {\n            // After removing all the deleted nodes, we might end up with an empty graph.\n            // The calls below expect a valid entry node, but we do not have one right now.\n            return;\n        }\n\n        if (refineFinalGraph && graph.getMaxLevel() > 0) {\n            // improve connections on everything in L1 & L0.\n            // It may be helpful for 2D use cases, but empirically it seems unnecessary for high-dimensional vectors.\n            // It may bring a slight improvement in recall for small maximum degrees,\n            // but it can be easily be compensated by using a slightly larger neighborOverflow.\n            parallelExecutor.submit(() -> {\n                graph.nodeStream(1).parallel().forEach(this::improveConnections);\n            }).join();\n        }\n\n        // clean up overflowed neighbor lists\n        parallelExecutor.submit(() -> {\n            IntStream.range(0, graph.getIdUpperBound()).parallel().forEach(id -> {\n                for (int layer = 0; layer < graph.layers.size(); layer++) {\n                    graph.layers.get(layer).enforceDegree(id);\n                }\n            });\n        }).join();\n    }\n\n    private void improveConnections(int node) {\n        var ssp = scoreProvider.searchProviderFor(node);\n        var bits = new ExcludingBits(node);\n        try (var gs = searchers.get()) {\n            gs.initializeInternal(ssp, graph.entry(), bits);\n            var acceptedBits = Bits.intersectionOf(bits, gs.getView().liveNodes());\n\n            // Move downward from entry.level to 0\n            for (int lvl = graph.entry().level; lvl >= 0; lvl--) {\n                // This additional call seems redundant given that we have already initialized an ssp above.\n                // However, there is a subtle interplay between the ssp of the search and the ssp used in insertDiverse.\n                // Do not remove this line.\n                ssp = scoreProvider.searchProviderFor(node);\n\n                if (graph.layers.get(lvl).get(node) != null) {\n                    gs.searchOneLayer(ssp, beamWidth, 0.0f, lvl, acceptedBits);\n\n                    var candidates = new NodeArray(gs.approximateResults.size());\n                    gs.approximateResults.foreach(candidates::insertSorted);\n                    var newNeighbors = graph.layers.get(lvl).insertDiverse(node, candidates);\n                    graph.layers.get(lvl).backlink(newNeighbors, node, neighborOverflow);\n                } else {\n                    gs.searchOneLayer(ssp, 1, 0.0f, lvl, acceptedBits);\n                }\n                gs.setEntryPointsFromPreviousLayer();\n            }\n        } catch (IOException e) {\n            throw new UncheckedIOException(e);\n        }\n    }\n\n    public OnHeapGraphIndex getGraph() {\n        return graph;\n    }\n\n    /**\n     * Number of inserts in progress, across all threads.  Useful as a sanity check\n     * when calling non-threadsafe methods like cleanup().  (Do not use it to try to\n     * _prevent_ races, only to detect them.)\n     */\n    public int insertsInProgress() {\n        return insertionsInProgress.size();\n    }\n\n    @Deprecated\n    public long addGraphNode(int node, RandomAccessVectorValues ravv) {\n        return addGraphNode(node, ravv.getVector(node));\n    }\n\n    /**\n     * Assigns a hierarchy level to a node at random. It follows the HNSW sampling strategy.\n     * @return The assigned level\n     */\n    private int getRandomGraphLevel() {\n        double ml;\n        double randDouble;\n        if (addHierarchy) {\n            ml = graph.getDegree(0) == 1 ? 1 : 1 / log(1.0 * graph.getDegree(0));\n            do {\n                randDouble = this.rng.nextDouble();  // avoid 0 value, as log(0) is undefined\n            } while (randDouble == 0.0);\n        } else {\n            ml = 0;\n            randDouble = 0;\n        }\n        return ((int) (-log(randDouble) * ml));\n    }\n\n    /**\n     * Inserts a node with the given vector value to the graph.\n     *\n     * <p>To allow correctness under concurrency, we track in-progress updates in a\n     * ConcurrentSkipListSet. After adding ourselves, we take a snapshot of this set, and consider all\n     * other in-progress updates as neighbor candidates.\n     *\n     * @param node the node ID to add\n     * @param vector the vector to add\n     * @return an estimate of the number of extra bytes used by the graph after adding the given node\n     */\n    public long addGraphNode(int node, VectorFloat<?> vector) {\n        var ssp = scoreProvider.searchProviderFor(vector);\n        return addGraphNode(node, ssp);\n    }\n\n    /**\n     * Inserts a node with the given vector value to the graph.\n     *\n     * <p>To allow correctness under concurrency, we track in-progress updates in a\n     * ConcurrentSkipListSet. After adding ourselves, we take a snapshot of this set, and consider all\n     * other in-progress updates as neighbor candidates.\n     *\n     * @param node the node ID to add\n     * @param searchScoreProvider a SearchScoreProvider corresponding to the vector to add.\n     *                            It needs to be compatible with the BuildScoreProvider provided to the constructor\n     * @return an estimate of the number of extra bytes used by the graph after adding the given node\n     */\n    public long addGraphNode(int node, SearchScoreProvider searchScoreProvider) {\n        var nodeLevel = new NodeAtLevel(getRandomGraphLevel(), node);\n        // do this before adding to in-progress, so a concurrent writer checking\n        // the in-progress set doesn't have to worry about uninitialized neighbor sets\n        graph.addNode(nodeLevel);\n\n        insertionsInProgress.add(nodeLevel);\n        var inProgressBefore = insertionsInProgress.clone();\n        try (var gs = searchers.get()) {\n            gs.setView(graph.getView()); // new snapshot\n            var naturalScratchPooled = naturalScratch.get();\n            var concurrentScratchPooled = concurrentScratch.get();\n\n            var bits = new ExcludingBits(nodeLevel.node);\n            var entry = graph.entry();\n            SearchResult result;\n            if (entry == null) {\n                result = new SearchResult(new NodeScore[] {}, 0, 0, 0, 0, 0);\n            } else {\n                gs.initializeInternal(searchScoreProvider, entry, bits);\n\n                // Move downward from entry.level to 1\n                for (int lvl = entry.level; lvl > 0; lvl--) {\n                    if (lvl > nodeLevel.level) {\n                        gs.searchOneLayer(searchScoreProvider, 1, 0.0f, lvl, gs.getView().liveNodes());\n                    } else {\n                        gs.searchOneLayer(searchScoreProvider, beamWidth, 0.0f, lvl, gs.getView().liveNodes());\n                        NodeScore[] neighbors = new NodeScore[gs.approximateResults.size()];\n                        AtomicInteger index = new AtomicInteger();\n                        // TODO extract an interface that lets us avoid the copy here and in toScratchCandidates\n                        gs.approximateResults.foreach((neighbor, score) -> {\n                            neighbors[index.getAndIncrement()] = new NodeScore(neighbor, score);\n                        });\n                        Arrays.sort(neighbors);\n                        updateNeighborsOneLayer(lvl, nodeLevel.node, neighbors, naturalScratchPooled, inProgressBefore, concurrentScratchPooled, searchScoreProvider);\n                    }\n                    gs.setEntryPointsFromPreviousLayer();\n                }\n\n                // Now do the main search at layer 0\n                result = gs.resume(beamWidth, beamWidth, 0.0f, 0.0f);\n            }\n\n            updateNeighborsOneLayer(0, nodeLevel.node, result.getNodes(), naturalScratchPooled, inProgressBefore, concurrentScratchPooled, searchScoreProvider);\n\n            graph.markComplete(nodeLevel);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        } finally {\n            insertionsInProgress.remove(nodeLevel);\n        }\n\n        return IntStream.range(0, nodeLevel.level).mapToLong(graph::ramBytesUsedOneNode).sum();\n    }\n\n    private void updateNeighborsOneLayer(int layer, int node, NodeScore[] neighbors, NodeArray naturalScratchPooled, ConcurrentSkipListSet<NodeAtLevel> inProgressBefore, NodeArray concurrentScratchPooled, SearchScoreProvider ssp) {\n        // Update neighbors with these candidates.\n        // The DiskANN paper calls for using the entire set of visited nodes along the search path as\n        // potential candidates, but in practice we observe neighbor lists being completely filled using\n        // just the topK results.  (Since the Robust Prune algorithm prioritizes closer neighbors,\n        // this means that considering additional nodes from the search path, that are by definition\n        // farther away than the ones in the topK, would not change the result.)\n        var natural = toScratchCandidates(neighbors, naturalScratchPooled);\n        var concurrent = getConcurrentCandidates(layer, node, inProgressBefore, concurrentScratchPooled, ssp.scoreFunction());\n        updateNeighbors(layer, node, natural, concurrent);\n    }\n\n    @VisibleForTesting\n    public void setEntryPoint(int level, int node) {\n        graph.updateEntryNode(new NodeAtLevel(level, node));\n    }\n\n    public void markNodeDeleted(int node) {\n        graph.markDeleted(node);\n    }", "func_start": 633, "func_end": 743, "body_len": 110, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestDeletions.java", "test_start": 127, "test_end": 131, "test_code": "    @Test\n    public void testMarkingAllNodesAsDeleted() {\n        testMarkingAllNodesAsDeleted(false);\n        testMarkingAllNodesAsDeleted(true);\n    }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestDeletions#testMarkingAllNodesAsDeleted\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1051}
{"hash_id": "efdbcbf31edb2be45f57c55b", "total_turn": 5, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `writeInline(int ordinal, Map<FeatureId, Feature.State> stateMap)` that writes feature state data to an output stream at a position calculated from the ordinal. The function should loop through a list of inline features, retrieve the state for each feature from the state map, and if the state exists, write it using the feature's `writeInline` method; otherwise, skip the feature's size in the output to maintain alignment. You should write self-contained code starting with: ```java\n<code>```", "solution": "public synchronized void writeInline(int ordinal, Map<FeatureId, Feature.State> stateMap) throws IOException\n{\n    out.seek(featureOffsetForOrdinal(ordinal));\n\n    for (var feature : inlineFeatures) {\n        var state = stateMap.get(feature.id());\n        if (state == null) {\n            out.seek(out.position() + feature.featureSize());\n        } else {\n            feature.writeInline(out, state);\n        }\n    }\n}", "test": "@Test\npublic void testIncrementalWrites() throws IOException {\n    var graph = new TestUtil.RandomlyConnectedGraphIndex(10, 4, getRandom());\n    var vectors = TestUtil.createRandomVectors(10, 8);\n    var ravv = new ListRandomAccessVectorValues(vectors, 8);\n\n    var incrementalPath = testDirectory.resolve(\"test_graph\");\n    try (var writer = new OnDiskGraphIndexWriter.Builder(graph, incrementalPath)\n            .with(new InlineVectors(ravv.dimension()))\n            .build())\n    {\n        // Test writing inline vectors for a few nodes\n        for (int i = 0; i < 3; i++) {\n            var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(i)));\n            writer.writeInline(i, state);\n        }\n        \n        writer.write(Map.of());\n    }\n\n    // Verify the vectors were written correctly\n    try (var readerSupplier = new SimpleMappedReader.Supplier(incrementalPath.toAbsolutePath());\n         var loadedGraph = OnDiskGraphIndex.load(readerSupplier);\n         var view = loadedGraph.getView())\n    {\n        for (int i = 0; i < 3; i++) {\n            var originalVector = ravv.getVector(i);\n            var loadedVector = view.getVector(i);\n            // Use the existing validateVectors method pattern from original test\n            assertEquals(originalVector.length(), loadedVector.length());\n            for (int j = 0; j < originalVector.length(); j++) {\n                assertEquals(originalVector.get(j), loadedVector.get(j), 1e-6f);\n            }\n        }\n    }\n}"}, {"turn_num": 2, "instruction": "Modify the function to ensure that before writing any data, it validates that all feature IDs present in the stateMap are contained in a predefined featureMap. If any feature ID is not found, throw an IllegalArgumentException with a message indicating the missing feature.", "solution": "public synchronized void writeInline(int ordinal, Map<FeatureId, Feature.State> stateMap) throws IOException\n{\n    for (var featureId : stateMap.keySet()) {\n        if (!featureMap.containsKey(featureId)) {\n            throw new IllegalArgumentException(String.format(\"Feature %s not configured for index\", featureId));\n        }\n    }\n\n    out.seek(featureOffsetForOrdinal(ordinal));\n\n    for (var feature : inlineFeatures) {\n        var state = stateMap.get(feature.id());\n        if (state == null) {\n            out.seek(out.position() + feature.featureSize());\n        } else {\n            feature.writeInline(out, state);\n        }\n    }\n}", "test": "@Test\npublic void testIncrementalWrites() throws IOException {\n    // generate 1000 node random graph\n    var graph = new TestUtil.RandomlyConnectedGraphIndex(1000, 32, getRandom());\n    var vectors = TestUtil.createRandomVectors(1000, 256);\n    var ravv = new ListRandomAccessVectorValues(vectors, 256);\n\n    // write out graph all at once\n    var bulkPath = testDirectory.resolve(\"bulk_graph\");\n    OnDiskGraphIndex.write(graph, ravv, bulkPath);\n\n    // write incrementally\n    var incrementalPath = testDirectory.resolve(\"bulk_graph\");\n    try (var writer = new OnDiskGraphIndexWriter.Builder(graph, incrementalPath)\n            .with(new InlineVectors(ravv.dimension()))\n            .build())\n    {\n        // write inline vectors incrementally\n        for (int i = 0; i < vectors.size(); i++) {\n            var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(i)));\n            writer.writeInline(i, state);\n        }\n\n        // write graph structure\n        writer.write(Map.of());\n    }\n\n    // Test validation: attempt to write with unconfigured feature should throw exception\n    try (var writer = new OnDiskGraphIndexWriter.Builder(graph, testDirectory.resolve(\"validation_test\"))\n            .with(new InlineVectors(ravv.dimension()))\n            .build())\n    {\n        var invalidState = Map.<FeatureId, Feature.State>of(FeatureId.FUSED_ADC, new FusedADC.State(null, null, 0));\n        assertThrows(IllegalArgumentException.class, () -> writer.writeInline(0, invalidState));\n    }\n\n    // all-at-once and incremental builds should be identical on disk\n    var bulkContents = Files.readAllBytes(bulkPath);\n    var incrementalContents = Files.readAllBytes(incrementalPath);\n    assertArrayEquals(bulkContents, incrementalContents);\n}"}, {"turn_num": 3, "instruction": "Ensure the function seeks to the exact file position using a helper method `featureOffsetForOrdinal(ordinal)` before writing any feature data, based on the given ordinal, to ensure data is written at the correct precomputed offset in the output stream.", "solution": "public synchronized void writeInline(int ordinal, Map<FeatureId, Feature.State> stateMap) throws IOException\n{\n    for (var featureId : stateMap.keySet()) {\n        if (!featureMap.containsKey(featureId)) {\n            throw new IllegalArgumentException(String.format(\"Feature %s not configured for index\", featureId));\n        }\n    }\n\n    out.seek(featureOffsetForOrdinal(ordinal));\n\n    for (var feature : inlineFeatures) {\n        var state = stateMap.get(feature.id());\n        if (state == null) {\n            out.seek(out.position() + feature.featureSize());\n        } else {\n            feature.writeInline(out, state);\n        }\n    }\n\n    maxOrdinalWritten = Math.max(maxOrdinalWritten, ordinal);\n}", "test": "@Test\npublic void testIncrementalWrites() throws IOException {\n    // generate small graph for precise position testing\n    var graph = new TestUtil.RandomlyConnectedGraphIndex(100, 32, getRandom());\n    var vectors = TestUtil.createRandomVectors(100, 256);\n    var ravv = new ListRandomAccessVectorValues(vectors, 256);\n\n    // write incrementally in non-sequential order to test precise positioning\n    var incrementalPath = testDirectory.resolve(\"incremental_graph\");\n    try (var writer = new OnDiskGraphIndexWriter.Builder(graph, incrementalPath)\n            .with(new InlineVectors(ravv.dimension()))\n            .build())\n    {\n        // write vectors in non-sequential order: 5, 2, 8, 1\n        // This tests that featureOffsetForOrdinal is used for exact positioning\n        int[] ordinals = {5, 2, 8, 1};\n        for (int ordinal : ordinals) {\n            var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(ordinal)));\n            writer.writeInline(ordinal, state);\n        }\n\n        // write graph structure\n        writer.write(Map.of());\n    }\n\n    // verify that data was written at correct positions by reading back in original order\n    try (var readerSupplier = new SimpleMappedReader.Supplier(incrementalPath.toAbsolutePath());\n         var graph2 = OnDiskGraphIndex.load(readerSupplier);\n         var view = graph2.getView())\n    {\n        // verify that vectors written out of order can be read back in correct positions\n        for (int ordinal : new int[]{1, 2, 5, 8}) {\n            var vector = view.getVector(ordinal);\n            assertNotNull(vector);\n            assertEquals(256, vector.length());\n            // verify the vector is not zero (indicating data was written)\n            assertTrue(vector.length() > 0);\n        }\n    }\n}"}, {"turn_num": 4, "instruction": "Ensure the function is synchronized to prevent concurrent access, and make sure it does not flush the output stream under any circumstances.", "solution": "public synchronized void writeInline(int ordinal, Map<FeatureId, Feature.State> stateMap) throws IOException\n{\n    for (var featureId : stateMap.keySet()) {\n        if (!featureMap.containsKey(featureId)) {\n            throw new IllegalArgumentException(String.format(\"Feature %s not configured for index\", featureId));\n        }\n    }\n\n    out.seek(featureOffsetForOrdinal(ordinal));\n\n    for (var feature : inlineFeatures) {\n        var state = stateMap.get(feature.id());\n        if (state == null) {\n            out.seek(out.position() + feature.featureSize());\n        } else {\n            feature.writeInline(out, state);\n        }\n    }\n\n    maxOrdinalWritten = Math.max(maxOrdinalWritten, ordinal);\n}", "test": "@Test\npublic void testIncrementalWrites() throws IOException {\n    var graph = new TestUtil.RandomlyConnectedGraphIndex(50, 16, getRandom());\n    var vectors = TestUtil.createRandomVectors(50, 128);\n    var ravv = new ListRandomAccessVectorValues(vectors, 128);\n    \n    var testPath = testDirectory.resolve(\"sync_test\");\n    \n    try (var writer = new OnDiskGraphIndexWriter.Builder(graph, testPath)\n            .with(new InlineVectors(ravv.dimension()))\n            .build())\n    {\n        // Test synchronization by verifying execution order\n        var executionOrder = new java.util.concurrent.ConcurrentLinkedQueue<Integer>();\n        var startLatch = new java.util.concurrent.CountDownLatch(1);\n        var threads = new Thread[3];\n        var exceptions = new Exception[3];\n        \n        for (int t = 0; t < threads.length; t++) {\n            final int threadId = t;\n            threads[t] = new Thread(() -> {\n                try {\n                    startLatch.await(); // Ensure all threads start simultaneously\n                    executionOrder.add(threadId);\n                    // Add a small delay to make synchronization issues more apparent\n                    Thread.sleep(10);\n                    var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(threadId)));\n                    writer.writeInline(threadId, state);\n                    executionOrder.add(threadId + 100); // Mark completion\n                } catch (Exception e) {\n                    exceptions[threadId] = e;\n                }\n            });\n        }\n        \n        for (var thread : threads) {\n            thread.start();\n        }\n        \n        startLatch.countDown(); // Start all threads simultaneously\n        \n        for (var thread : threads) {\n            thread.join();\n        }\n        \n        // Verify no exceptions occurred\n        for (var exception : exceptions) {\n            if (exception != null) {\n                throw new AssertionError(\"Concurrent access failed\", exception);\n            }\n        }\n        \n        // Verify that execution was properly serialized\n        // With proper synchronization, we should see complete execution blocks\n        var orderList = new java.util.ArrayList<>(executionOrder);\n        assertTrue(\"Execution order suggests lack of synchronization\", orderList.size() == 6);\n        \n        // Test that method completes without flushing by writing more data\n        for (int i = 10; i < 20; i++) {\n            var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(i % vectors.size())));\n            writer.writeInline(i, state);\n        }\n        \n        writer.write(Map.of());\n    } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        throw new RuntimeException(e);\n    }\n    \n    assertTrue(Files.exists(testPath));\n}"}, {"turn_num": 5, "instruction": "After writing all feature data, update a shared variable `maxOrdinalWritten` to track the highest ordinal that has been written so far by setting it to the maximum of its current value and the provided ordinal.", "solution": "public synchronized void writeInline(int ordinal, Map<FeatureId, Feature.State> stateMap) throws IOException\n{\n    for (var featureId : stateMap.keySet()) {\n        if (!featureMap.containsKey(featureId)) {\n            throw new IllegalArgumentException(String.format(\"Feature %s not configured for index\", featureId));\n        }\n    }\n\n    out.seek(featureOffsetForOrdinal(ordinal));\n\n    for (var feature : inlineFeatures) {\n        var state = stateMap.get(feature.id());\n        if (state == null) {\n            out.seek(out.position() + feature.featureSize());\n        } else {\n            feature.writeInline(out, state);\n        }\n    }\n\n    maxOrdinalWritten = Math.max(maxOrdinalWritten, ordinal);\n}", "test": "@Test\npublic void testIncrementalWrites() throws IOException {\n    // generate 1000 node random graph\n    var graph = new TestUtil.RandomlyConnectedGraphIndex(1000, 32, getRandom());\n    var vectors = TestUtil.createRandomVectors(1000, 256);\n    var ravv = new ListRandomAccessVectorValues(vectors, 256);\n\n    // write out graph all at once\n    var bulkPath = testDirectory.resolve(\"bulk_graph\");\n    OnDiskGraphIndex.write(graph, ravv, bulkPath);\n\n    // write incrementally\n    var incrementalPath = testDirectory.resolve(\"bulk_graph\");\n    try (var writer = new OnDiskGraphIndexWriter.Builder(graph, incrementalPath)\n            .with(new InlineVectors(ravv.dimension()))\n            .build())\n    {\n        // write inline vectors incrementally\n        for (int i = 0; i < vectors.size(); i++) {\n            var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(i)));\n            writer.writeInline(i, state);\n        }\n\n        // write graph structure\n        writer.write(Map.of());\n    }\n\n    // all-at-once and incremental builds should be identical on disk\n    var bulkContents = Files.readAllBytes(bulkPath);\n    var incrementalContents = Files.readAllBytes(incrementalPath);\n    assertArrayEquals(bulkContents, incrementalContents);\n\n    // write incrementally and add Fused ADC Feature\n    var incrementalFadcPath = testDirectory.resolve(\"incremental_graph\");\n    var pq = ProductQuantization.compute(ravv, 64, 256, false);\n    var pqv = (PQVectors) pq.encodeAll(ravv);\n    try (var writer = new OnDiskGraphIndexWriter.Builder(graph, incrementalFadcPath)\n            .with(new InlineVectors(ravv.dimension()))\n            .with(new FusedADC(graph.getDegree(0), pq))\n            .build())\n    {\n        // write inline vectors incrementally\n        for (int i = 0; i < vectors.size(); i++) {\n            var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(i)));\n            writer.writeInline(i, state);\n        }\n        // write graph structure, fused ADC\n        writer.write(Feature.singleStateFactory(FeatureId.FUSED_ADC, i -> new FusedADC.State(graph.getView(), pqv, i)));\n        writer.write(Map.of());\n    }\n\n    // graph and vectors should be identical\n    try (var bulkReaderSupplier = new SimpleMappedReader.Supplier(bulkPath.toAbsolutePath());\n         var bulkGraph = OnDiskGraphIndex.load(bulkReaderSupplier);\n         var incrementalReaderSupplier = new SimpleMappedReader.Supplier(incrementalFadcPath.toAbsolutePath());\n         var incrementalGraph = OnDiskGraphIndex.load(incrementalReaderSupplier);\n         var incrementalView = incrementalGraph.getView())\n    {\n        assertTrue(OnDiskGraphIndex.areHeadersEqual(incrementalGraph, bulkGraph));\n        TestUtil.assertGraphEquals(incrementalGraph, bulkGraph); // incremental and bulk graph should have same structure\n        validateVectors(incrementalView, ravv); // inline vectors should be the same\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n}"}], "metadata": {"source": "mrgbench", "func": "    public synchronized void writeInline(int ordinal, Map<FeatureId, Feature.State> stateMap) throws IOException\n    {\n        for (var featureId : stateMap.keySet()) {\n            if (!featureMap.containsKey(featureId)) {\n                throw new IllegalArgumentException(String.format(\"Feature %s not configured for index\", featureId));\n            }\n        }\n\n        out.seek(featureOffsetForOrdinal(ordinal));\n\n        for (var feature : inlineFeatures) {\n            var state = stateMap.get(feature.id());\n            if (state == null) {\n                out.seek(out.position() + feature.featureSize());\n            } else {\n                feature.writeInline(out, state);\n            }\n        }\n\n        maxOrdinalWritten = Math.max(maxOrdinalWritten, ordinal);\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/disk/OnDiskGraphIndexWriter.java", "func_name": "writeInline", "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph.disk;\n\nimport io.github.jbellis.jvector.disk.BufferedRandomAccessWriter;\nimport io.github.jbellis.jvector.disk.RandomAccessWriter;\nimport io.github.jbellis.jvector.graph.GraphIndex;\nimport io.github.jbellis.jvector.graph.OnHeapGraphIndex;\nimport io.github.jbellis.jvector.graph.disk.feature.Feature;\nimport io.github.jbellis.jvector.graph.disk.feature.FeatureId;\nimport io.github.jbellis.jvector.graph.disk.feature.InlineVectors;\nimport io.github.jbellis.jvector.graph.disk.feature.NVQ;\nimport io.github.jbellis.jvector.graph.disk.feature.SeparatedFeature;\nimport io.github.jbellis.jvector.graph.disk.feature.SeparatedNVQ;\nimport io.github.jbellis.jvector.graph.disk.feature.SeparatedVectors;\n\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\nimport java.nio.file.Path;\nimport java.util.EnumMap;\nimport java.util.Map;\nimport java.util.function.IntFunction;\n\n/**\n * Writes a graph index to disk in a format that can be loaded as an OnDiskGraphIndex.\n * <p>\n * The serialization process follows these steps:\n * \n * 1. File Layout:\n *    - CommonHeader: Contains version, dimension, entry node, and layer information\n *    - Header with Features: Contains feature-specific headers\n *    - Layer 0 data: Contains node ordinals, inline features, and edges for all nodes\n *    - Higher layer data (levels 1..N): Contains sparse node ordinals and edges\n *    - Separated features: Contains feature data stored separately from nodes\n * \n * 2. Serialization Process:\n *    - First, a placeholder header is written to reserve space\n *    - For each node in layer 0:\n *      - Write node ordinal\n *      - Write inline features (vectors, quantized data, etc.)\n *      - Write neighbor count and neighbor ordinals\n *    - For each higher layer (1..N):\n *      - Write only nodes that exist in that layer\n *      - For each node: write ordinal, neighbor count, and neighbor ordinals\n *    - For each separated feature:\n *      - Write feature data for all nodes sequentially\n *    - Finally, rewrite the header with correct offsets\n * \n * 3. Ordinal Mapping:\n *    - The writer uses an OrdinalMapper to map between original node IDs and \n *      the sequential IDs used in the on-disk format\n *    - This allows for compaction (removing \"holes\" from deleted nodes)\n *    - It also enables custom ID mapping schemes for specific use cases\n * \n * The class supports incremental writing through the writeInline method, which\n * allows writing features for individual nodes without writing the entire graph.\n */\npublic class OnDiskGraphIndexWriter extends AbstractGraphIndexWriter<RandomAccessWriter> {\n    private final long startOffset;\n\n    OnDiskGraphIndexWriter(RandomAccessWriter randomAccessWriter,\n                                   int version,\n                                   long startOffset,\n                                   GraphIndex graph,\n                                   OrdinalMapper oldToNewOrdinals,\n                                   int dimension,\n                                   EnumMap<FeatureId, Feature> features)\n    {\n        super(randomAccessWriter, version, graph, oldToNewOrdinals, dimension, features);\n        this.startOffset = startOffset;\n    }\n\n    /**\n     * Close the view and the output stream. Unlike the super method, for backwards compatibility reasons,\n     * this method assumes ownership of the output stream.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        view.close();\n        out.close();\n    }\n\n    /**\n     * Caller should synchronize on this OnDiskGraphIndexWriter instance if mixing usage of the\n     * output with calls to any of the synchronized methods in this class.\n     * <p>\n     * Provided for callers (like Cassandra) that want to add their own header/footer to the output.\n     */\n    public RandomAccessWriter getOutput() {\n        return out;\n    }", "func_start": 115, "func_end": 135, "body_len": 20, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java", "test_start": 462, "test_end": 526, "test_code": "    @Test\n    public void testIncrementalWrites() throws IOException {\n        // generate 1000 node random graph\n        var graph = new TestUtil.RandomlyConnectedGraphIndex(1000, 32, getRandom());\n        var vectors = TestUtil.createRandomVectors(1000, 256);\n        var ravv = new ListRandomAccessVectorValues(vectors, 256);\n\n        // write out graph all at once\n        var bulkPath = testDirectory.resolve(\"bulk_graph\");\n        OnDiskGraphIndex.write(graph, ravv, bulkPath);\n\n        // write incrementally\n        var incrementalPath = testDirectory.resolve(\"bulk_graph\");\n        try (var writer = new OnDiskGraphIndexWriter.Builder(graph, incrementalPath)\n                .with(new InlineVectors(ravv.dimension()))\n                .build())\n        {\n            // write inline vectors incrementally\n            for (int i = 0; i < vectors.size(); i++) {\n                var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(i)));\n                writer.writeInline(i, state);\n            }\n\n            // write graph structure\n            writer.write(Map.of());\n        }\n\n        // all-at-once and incremental builds should be identical on disk\n        var bulkContents = Files.readAllBytes(bulkPath);\n        var incrementalContents = Files.readAllBytes(incrementalPath);\n        assertArrayEquals(bulkContents, incrementalContents);\n\n        // write incrementally and add Fused ADC Feature\n        var incrementalFadcPath = testDirectory.resolve(\"incremental_graph\");\n        var pq = ProductQuantization.compute(ravv, 64, 256, false);\n        var pqv = (PQVectors) pq.encodeAll(ravv);\n        try (var writer = new OnDiskGraphIndexWriter.Builder(graph, incrementalFadcPath)\n                .with(new InlineVectors(ravv.dimension()))\n                .with(new FusedADC(graph.getDegree(0), pq))\n                .build())\n        {\n            // write inline vectors incrementally\n            for (int i = 0; i < vectors.size(); i++) {\n                var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(i)));\n                writer.writeInline(i, state);\n            }\n            // write graph structure, fused ADC\n            writer.write(Feature.singleStateFactory(FeatureId.FUSED_ADC, i -> new FusedADC.State(graph.getView(), pqv, i)));\n            writer.write(Map.of());\n        }\n\n        // graph and vectors should be identical\n        try (var bulkReaderSupplier = new SimpleMappedReader.Supplier(bulkPath.toAbsolutePath());\n             var bulkGraph = OnDiskGraphIndex.load(bulkReaderSupplier);\n             var incrementalReaderSupplier = new SimpleMappedReader.Supplier(incrementalFadcPath.toAbsolutePath());\n             var incrementalGraph = OnDiskGraphIndex.load(incrementalReaderSupplier);\n             var incrementalView = incrementalGraph.getView())\n        {\n            assertTrue(OnDiskGraphIndex.areHeadersEqual(incrementalGraph, bulkGraph));\n            TestUtil.assertGraphEquals(incrementalGraph, bulkGraph); // incremental and bulk graph should have same structure\n            validateVectors(incrementalView, ravv); // inline vectors should be the same\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.disk.TestOnDiskGraphIndex#testIncrementalWrites\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1052}
{"hash_id": "a3045dd878c9235e7d26de84", "total_turn": 5, "turn_datas": [{"turn_num": 1, "instruction": "Write a static method `randomAccessScoreProvider` that takes a `RandomAccessVectorValues` and a `VectorSimilarityFunction` as inputs and returns a `BuildScoreProvider` implementation. The returned provider should support exact score computations and provide an approximate centroid calculation by averaging all vectors from the input `RandomAccessVectorValues`. You should write self-contained code starting with: ```java\nstatic BuildScoreProvider randomAccessScoreProvider(RandomAccessVectorValues ravv, VectorSimilarityFunction similarityFunction) {```", "solution": "static BuildScoreProvider randomAccessScoreProvider(RandomAccessVectorValues ravv, VectorSimilarityFunction similarityFunction) {\n    return new BuildScoreProvider() {\n        @Override\n        public boolean isExact() {\n            return true;\n        }\n\n        @Override\n        public VectorFloat<?> approximateCentroid() {\n            var centroid = vts.createFloatVector(ravv.dimension());\n            int count = 0;\n            for (int i = 0; i < ravv.size(); i++) {\n                var v = ravv.getVector(i);\n                if (v != null) {\n                    VectorUtil.addInPlace(centroid, v);\n                    count++;\n                }\n            }\n            if (count > 0) {\n                VectorUtil.scale(centroid, 1.0f / count);\n            }\n            return centroid;\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(VectorFloat<?> vector) {\n            return DefaultSearchScoreProvider.exact(vector, similarityFunction, ravv);\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(int node1) {\n            var v = ravv.getVector(node1);\n            return searchProviderFor(v);\n        }\n\n        @Override\n        public SearchScoreProvider diversityProviderFor(int node1) {\n            var v = ravv.getVector(node1);\n            return DefaultSearchScoreProvider.exact(v, similarityFunction, ravv);\n        }\n    };\n}", "test": "@Test\npublic void testInsertDiverseConcurrent() {\n    // set up BSP\n    var sf = VectorSimilarityFunction.DOT_PRODUCT;\n    var vectors = new TestVectorGraph.CircularFloatVectorValues(10);\n    var natural = new NodeArray(10);\n    var concurrent = new NodeArray(10);\n    var bsp = BuildScoreProvider.randomAccessScoreProvider(vectors, sf);\n    // \"natural\" candidates are [0..7), \"concurrent\" are [8..10)\n    IntStream.range(0, 7)\n        .forEach(i -> natural.insertSorted(i, scoreBetween(bsp, 7, i)));\n    IntStream.range(8, 10)\n        .forEach(\n            i -> concurrent.insertSorted(i, scoreBetween(bsp, 7, i)));\n\n    // only nodes 6 and 8 are diverse wrt 7\n    var cnm = new ConcurrentNeighborMap(new VamanaDiversityProvider(bsp, 1.0f), 10, 10);\n    cnm.addNode(7);\n    var neighbors = cnm.insertDiverse(7, NodeArray.merge(natural, concurrent));\n    assertEquals(2, neighbors.size());\n    assert neighbors.contains(8);\n    assert neighbors.contains(6);\n    validateSortedByScore(neighbors);\n}"}, {"turn_num": 2, "instruction": "Modify the implementation to use thread-local instances of the vector values to avoid interference during concurrent access, ensuring thread safety when computing the centroid.", "solution": "static BuildScoreProvider randomAccessScoreProvider(RandomAccessVectorValues ravv, VectorSimilarityFunction similarityFunction) {\n    // We need two sources of vectors in order to perform diversity check comparisons without\n    // colliding.  ThreadLocalSupplier makes this a no-op if the RAVV is actually un-shared.\n    var vectors = ravv.threadLocalSupplier();\n    var vectorsCopy = ravv.threadLocalSupplier();\n\n    return new BuildScoreProvider() {\n        @Override\n        public boolean isExact() {\n            return true;\n        }\n\n        @Override\n        public VectorFloat<?> approximateCentroid() {\n            var vv = vectors.get();\n            var centroid = vts.createFloatVector(vv.dimension());\n            int count = 0;\n            for (int i = 0; i < vv.size(); i++) {\n                var v = vv.getVector(i);\n                if (v != null) {\n                    VectorUtil.addInPlace(centroid, v);\n                    count++;\n                }\n            }\n            if (count > 0) {\n                VectorUtil.scale(centroid, 1.0f / count);\n            }\n            return centroid;\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(VectorFloat<?> vector) {\n            var vc = vectorsCopy.get();\n            return DefaultSearchScoreProvider.exact(vector, similarityFunction, vc);\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(int node1) {\n            RandomAccessVectorValues randomAccessVectorValues = vectors.get();\n            var v = randomAccessVectorValues.getVector(node1);\n            return searchProviderFor(v);\n        }\n\n        @Override\n        public SearchScoreProvider diversityProviderFor(int node1) {\n            RandomAccessVectorValues randomAccessVectorValues = vectors.get();\n            var v = randomAccessVectorValues.getVector(node1);\n            var vc = vectorsCopy.get();\n            return DefaultSearchScoreProvider.exact(v, similarityFunction, vc);\n        }\n    };\n}", "test": "@Test\npublic void testInsertDiverseConcurrent() {\n  // Test thread-local vector access using existing test structure\n  var sf = VectorSimilarityFunction.DOT_PRODUCT;\n  var vectors = new TestVectorGraph.CircularFloatVectorValues(10);\n  var natural = new NodeArray(10);\n  var concurrent = new NodeArray(10);\n  var bsp = BuildScoreProvider.randomAccessScoreProvider(vectors, sf);\n  \n  // Test concurrent access to verify thread-local instances work correctly\n  var threads = new Thread[2];\n  var results = new boolean[2];\n  \n  for (int i = 0; i < 2; i++) {\n    final int threadId = i;\n    threads[i] = new Thread(() -> {\n      try {\n        // Each thread should get its own thread-local vector instance\n        IntStream.range(0, 7)\n            .forEach(j -> natural.insertSorted(j, scoreBetween(bsp, 7, j)));\n        IntStream.range(8, 10)\n            .forEach(j -> concurrent.insertSorted(j, scoreBetween(bsp, 7, j)));\n        \n        var cnm = new ConcurrentNeighborMap(new VamanaDiversityProvider(bsp, 1.0f), 10, 10);\n        cnm.addNode(7);\n        var neighbors = cnm.insertDiverse(7, NodeArray.merge(natural, concurrent));\n        \n        // Verify expected behavior with thread-local instances\n        results[threadId] = (neighbors.size() == 2 && neighbors.contains(8) && neighbors.contains(6));\n      } catch (Exception e) {\n        results[threadId] = false;\n      }\n    });\n  }\n  \n  // Start all threads and wait for completion\n  for (Thread thread : threads) {\n    thread.start();\n  }\n  \n  for (Thread thread : threads) {\n    try {\n      thread.join();\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n  }\n  \n  // Verify all threads completed successfully using assert statements\n  for (boolean result : results) {\n    assert result;\n  }\n}"}, {"turn_num": 3, "instruction": "Add support for a search score provider for a given vector, using exact similarity comparisons via the provided similarity function and a separate thread-local copy of the vector values to prevent conflicts during parallel execution.", "solution": "static BuildScoreProvider randomAccessScoreProvider(RandomAccessVectorValues ravv, VectorSimilarityFunction similarityFunction) {\n    // We need two sources of vectors in order to perform diversity check comparisons without\n    // colliding.  ThreadLocalSupplier makes this a no-op if the RAVV is actually un-shared.\n    var vectors = ravv.threadLocalSupplier();\n    var vectorsCopy = ravv.threadLocalSupplier();\n\n    return new BuildScoreProvider() {\n        @Override\n        public boolean isExact() {\n            return true;\n        }\n\n        @Override\n        public VectorFloat<?> approximateCentroid() {\n            var vv = vectors.get();\n            var centroid = vts.createFloatVector(vv.dimension());\n            int count = 0;\n            for (int i = 0; i < vv.size(); i++) {\n                var v = vv.getVector(i);\n                if (v != null) {\n                    VectorUtil.addInPlace(centroid, v);\n                    count++;\n                }\n            }\n            if (count > 0) {\n                VectorUtil.scale(centroid, 1.0f / count);\n            }\n            return centroid;\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(VectorFloat<?> vector) {\n            var vc = vectorsCopy.get();\n            return DefaultSearchScoreProvider.exact(vector, similarityFunction, vc);\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(int node1) {\n            RandomAccessVectorValues randomAccessVectorValues = vectors.get();\n            var v = randomAccessVectorValues.getVector(node1);\n            return searchProviderFor(v);\n        }\n\n        @Override\n        public SearchScoreProvider diversityProviderFor(int node1) {\n            RandomAccessVectorValues randomAccessVectorValues = vectors.get();\n            var v = randomAccessVectorValues.getVector(node1);\n            var vc = vectorsCopy.get();\n            return DefaultSearchScoreProvider.exact(v, similarityFunction, vc);\n        }\n    };\n}", "test": "@Test\npublic void testInsertDiverseConcurrent() {\n  // set up BSP\n  var sf = VectorSimilarityFunction.DOT_PRODUCT;\n  var vectors = new TestVectorGraph.CircularFloatVectorValues(10);\n  var natural = new NodeArray(10);\n  var concurrent = new NodeArray(10);\n  var bsp = BuildScoreProvider.randomAccessScoreProvider(vectors, sf);\n  \n  // Test search score provider for a given vector with exact similarity comparisons\n  var queryVector = vectors.getVector(0);\n  var searchProvider = bsp.searchProviderFor(queryVector);\n  \n  // Test that the search provider uses exact similarity function\n  assert bsp.isExact();\n  \n  // Test parallel execution with separate thread-local copies\n  var threads = new Thread[2];\n  var success = new boolean[2];\n  \n  for (int t = 0; t < 2; t++) {\n    final int threadId = t;\n    threads[t] = new Thread(() -> {\n      try {\n        var threadSearchProvider = bsp.searchProviderFor(queryVector);\n        success[threadId] = (threadSearchProvider != null);\n      } catch (Exception e) {\n        success[threadId] = false;\n      }\n    });\n  }\n  \n  for (Thread thread : threads) {\n    thread.start();\n  }\n  \n  for (Thread thread : threads) {\n    try {\n      thread.join();\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n  }\n  \n  // Verify all threads successfully created search providers\n  for (boolean result : success) {\n    assert result;\n  }\n  \n  // Test the original functionality still works\n  IntStream.range(0, 7)\n      .forEach(i -> natural.insertSorted(i, scoreBetween(bsp, 7, i)));\n  IntStream.range(8, 10)\n      .forEach(\n          i -> concurrent.insertSorted(i, scoreBetween(bsp, 7, i)));\n\n  var cnm = new ConcurrentNeighborMap(new VamanaDiversityProvider(bsp, 1.0f), 10, 10);\n  cnm.addNode(7);\n  var neighbors = cnm.insertDiverse(7, NodeArray.merge(natural, concurrent));\n  assertEquals(2, neighbors.size());\n  assert neighbors.contains(8);\n  assert neighbors.contains(6);\n  validateSortedByScore(neighbors);\n}"}, {"turn_num": 4, "instruction": "Implement the `searchProviderFor(int node1)` method to return a search score provider for the vector at the given node index, retrieving the vector using a thread-local instance of the vector values.", "solution": "static BuildScoreProvider randomAccessScoreProvider(RandomAccessVectorValues ravv, VectorSimilarityFunction similarityFunction) {\n    // We need two sources of vectors in order to perform diversity check comparisons without\n    // colliding.  ThreadLocalSupplier makes this a no-op if the RAVV is actually un-shared.\n    var vectors = ravv.threadLocalSupplier();\n    var vectorsCopy = ravv.threadLocalSupplier();\n\n    return new BuildScoreProvider() {\n        @Override\n        public boolean isExact() {\n            return true;\n        }\n\n        @Override\n        public VectorFloat<?> approximateCentroid() {\n            var vv = vectors.get();\n            var centroid = vts.createFloatVector(vv.dimension());\n            int count = 0;\n            for (int i = 0; i < vv.size(); i++) {\n                var v = vv.getVector(i);\n                if (v != null) {\n                    VectorUtil.addInPlace(centroid, v);\n                    count++;\n                }\n            }\n            if (count > 0) {\n                VectorUtil.scale(centroid, 1.0f / count);\n            }\n            return centroid;\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(VectorFloat<?> vector) {\n            var vc = vectorsCopy.get();\n            return DefaultSearchScoreProvider.exact(vector, similarityFunction, vc);\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(int node1) {\n            var vv = vectors.get();\n            var v = vv.getVector(node1);\n            return searchProviderFor(v);\n        }\n\n        @Override\n        public SearchScoreProvider diversityProviderFor(int node1) {\n            RandomAccessVectorValues randomAccessVectorValues = vectors.get();\n            var v = randomAccessVectorValues.getVector(node1);\n            var vc = vectorsCopy.get();\n            return DefaultSearchScoreProvider.exact(v, similarityFunction, vc);\n        }\n    };\n}", "test": "@Test\npublic void testInsertDiverseConcurrent() {\n  // set up BSP\n  var sf = VectorSimilarityFunction.DOT_PRODUCT;\n  var vectors = new TestVectorGraph.CircularFloatVectorValues(10);\n  var natural = new NodeArray(10);\n  var concurrent = new NodeArray(10);\n  var bsp = BuildScoreProvider.randomAccessScoreProvider(vectors, sf);\n  \n  // Test searchProviderFor(int node1) - verify it returns a non-null SearchScoreProvider\n  var searchProvider0 = bsp.searchProviderFor(0);\n  var searchProvider1 = bsp.searchProviderFor(1);\n  \n  // Verify the search providers are created successfully\n  assert searchProvider0 != null;\n  assert searchProvider1 != null;\n  \n  // \"natural\" candidates are [0..7), \"concurrent\" are [8..10)\n  IntStream.range(0, 7)\n      .forEach(i -> natural.insertSorted(i, scoreBetween(bsp, 7, i)));\n  IntStream.range(8, 10)\n      .forEach(\n          i -> concurrent.insertSorted(i, scoreBetween(bsp, 7, i)));\n\n  // only nodes 6 and 8 are diverse wrt 7\n  var cnm = new ConcurrentNeighborMap(new VamanaDiversityProvider(bsp, 1.0f), 10, 10);\n  cnm.addNode(7);\n  var neighbors = cnm.insertDiverse(7, NodeArray.merge(natural, concurrent));\n  assertEquals(2, neighbors.size());\n  assert neighbors.contains(8);\n  assert neighbors.contains(6);\n  validateSortedByScore(neighbors);\n}"}, {"turn_num": 5, "instruction": "Implement the `diversityProviderFor(int node1)` method to return a search score provider that uses the vector at the given node index and a separate thread-local copy of the vector values for comparison, enabling diversity checks without interference.", "solution": "static BuildScoreProvider randomAccessScoreProvider(RandomAccessVectorValues ravv, VectorSimilarityFunction similarityFunction) {\n    // We need two sources of vectors in order to perform diversity check comparisons without\n    // colliding.  ThreadLocalSupplier makes this a no-op if the RAVV is actually un-shared.\n    var vectors = ravv.threadLocalSupplier();\n    var vectorsCopy = ravv.threadLocalSupplier();\n\n    return new BuildScoreProvider() {\n        @Override\n        public boolean isExact() {\n            return true;\n        }\n\n        @Override\n        public VectorFloat<?> approximateCentroid() {\n            var vv = vectors.get();\n            var centroid = vts.createFloatVector(vv.dimension());\n            int count = 0;\n            for (int i = 0; i < vv.size(); i++) {\n                var v = vv.getVector(i);\n                if (v != null) {\n                    VectorUtil.addInPlace(centroid, v);\n                    count++;\n                }\n            }\n            if (count > 0) {\n                VectorUtil.scale(centroid, 1.0f / count);\n            }\n            return centroid;\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(VectorFloat<?> vector) {\n            var vc = vectorsCopy.get();\n            return DefaultSearchScoreProvider.exact(vector, similarityFunction, vc);\n        }\n\n        @Override\n        public SearchScoreProvider searchProviderFor(int node1) {\n            var vv = vectors.get();\n            var v = vv.getVector(node1);\n            return searchProviderFor(v);\n        }\n\n        @Override\n        public SearchScoreProvider diversityProviderFor(int node1) {\n            var vv = vectors.get();\n            var v = vv.getVector(node1);\n            var vc = vectorsCopy.get();\n            return DefaultSearchScoreProvider.exact(v, similarityFunction, vc);\n        }\n    };\n}", "test": "@Test\npublic void testInsertDiverseConcurrent() {\n  // set up BSP\n  var sf = VectorSimilarityFunction.DOT_PRODUCT;\n  var vectors = new TestVectorGraph.CircularFloatVectorValues(10);\n  var natural = new NodeArray(10);\n  var concurrent = new NodeArray(10);\n  var bsp = BuildScoreProvider.randomAccessScoreProvider(vectors, sf);\n  // \"natural\" candidates are [0..7), \"concurrent\" are [8..10)\n  IntStream.range(0, 7)\n      .forEach(i -> natural.insertSorted(i, scoreBetween(bsp, 7, i)));\n  IntStream.range(8, 10)\n      .forEach(\n          i -> concurrent.insertSorted(i, scoreBetween(bsp, 7, i)));\n\n  // Test diversityProviderFor specifically - verify it returns a non-null provider\n  var diversityProvider = bsp.diversityProviderFor(7);\n  assert diversityProvider != null;\n  \n  // Verify diversityProviderFor uses the vector at node1 with separate thread-local copy\n  var regularProvider = bsp.searchProviderFor(7);\n  assert regularProvider != null;\n  \n  // only nodes 6 and 8 are diverse wrt 7\n  var cnm = new ConcurrentNeighborMap(new VamanaDiversityProvider(bsp, 1.0f), 10, 10);\n  cnm.addNode(7);\n  var neighbors = cnm.insertDiverse(7, NodeArray.merge(natural, concurrent));\n  assertEquals(2, neighbors.size());\n  assert neighbors.contains(8);\n  assert neighbors.contains(6);\n  validateSortedByScore(neighbors);\n}"}], "metadata": {"source": "mrgbench", "func": "    static BuildScoreProvider randomAccessScoreProvider(RandomAccessVectorValues ravv, VectorSimilarityFunction similarityFunction) {\n        // We need two sources of vectors in order to perform diversity check comparisons without\n        // colliding.  ThreadLocalSupplier makes this a no-op if the RAVV is actually un-shared.\n        var vectors = ravv.threadLocalSupplier();\n        var vectorsCopy = ravv.threadLocalSupplier();\n\n        return new BuildScoreProvider() {\n            @Override\n            public boolean isExact() {\n                return true;\n            }\n\n            @Override\n            public VectorFloat<?> approximateCentroid() {\n                var vv = vectors.get();\n                var centroid = vts.createFloatVector(vv.dimension());\n                for (int i = 0; i < vv.size(); i++) {\n                    var v = vv.getVector(i);\n                    if (v != null) { // MapRandomAccessVectorValues is not necessarily dense\n                        VectorUtil.addInPlace(centroid, v);\n                    }\n                }\n                VectorUtil.scale(centroid, 1.0f / vv.size());\n                return centroid;\n            }\n\n            @Override\n            public SearchScoreProvider searchProviderFor(VectorFloat<?> vector) {\n                var vc = vectorsCopy.get();\n                return DefaultSearchScoreProvider.exact(vector, similarityFunction, vc);\n            }\n\n            @Override\n            public SearchScoreProvider searchProviderFor(int node1) {\n                RandomAccessVectorValues randomAccessVectorValues = vectors.get();\n                var v = randomAccessVectorValues.getVector(node1);\n                return searchProviderFor(v);\n            }\n\n            @Override\n            public SearchScoreProvider diversityProviderFor(int node1) {\n                RandomAccessVectorValues randomAccessVectorValues = vectors.get();\n                var v = randomAccessVectorValues.getVector(node1);\n                var vc = vectorsCopy.get();\n                return DefaultSearchScoreProvider.exact(v, similarityFunction, vc);\n            }\n        };\n    }", "repo": "jvector", "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/similarity/BuildScoreProvider.java", "func_name": "randomAccessScoreProvider", "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph.similarity;\n\nimport io.github.jbellis.jvector.graph.RandomAccessVectorValues;\nimport io.github.jbellis.jvector.quantization.BQVectors;\nimport io.github.jbellis.jvector.quantization.PQVectors;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.VectorUtil;\nimport io.github.jbellis.jvector.vector.VectorizationProvider;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\n/**\n * Encapsulates comparing node distances for GraphIndexBuilder.\n */\npublic interface BuildScoreProvider {\n    VectorTypeSupport vts = VectorizationProvider.getInstance().getVectorTypeSupport();\n\n    /**\n     * @return true if the primary score functions used for construction are exact.  This\n     * is modestly redundant, but it saves having to allocate new Search/Diversity provider\n     * objects in some hot construction loops.\n     */\n    boolean isExact();\n\n    /**\n     * @return the approximate centroid of the known nodes.  We use the closest node\n     * to this centroid as the graph entry point, so this is called when the entry point is deleted\n     * or every time the graph size doubles.\n     * <p>\n     * This is not called on a path that blocks searches or modifications, so it is okay for it to be O(N).\n     */\n    VectorFloat<?> approximateCentroid();\n\n    /**\n     * Create a search score provider to use *internally* during construction.\n     * <p>\n     * \"Internally\" means that this may differ from a typical SSP in that it may use\n     * approximate scores *without* reranking.  (In this case, reranking will be done\n     * separately by the ConcurrentNeighborSet diversity code.)\n     * <p>\n     * @param vector the query vector to provide similarity scores against\n     */\n    SearchScoreProvider searchProviderFor(VectorFloat<?> vector);\n\n    /**\n     * Create a search score provider to use *internally* during construction.\n     * <p>\n     * \"Internally\" means that this may differ from a typical SSP in that it may use\n     * approximate scores *without* reranking.  (In this case, reranking will be done\n     * separately by the ConcurrentNeighborSet diversity code.)\n     * <p>\n     * @param node1 the graph node to provide similarity scores against\n     */\n    SearchScoreProvider searchProviderFor(int node1);\n\n    /**\n     * Create a score provider to use internally during construction.\n     * <p>\n     * The difference between the diversity provider and the search provider is\n     * that the diversity provider is only expected to be used a few dozen times per node,\n     * which influences the implementation choices.\n     * <p>\n     * When scoring is approximate, the scores from the search and diversity provider\n     * must be consistent, i.e. mixing different types of CompressedVectors will cause problems.\n     */\n    SearchScoreProvider diversityProviderFor(int node1);", "func_start": 87, "func_end": 134, "body_len": 47, "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java", "test_start": 59, "test_end": 82, "test_code": "  @Test\n  public void testInsertDiverseConcurrent() {\n    // set up BSP\n    var sf = VectorSimilarityFunction.DOT_PRODUCT;\n    var vectors = new TestVectorGraph.CircularFloatVectorValues(10);\n    var natural = new NodeArray(10);\n    var concurrent = new NodeArray(10);\n    var bsp = BuildScoreProvider.randomAccessScoreProvider(vectors, sf);\n    // \"natural\" candidates are [0..7), \"concurrent\" are [8..10)\n    IntStream.range(0, 7)\n        .forEach(i -> natural.insertSorted(i, scoreBetween(bsp, 7, i)));\n    IntStream.range(8, 10)\n        .forEach(\n            i -> concurrent.insertSorted(i, scoreBetween(bsp, 7, i)));\n\n    // only nodes 6 and 8 are diverse wrt 7\n    var cnm = new ConcurrentNeighborMap(new VamanaDiversityProvider(bsp, 1.0f), 10, 10);\n    cnm.addNode(7);\n    var neighbors = cnm.insertDiverse(7, NodeArray.merge(natural, concurrent));\n    assertEquals(2, neighbors.size());\n    assert neighbors.contains(8);\n    assert neighbors.contains(6);\n    validateSortedByScore(neighbors);\n  }", "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNeighbors#testInsertDiverseConcurrent\" -Dsurefire.failIfNoSpecifiedTests=false", "language": "java"}, "mt_id": 1054}
{"hash_id": "b0b8d5adc9c586600826f226", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `determineStandardDeviationDoubles` that takes a list of `BenchmarkResult` objects, a `Score` object called `averageScore`, and an integer `successCount`. Assume valid inputs: `averageScore` is not null, `successCount` is positive, and there is at least one successful benchmark result. The function should compute the standard deviation of the average scores from the successful results (where `hasAllSuccess()` is true) relative to `averageScore`. For each level in the score (using `toLevelDoubles()`), calculate the squared differences in double precision, sum them, divide by `successCount`, and take the square root using `Math.pow(..., 0.5)`. Return a double array of standard deviations per level. You should write self-contained code starting with: ```java\npublic static double[] determineStandardDeviationDoubles(List<? extends BenchmarkResult> benchmarkResultList, Score averageScore, int successCount) {```", "solution": "public static double[] determineStandardDeviationDoubles(\n        List<? extends BenchmarkResult> benchmarkResultList, Score averageScore, int successCount) {\n    double[] differenceSquaredTotalDoubles = null;\n    \n    for (BenchmarkResult benchmarkResult : benchmarkResultList) {\n        if (benchmarkResult.hasAllSuccess()) {\n            Score difference = benchmarkResult.getAverageScore().subtract(averageScore);\n            double[] differenceDoubles = difference.toLevelDoubles();\n            \n            if (differenceSquaredTotalDoubles == null) {\n                differenceSquaredTotalDoubles = new double[differenceDoubles.length];\n            }\n            \n            for (int i = 0; i < differenceDoubles.length; i++) {\n                differenceSquaredTotalDoubles[i] += Math.pow(differenceDoubles[i], 2.0);\n            }\n        }\n    }\n    \n    double[] standardDeviationDoubles = new double[differenceSquaredTotalDoubles.length];\n    for (int i = 0; i < differenceSquaredTotalDoubles.length; i++) {\n        standardDeviationDoubles[i] = Math.pow(differenceSquaredTotalDoubles[i] / successCount, 0.5);\n    }\n    \n    return standardDeviationDoubles;\n}", "test": "@Test\nvoid singleDetermineStandardDeviationDoubles() {\n    List<SubSingleBenchmarkResult> subSingleBenchmarkResultList = Arrays\n            .asList(createSubSingleBenchmarkResult(SimpleScore.of(0), 0));\n    assertThat(StatisticUtils.determineStandardDeviationDoubles(subSingleBenchmarkResultList,\n            SimpleScore.of(0), subSingleBenchmarkResultList.size()))\n            .containsSequence(new double[] { 0d }, offset(DELTA));\n}"}, {"turn_num": 2, "instruction": "Add a check at the start of the function: if `averageScore` is null, throw an IllegalArgumentException with the message 'Average score (null) cannot be null.' If `successCount` is zero or negative, return an empty double array immediately, before processing any benchmark results.", "solution": "public static double[] determineStandardDeviationDoubles(\n        List<? extends BenchmarkResult> benchmarkResultList, Score averageScore, int successCount) {\n    if (averageScore == null) {\n        throw new IllegalArgumentException(\"Average score (null) cannot be null.\");\n    }\n    if (successCount <= 0) {\n        return new double[0];\n    }\n    \n    double[] differenceSquaredTotalDoubles = null;\n    \n    for (BenchmarkResult benchmarkResult : benchmarkResultList) {\n        if (benchmarkResult.hasAllSuccess()) {\n            Score difference = benchmarkResult.getAverageScore().subtract(averageScore);\n            double[] differenceDoubles = difference.toLevelDoubles();\n            \n            if (differenceSquaredTotalDoubles == null) {\n                differenceSquaredTotalDoubles = new double[differenceDoubles.length];\n            }\n            \n            for (int i = 0; i < differenceDoubles.length; i++) {\n                differenceSquaredTotalDoubles[i] += Math.pow(differenceDoubles[i], 2.0);\n            }\n        }\n    }\n    \n    if (differenceSquaredTotalDoubles == null) {\n        return new double[0];\n    }\n    \n    double[] standardDeviationDoubles = new double[differenceSquaredTotalDoubles.length];\n    for (int i = 0; i < differenceSquaredTotalDoubles.length; i++) {\n        standardDeviationDoubles[i] = Math.pow(differenceSquaredTotalDoubles[i] / successCount, 0.5);\n    }\n    \n    return standardDeviationDoubles;\n}", "test": "@Test\nvoid singleDetermineStandardDeviationDoubles() {\n    List<SubSingleBenchmarkResult> subSingleBenchmarkResultList = Arrays\n            .asList(createSubSingleBenchmarkResult(SimpleScore.of(0), 0));\n    \n    // Test zero successCount returns empty array\n    assertThat(StatisticUtils.determineStandardDeviationDoubles(\n            subSingleBenchmarkResultList, SimpleScore.of(0), 0))\n            .isEmpty();\n    \n    // Test negative successCount returns empty array\n    assertThat(StatisticUtils.determineStandardDeviationDoubles(\n            subSingleBenchmarkResultList, SimpleScore.of(0), -1))\n            .isEmpty();\n    \n    // Test normal case still works\n    assertThat(StatisticUtils.determineStandardDeviationDoubles(subSingleBenchmarkResultList,\n            SimpleScore.of(0), subSingleBenchmarkResultList.size()))\n            .containsSequence(new double[] { 0d }, offset(DELTA));\n}"}, {"turn_num": 3, "instruction": "After processing all benchmark results, if no successful benchmarks were found (i.e., no result where `hasAllSuccess()` is true), return an empty double array regardless of the `successCount` value.", "solution": "public static double[] determineStandardDeviationDoubles(\n        List<? extends BenchmarkResult> benchmarkResultList, Score averageScore, int successCount) {\n    if (averageScore == null) {\n        throw new IllegalArgumentException(\"Average score (null) cannot be null.\");\n    }\n    if (successCount <= 0) {\n        return new double[0];\n    }\n    \n    double[] differenceSquaredTotalDoubles = null;\n    \n    for (BenchmarkResult benchmarkResult : benchmarkResultList) {\n        if (benchmarkResult.hasAllSuccess()) {\n            Score difference = benchmarkResult.getAverageScore().subtract(averageScore);\n            double[] differenceDoubles = difference.toLevelDoubles();\n            \n            if (differenceSquaredTotalDoubles == null) {\n                differenceSquaredTotalDoubles = new double[differenceDoubles.length];\n            }\n            \n            for (int i = 0; i < differenceDoubles.length; i++) {\n                differenceSquaredTotalDoubles[i] += Math.pow(differenceDoubles[i], 2.0);\n            }\n        }\n    }\n    \n    if (differenceSquaredTotalDoubles == null) {\n        return new double[0];\n    }\n    \n    double[] standardDeviationDoubles = new double[differenceSquaredTotalDoubles.length];\n    for (int i = 0; i < differenceSquaredTotalDoubles.length; i++) {\n        standardDeviationDoubles[i] = Math.pow(differenceSquaredTotalDoubles[i] / successCount, 0.5);\n    }\n    \n    return standardDeviationDoubles;\n}", "test": "@Test\nvoid singleDetermineStandardDeviationDoubles() {\n    // Test the original successful case\n    List<SubSingleBenchmarkResult> subSingleBenchmarkResultList = Arrays\n            .asList(createSubSingleBenchmarkResult(SimpleScore.of(0), 0));\n    assertThat(StatisticUtils.determineStandardDeviationDoubles(subSingleBenchmarkResultList,\n            SimpleScore.of(0), subSingleBenchmarkResultList.size()))\n            .containsSequence(new double[] { 0d }, offset(DELTA));\n    \n    // Test case for current instruction: when no successful benchmarks exist, return empty array\n    // Use an empty list to ensure no successful benchmarks\n    List<SubSingleBenchmarkResult> emptyBenchmarks = Arrays.asList();\n    assertThat(StatisticUtils.determineStandardDeviationDoubles(emptyBenchmarks,\n            SimpleScore.of(0), 5)) // successCount > 0 but should still return empty\n            .isEmpty();\n}"}], "metadata": {"source": "mrgbench", "func": "    public static double[] determineStandardDeviationDoubles(\n            List<? extends BenchmarkResult> benchmarkResultList, Score averageScore, int successCount) {\n        if (successCount <= 0) {\n            return new double[0];\n        }\n        if (averageScore == null) {\n            throw new IllegalArgumentException(\"Average score (\" + averageScore + \") cannot be null.\");\n        }\n        // averageScore can no longer be null\n        double[] differenceSquaredTotalDoubles = null;\n        for (BenchmarkResult benchmarkResult : benchmarkResultList) {\n            if (benchmarkResult.hasAllSuccess()) {\n                Score difference = benchmarkResult.getAverageScore().subtract(averageScore);\n                // Calculations done on doubles to avoid common overflow when executing with an int score > 500 000\n                double[] differenceDoubles = difference.toLevelDoubles();\n                if (differenceSquaredTotalDoubles == null) {\n                    differenceSquaredTotalDoubles = new double[differenceDoubles.length];\n                }\n                for (int i = 0; i < differenceDoubles.length; i++) {\n                    differenceSquaredTotalDoubles[i] += Math.pow(differenceDoubles[i], 2.0);\n                }\n            }\n        }\n\n        if (differenceSquaredTotalDoubles == null) { // no successful benchmarks\n            return new double[0];\n        }\n\n        double[] standardDeviationDoubles = new double[differenceSquaredTotalDoubles.length];\n        for (int i = 0; i < differenceSquaredTotalDoubles.length; i++) {\n            standardDeviationDoubles[i] = Math.pow(differenceSquaredTotalDoubles[i] / successCount, 0.5);\n        }\n        return standardDeviationDoubles;\n    }", "repo": "timefold-solver", "file_path": "benchmark/src/main/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtils.java", "func_name": "determineStandardDeviationDoubles", "context": "package ai.timefold.solver.benchmark.impl.statistic;\n\nimport java.text.DecimalFormat;\nimport java.text.DecimalFormatSymbols;\nimport java.util.List;\nimport java.util.Locale;\n\nimport ai.timefold.solver.benchmark.impl.result.BenchmarkResult;\nimport ai.timefold.solver.core.api.score.Score;\n\npublic class StatisticUtils {\n\n    private StatisticUtils() {\n        // This class is not instantiable\n    }", "func_start": 23, "func_end": 56, "body_len": 33, "test_file": "benchmark/src/test/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtilsTest.java", "test_start": 23, "test_end": 30, "test_code": "    @Test\n    void singleDetermineStandardDeviationDoubles() {\n        List<SubSingleBenchmarkResult> subSingleBenchmarkResultList = Arrays\n                .asList(createSubSingleBenchmarkResult(SimpleScore.of(0), 0));\n        assertThat(StatisticUtils.determineStandardDeviationDoubles(subSingleBenchmarkResultList,\n                SimpleScore.of(0), subSingleBenchmarkResultList.size()))\n                .containsSequence(new double[] { 0d }, offset(DELTA));\n    }", "test_instruction": "mvn test -pl benchmark -Dtest=\"ai.timefold.solver.benchmark.impl.statistic.StatisticUtilsTest#singleDetermineStandardDeviationDoubles\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1055}
{"hash_id": "93ea73deeb0d977a08c039c9", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `createAccessorFor` that takes a Member, an annotation class, a boolean for return type requirement, and a GizmoClassLoader, and returns a MemberAccessor instance by generating bytecode for an accessor class using Gizmo. Use a ClassOutput that captures the generated bytecode, define the class with member information, and create a new instance of the generated class. You should write self-contained code starting with: ```java\nstatic MemberAccessor createAccessorFor(Member member, Class<? extends Annotation> annotationClass, boolean returnTypeRequired, GizmoClassLoader gizmoClassLoader) { ```", "solution": "static MemberAccessor createAccessorFor(Member member, Class<? extends Annotation> annotationClass, boolean returnTypeRequired, GizmoClassLoader gizmoClassLoader) {\n    String className = GizmoMemberAccessorFactory.getGeneratedClassName(member);\n    if (gizmoClassLoader.hasBytecodeFor(className)) {\n        return createInstance(className, gizmoClassLoader);\n    }\n    final MutableReference<byte[]> classBytecodeHolder = new MutableReference<>(null);\n    ClassOutput classOutput = (path, byteCode) -> classBytecodeHolder.setValue(byteCode);\n    GizmoMemberInfo memberInfo = new GizmoMemberInfo(new GizmoMemberDescriptor(member), returnTypeRequired, annotationClass);\n    defineAccessorFor(className, classOutput, memberInfo);\n    byte[] classBytecode = classBytecodeHolder.getValue();\n    \n    gizmoClassLoader.storeBytecode(className, classBytecode);\n    return createInstance(className, gizmoClassLoader);\n}", "test": "@Test\nvoid testThrowsWhenGetterMethodHasParameters() throws NoSuchMethodException {\n    Member member = GizmoTestdataEntity.class.getMethod(\"methodWithParameters\", String.class);\n    assertThatCode(() -> {\n        GizmoMemberAccessorImplementor.createAccessorFor(member, PlanningVariable.class, true, new GizmoClassLoader());\n    }).hasMessage(\"The getterMethod (methodWithParameters) with a PlanningVariable annotation \" +\n            \"must not have any parameters, but has parameters ([Ljava/lang/String;]).\");\n}"}, {"turn_num": 2, "instruction": "Derive the className using GizmoMemberAccessorFactory.getGeneratedClassName(member) instead of assuming it. Before generating bytecode, check if the GizmoClassLoader already has bytecode for the generated class name. If it does, skip generation and directly create and return a MemberAccessor instance using createInstance.", "solution": "static MemberAccessor createAccessorFor(Member member, Class<? extends Annotation> annotationClass,\n        boolean returnTypeRequired, GizmoClassLoader gizmoClassLoader) {\n    String className = GizmoMemberAccessorFactory.getGeneratedClassName(member);\n    if (gizmoClassLoader.hasBytecodeFor(className)) {\n        return createInstance(className, gizmoClassLoader);\n    }\n    final MutableReference<byte[]> classBytecodeHolder = new MutableReference<>(null);\n    ClassOutput classOutput = (path, byteCode) -> classBytecodeHolder.setValue(byteCode);\n    GizmoMemberInfo memberInfo =\n            new GizmoMemberInfo(new GizmoMemberDescriptor(member), returnTypeRequired, annotationClass);\n    defineAccessorFor(className, classOutput, memberInfo);\n    byte[] classBytecode = classBytecodeHolder.getValue();\n\n    gizmoClassLoader.storeBytecode(className, classBytecode);\n    return createInstance(className, gizmoClassLoader);\n}", "test": "@Test\nvoid testThrowsWhenGetterMethodHasParameters() throws NoSuchMethodException {\n    Member member = GizmoTestdataEntity.class.getMethod(\"methodWithParameters\", String.class);\n    assertThatCode(() -> {\n        GizmoMemberAccessorImplementor.createAccessorFor(member, PlanningVariable.class, true, new GizmoClassLoader());\n    }).hasMessage(\"The getterMethod (methodWithParameters) with a PlanningVariable annotation \" +\n            \"must not have any parameters, but has parameters ([Ljava/lang/String;]).\");\n}"}, {"turn_num": 3, "instruction": "After generating the bytecode, store it in the GizmoClassLoader using gizmoClassLoader.storeBytecode(className, classBytecode) so future calls can reuse it.", "solution": "static MemberAccessor createAccessorFor(Member member, Class<? extends Annotation> annotationClass,\n        boolean returnTypeRequired, GizmoClassLoader gizmoClassLoader) {\n    String className = GizmoMemberAccessorFactory.getGeneratedClassName(member);\n    if (gizmoClassLoader.hasBytecodeFor(className)) {\n        return createInstance(className, gizmoClassLoader);\n    }\n    final MutableReference<byte[]> classBytecodeHolder = new MutableReference<>(null);\n    ClassOutput classOutput = (path, byteCode) -> {\n        classBytecodeHolder.setValue(byteCode);\n        gizmoClassLoader.storeBytecode(className, byteCode);\n    };\n    GizmoMemberInfo memberInfo =\n            new GizmoMemberInfo(new GizmoMemberDescriptor(member), returnTypeRequired, annotationClass);\n    defineAccessorFor(className, classOutput, memberInfo);\n    byte[] classBytecode = classBytecodeHolder.getValue();\n\n    return createInstance(className, gizmoClassLoader);\n}", "test": "@Test\nvoid testThrowsWhenGetterMethodHasParameters() throws NoSuchMethodException {\n    Member member = GizmoTestdataEntity.class.getMethod(\"methodWithParameters\", String.class);\n    GizmoClassLoader gizmoClassLoader = new GizmoClassLoader();\n    \n    // First call should generate and store bytecode\n    assertThatCode(() -> {\n        GizmoMemberAccessorImplementor.createAccessorFor(member, PlanningVariable.class, true, gizmoClassLoader);\n    }).hasMessage(\"The getterMethod (methodWithParameters) with a PlanningVariable annotation \" +\n            \"must not have any parameters, but has parameters ([Ljava/lang/String;]).\");\n    \n    // Verify bytecode was stored despite the exception\n    String className = GizmoMemberAccessorFactory.getGeneratedClassName(member);\n    assertThat(gizmoClassLoader.hasBytecodeFor(className)).isTrue();\n}"}], "metadata": {"source": "mrgbench", "func": "    static MemberAccessor createAccessorFor(Member member, Class<? extends Annotation> annotationClass,\n            boolean returnTypeRequired, GizmoClassLoader gizmoClassLoader) {\n        String className = GizmoMemberAccessorFactory.getGeneratedClassName(member);\n        if (gizmoClassLoader.hasBytecodeFor(className)) {\n            return createInstance(className, gizmoClassLoader);\n        }\n        final MutableReference<byte[]> classBytecodeHolder = new MutableReference<>(null);\n        ClassOutput classOutput = (path, byteCode) -> classBytecodeHolder.setValue(byteCode);\n        GizmoMemberInfo memberInfo =\n                new GizmoMemberInfo(new GizmoMemberDescriptor(member), returnTypeRequired, annotationClass);\n        defineAccessorFor(className, classOutput, memberInfo);\n        byte[] classBytecode = classBytecodeHolder.getValue();\n\n        gizmoClassLoader.storeBytecode(className, classBytecode);\n        return createInstance(className, gizmoClassLoader);\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementor.java", "func_name": "createAccessorFor", "context": "package ai.timefold.solver.core.impl.domain.common.accessor.gizmo;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.AnnotatedElement;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Modifier;\nimport java.lang.reflect.Type;\nimport java.util.Arrays;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\nimport ai.timefold.solver.core.impl.domain.common.accessor.MemberAccessor;\nimport ai.timefold.solver.core.impl.util.MutableReference;\n\nimport io.quarkus.gizmo.ClassCreator;\nimport io.quarkus.gizmo.ClassOutput;\nimport io.quarkus.gizmo.FieldDescriptor;\nimport io.quarkus.gizmo.MethodCreator;\nimport io.quarkus.gizmo.MethodDescriptor;\nimport io.quarkus.gizmo.ResultHandle;\n\n/**\n * Generates the bytecode for the MemberAccessor of a particular Member\n */\npublic final class GizmoMemberAccessorImplementor {\n\n    final static String GENERIC_TYPE_FIELD = \"genericType\";\n    final static String ANNOTATED_ELEMENT_FIELD = \"annotatedElement\";\n\n    /**\n     * Generates the constructor and implementations of {@link AbstractGizmoMemberAccessor} methods for the given\n     * {@link Member}.\n     *\n     * @param className never null\n     * @param classOutput never null, defines how to write the bytecode\n     * @param memberInfo never null, member to generate MemberAccessor methods implementation for\n     */\n    public static void defineAccessorFor(String className, ClassOutput classOutput, GizmoMemberInfo memberInfo) {\n        Class<? extends AbstractGizmoMemberAccessor> superClass = getCorrectSuperclass(memberInfo);\n        try (ClassCreator classCreator = ClassCreator.builder()\n                .className(className)\n                .superClass(superClass)\n                .classOutput(classOutput)\n                .setFinal(true)\n                .build()) {\n            classCreator.getFieldCreator(\"genericType\", Type.class)\n                    .setModifiers(Modifier.FINAL);\n            classCreator.getFieldCreator(\"annotatedElement\", AnnotatedElement.class)\n                    .setModifiers(Modifier.FINAL);\n\n            // ************************************************************************\n            // MemberAccessor methods\n            // ************************************************************************\n            createConstructor(classCreator, memberInfo);\n            createGetDeclaringClass(classCreator, memberInfo);\n            createGetType(classCreator, memberInfo);\n            createGetGenericType(classCreator);\n            createGetName(classCreator, memberInfo);\n            createExecuteGetter(classCreator, memberInfo);\n            if (superClass == AbstractReadWriteGizmoMemberAccessor.class) {\n                createExecuteSetter(classCreator, memberInfo);\n            }\n            createGetAnnotation(classCreator);\n            createDeclaredAnnotationsByType(classCreator);\n        }\n    }\n\n    private static Class<? extends AbstractGizmoMemberAccessor> getCorrectSuperclass(GizmoMemberInfo memberInfo) {\n        AtomicBoolean supportsSetter = new AtomicBoolean();\n        memberInfo.descriptor().whenIsMethod(method -> {\n            supportsSetter.set(memberInfo.descriptor().getSetter().isPresent());\n        });\n        memberInfo.descriptor().whenIsField(field -> {\n            supportsSetter.set(true);\n        });\n        if (supportsSetter.get()) {\n            return AbstractReadWriteGizmoMemberAccessor.class;\n        } else {\n            return AbstractReadOnlyGizmoMemberAccessor.class;\n        }\n    }", "func_start": 99, "func_end": 114, "body_len": 15, "test_file": "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java", "test_start": 134, "test_end": 141, "test_code": "    @Test\n    void testThrowsWhenGetterMethodHasParameters() throws NoSuchMethodException {\n        Member member = GizmoTestdataEntity.class.getMethod(\"methodWithParameters\", String.class);\n        assertThatCode(() -> {\n            GizmoMemberAccessorImplementor.createAccessorFor(member, PlanningVariable.class, true, new GizmoClassLoader());\n        }).hasMessage(\"The getterMethod (methodWithParameters) with a PlanningVariable annotation \" +\n                \"must not have any parameters, but has parameters ([Ljava/lang/String;]).\");\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.domain.common.accessor.gizmo.GizmoMemberAccessorImplementorTest#testThrowsWhenGetterMethodHasParameters\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1056}
{"hash_id": "4959408a29d07ab85f1fdf99", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `set` that takes a new best solution of type `Solution_` and updates the current best solution. You should write self-contained code starting with: ```java\nvoid set(Solution_ bestSolution) {\n}```", "solution": "void set(Solution_ bestSolution, BooleanSupplier isEveryProblemChangeProcessed) {\n    synchronized (this) {\n        versionedBestSolution = new VersionedBestSolution<>(bestSolution, currentVersion);\n        currentVersion = currentVersion.add(BigInteger.ONE);\n    }\n}", "test": "@Test\nvoid setBestSolution() {\n    BestSolutionHolder<TestdataSolution> bestSolutionHolder = new BestSolutionHolder<>();\n    assertThat(bestSolutionHolder.take()).isNull();\n\n    TestdataSolution solution1 = TestdataSolution.generateSolution();\n    TestdataSolution solution2 = TestdataSolution.generateSolution();\n\n    bestSolutionHolder.set(solution1, () -> true);\n    assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution1);\n    assertThat(bestSolutionHolder.take()).isNull();\n\n    bestSolutionHolder.set(solution2, () -> true);\n    assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution2);\n}"}, {"turn_num": 2, "instruction": "Modify the method to accept a second parameter: a `BooleanSupplier` named `isEveryProblemChangeProcessed`. Only update the best solution if this supplier returns true; otherwise, do nothing.", "solution": "void set(Solution_ bestSolution, BooleanSupplier isEveryProblemChangeProcessed) {\n    if (isEveryProblemChangeProcessed.getAsBoolean()) {\n        synchronized (this) {\n            versionedBestSolution = new VersionedBestSolution<>(bestSolution, currentVersion);\n            currentVersion = currentVersion.add(BigInteger.ONE);\n        }\n    }\n}", "test": "@Test\nvoid setBestSolution() {\n    BestSolutionHolder<TestdataSolution> bestSolutionHolder = new BestSolutionHolder<>();\n    assertThat(bestSolutionHolder.take()).isNull();\n\n    TestdataSolution solution1 = TestdataSolution.generateSolution();\n    TestdataSolution solution2 = TestdataSolution.generateSolution();\n\n    bestSolutionHolder.set(solution1, () -> true);\n    assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution1);\n    assertThat(bestSolutionHolder.take()).isNull();\n\n    bestSolutionHolder.set(solution1, () -> true);\n    bestSolutionHolder.set(solution2, () -> false);\n    assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution1);\n\n    bestSolutionHolder.set(solution1, () -> true);\n    bestSolutionHolder.set(solution2, () -> true);\n    assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution2);\n}"}, {"turn_num": 3, "instruction": "Store the new solution in a field called `versionedBestSolution` as a `VersionedBestSolution<Solution_>` object that also holds a version number from a `currentVersion` field of type `BigInteger`. After storing the new solution with its version, increment the `currentVersion` by one using `BigInteger.ONE`. Ensure thread safety by synchronizing the entire update block on the current object to prevent race conditions during version and solution updates.", "solution": "void set(Solution_ bestSolution, BooleanSupplier isEveryProblemChangeProcessed) {\n    if (isEveryProblemChangeProcessed.getAsBoolean()) {\n        synchronized (this) {\n            versionedBestSolution = new VersionedBestSolution<>(bestSolution, currentVersion);\n            currentVersion = currentVersion.add(BigInteger.ONE);\n        }\n    }\n}", "test": "@Test\nvoid setBestSolution() {\n    BestSolutionHolder<TestdataSolution> bestSolutionHolder = new BestSolutionHolder<>();\n    assertThat(bestSolutionHolder.take()).isNull();\n\n    TestdataSolution solution1 = TestdataSolution.generateSolution();\n    TestdataSolution solution2 = TestdataSolution.generateSolution();\n\n    bestSolutionHolder.set(solution1, () -> true);\n    assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution1);\n    assertThat(bestSolutionHolder.take()).isNull();\n\n    bestSolutionHolder.set(solution1, () -> true);\n    bestSolutionHolder.set(solution2, () -> false);\n    assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution1);\n\n    bestSolutionHolder.set(solution1, () -> true);\n    bestSolutionHolder.set(solution2, () -> true);\n    assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution2);\n}"}], "metadata": {"source": "mrgbench", "func": "    void set(Solution_ bestSolution, BooleanSupplier isEveryProblemChangeProcessed) {\n        // The new best solution can be accepted only if there are no pending problem changes\n        // nor any additional changes may come during this operation.\n        // Otherwise, a race condition might occur\n        // that leads to associating problem changes with a solution that was created later,\n        // but does not contain them yet.\n        // As a result, CompletableFutures representing these changes would be completed too early.\n        if (isEveryProblemChangeProcessed.getAsBoolean()) {\n            synchronized (this) {\n                versionedBestSolution = new VersionedBestSolution<>(bestSolution, currentVersion);\n                currentVersion = currentVersion.add(BigInteger.ONE);\n            }\n        }\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/BestSolutionHolder.java", "func_name": "set", "context": "package ai.timefold.solver.core.impl.solver;\n\nimport java.math.BigInteger;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.BooleanSupplier;\nimport java.util.function.UnaryOperator;\n\nimport ai.timefold.solver.core.api.solver.Solver;\nimport ai.timefold.solver.core.api.solver.change.ProblemChange;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.NullMarked;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * The goal of this class is to register problem changes and best solutions in a thread-safe way.\n * Problem changes are {@link #addProblemChange(Solver, List) put in a queue}\n * and later associated with the best solution which contains them.\n * The best solution is associated with a version number\n * that is incremented each time a {@link #set new best solution is set}.\n * The best solution is {@link #take() taken} together with all problem changes\n * that were registered before the best solution was set.\n * \n * <p>\n * This class needs to be thread-safe.\n * \n * @param <Solution_>\n */\n@NullMarked\nfinal class BestSolutionHolder<Solution_> {\n\n    private final AtomicReference<BigInteger> lastProcessedVersion = new AtomicReference<>(BigInteger.valueOf(-1));\n\n    // These references are non-final and being accessed from multiple threads, \n    // therefore they need to be volatile and all access synchronized.\n    // Both the map and the best solution are based on the current version,\n    // and therefore access to both needs to be guarded by the same lock.\n    // The version is BigInteger to avoid long overflow.\n    // The solver can run potentially forever, so long overflow is a (remote) possibility.\n    private volatile SortedMap<BigInteger, List<CompletableFuture<Void>>> problemChangesPerVersionMap =\n            createNewProblemChangesMap();\n    private volatile @Nullable VersionedBestSolution<Solution_> versionedBestSolution = null;\n    private volatile BigInteger currentVersion = BigInteger.ZERO;\n\n    private static SortedMap<BigInteger, List<CompletableFuture<Void>>> createNewProblemChangesMap() {\n        return createNewProblemChangesMap(Collections.emptySortedMap());\n    }\n\n    private static SortedMap<BigInteger, List<CompletableFuture<Void>>>\n            createNewProblemChangesMap(SortedMap<BigInteger, List<CompletableFuture<Void>>> map) {\n        return new TreeMap<>(map);\n    }\n\n    synchronized boolean isEmpty() {\n        return this.versionedBestSolution == null;\n    }\n\n    /**\n     * @return the last best solution together with problem changes the solution contains.\n     *         If there is no new best solution, returns null.\n     */\n    @Nullable\n    BestSolutionContainingProblemChanges<Solution_> take() {\n        var latestVersionedBestSolution = resetVersionedBestSolution();\n        if (latestVersionedBestSolution == null) {\n            return null;\n        }\n\n        var bestSolutionVersion = latestVersionedBestSolution.version();\n        var latestProcessedVersion = this.lastProcessedVersion.getAndUpdate(bestSolutionVersion::max);\n        if (latestProcessedVersion.compareTo(bestSolutionVersion) > 0) {\n            // Corner case: The best solution has already been taken,\n            // because a later take() was scheduled to run before an earlier take().\n            // This causes the later take() to return the latest best solution and all the problem changes,\n            // and the earlier best solution to be skipped entirely.\n            return null;\n        }\n        // The map is replaced by a map containing only the problem changes that are not contained in the best solution.\n        // This is fully synchronized, so no other thread can access the old map anymore.\n        // The old map can then be processed by the current thread without synchronization.\n        // The copying of maps is possibly expensive, but due to the nature of problem changes,\n        // we do not expect the map to ever get too big.\n        // It is not practical to submit a problem change every second, as that gives the solver no time to react.\n        // This limits the size of the map on input.\n        // The solver also finds new best solutions, which regularly trims the size of the map as well.\n        var boundaryVersion = bestSolutionVersion.add(BigInteger.ONE);\n        var oldProblemChangesPerVersion =\n                replaceMapSynchronized(map -> createNewProblemChangesMap(map.tailMap(boundaryVersion)));\n        // At this point, the old map is not accessible to any other thread.\n        // We also do not need to clear it, because this being the only reference, \n        // garbage collector will do it for us.\n        var containedProblemChanges = oldProblemChangesPerVersion.headMap(boundaryVersion)\n                .values()\n                .stream()\n                .flatMap(Collection::stream)\n                .toList();\n        return new BestSolutionContainingProblemChanges<>(latestVersionedBestSolution.bestSolution(), containedProblemChanges);\n    }\n\n    private synchronized @Nullable VersionedBestSolution<Solution_> resetVersionedBestSolution() {\n        var oldVersionedBestSolution = this.versionedBestSolution;\n        this.versionedBestSolution = null;\n        return oldVersionedBestSolution;\n    }\n\n    private synchronized SortedMap<BigInteger, List<CompletableFuture<Void>>> replaceMapSynchronized(\n            UnaryOperator<SortedMap<BigInteger, List<CompletableFuture<Void>>>> replaceFunction) {\n        var oldMap = problemChangesPerVersionMap;\n        problemChangesPerVersionMap = replaceFunction.apply(oldMap);\n        return oldMap;\n    }", "func_start": 127, "func_end": 140, "body_len": 13, "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/BestSolutionHolderTest.java", "test_start": 35, "test_end": 54, "test_code": "    @Test\n    void setBestSolution() {\n        BestSolutionHolder<TestdataSolution> bestSolutionHolder = new BestSolutionHolder<>();\n        assertThat(bestSolutionHolder.take()).isNull();\n\n        TestdataSolution solution1 = TestdataSolution.generateSolution();\n        TestdataSolution solution2 = TestdataSolution.generateSolution();\n\n        bestSolutionHolder.set(solution1, () -> true);\n        assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution1);\n        assertThat(bestSolutionHolder.take()).isNull();\n\n        bestSolutionHolder.set(solution1, () -> true);\n        bestSolutionHolder.set(solution2, () -> false);\n        assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution1);\n\n        bestSolutionHolder.set(solution1, () -> true);\n        bestSolutionHolder.set(solution2, () -> true);\n        assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution2);\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.BestSolutionHolderTest#setBestSolution\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1057}
{"hash_id": "77f9a38b126a73038c8f78f0", "total_turn": 2, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `nextLong(Random random, long n)` that returns a pseudorandom long value between 0 (inclusive) and n (exclusive). Assume the input Random instance is non-null and n is positive. For now, only handle the case where n is less than Integer.MAX_VALUE by delegating to nextInt. You should write self-contained code starting with: ```java\npublic static long nextLong(Random random, long n) {    \n}```", "solution": "public static long nextLong(Random random, long n) {\n    if (n < Integer.MAX_VALUE) {\n        return random.nextInt((int) n);\n    }\n    // For now, we only handle cases where n < Integer.MAX_VALUE\n    throw new UnsupportedOperationException(\"Not implemented for n >= Integer.MAX_VALUE\");\n}", "test": "@Test\nvoid testNextLong() {\n    Random random = new Random(37);\n    // Test case where n < Integer.MAX_VALUE\n    long result = RandomUtils.nextLong(random, 1000L);\n    assertTrue(result >= 0 && result < 1000L);\n    \n    // Test with n exactly at the boundary\n    long result2 = RandomUtils.nextLong(random, Integer.MAX_VALUE - 1L);\n    assertTrue(result2 >= 0 && result2 < Integer.MAX_VALUE - 1L);\n}"}, {"turn_num": 2, "instruction": "Add input validation to throw an IllegalArgumentException if n is less than or equal to zero. Extend the function to handle values of n greater than or equal to Integer.MAX_VALUE using rejection sampling to ensure a uniform distribution, avoiding modulo bias. Use the upper 63 bits of nextLong() and apply the rejection condition to guarantee fairness.", "solution": "public static long nextLong(Random random, long n) {\n    if (n <= 0L) {\n        throw new IllegalArgumentException(\"n must be positive\");\n    }\n    if (n < Integer.MAX_VALUE) {\n        return random.nextInt((int) n);\n    }\n\n    long bits;\n    long val;\n    do {\n        bits = (random.nextLong() << 1) >>> 1;\n        val = bits % n;\n    } while (bits - val + (n - 1L) < 0L);\n    return val;\n}", "test": "@Test\nvoid testNextLong() {\n    Random random = new Random(37);\n    \n    // Test input validation\n    assertThrows(IllegalArgumentException.class, () -> RandomUtils.nextLong(random, 0L));\n    assertThrows(IllegalArgumentException.class, () -> RandomUtils.nextLong(random, -1L));\n    \n    // Test rejection sampling for large n\n    long result = RandomUtils.nextLong(random, 10L + Integer.MAX_VALUE);\n    assertTrue(result >= 0 && result < 10L + Integer.MAX_VALUE);\n}"}], "metadata": {"source": "mrgbench", "func": "    public static long nextLong(Random random, long n) {\n        // This code is based on java.util.Random#nextInt(int)'s javadoc.\n        if (n <= 0L) {\n            throw new IllegalArgumentException(\"n must be positive\");\n        }\n        if (n < Integer.MAX_VALUE) {\n            return random.nextInt((int) n);\n        }\n\n        long bits;\n        long val;\n        do {\n            bits = (random.nextLong() << 1) >>> 1;\n            val = bits % n;\n        } while (bits - val + (n - 1L) < 0L);\n        return val;\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/random/RandomUtils.java", "func_name": "nextLong", "context": "package ai.timefold.solver.core.impl.solver.random;\n\nimport java.util.Random;\n\npublic class RandomUtils {", "func_start": 15, "func_end": 31, "body_len": 16, "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/random/RandomUtilsTest.java", "test_start": 9, "test_end": 13, "test_code": "    @Test\n    void testNextLong() {\n        Random random = new Random(37);\n        RandomUtils.nextLong(random, 10L + Integer.MAX_VALUE);\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.random.RandomUtilsTest#testNextLong\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1058}
{"hash_id": "30451f707c66320050999389", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `calculateSolverTimeGradient` that takes a `SolverScope<Solution_>` object and returns a double. The method should iterate over a list of solver terminations, call `calculateSolverTimeGradient` on each termination with the solver scope, and return the minimum value among all the returned gradients. Start with an initial timeGradient of 1.0 and update it using `Math.min`. You should write self-contained code starting with: ```java\n<code>```", "solution": "public double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n    var timeGradient = 1.0;\n    for (var termination : solverTerminationList) {\n        var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n        if (nextTimeGradient >= 0.0) {\n            timeGradient = Math.min(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid withConstructionHeuristicAndLocalSearch() {\n    var solverScope = spy(new SolverScope<TestdataSolution>());\n    var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n    var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n    var clock = mock(Clock.class);\n\n    var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n            SimpleScore.of(7), clock);\n\n    doReturn(START_TIME_MILLIS).when(clock).millis();\n    doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n    doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n    termination.solvingStarted(solverScope);\n    termination.phaseStarted(phaseScope);\n    termination.stepEnded(stepScope);\n\n    // Test initial state - should return 0.0\n    doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n}"}, {"turn_num": 2, "instruction": "Modify the method to only consider non-negative timeGradient values from each termination. If a termination returns -1.0, which indicates an unsupported time gradient, skip it during the minimum calculation.", "solution": "@Override\npublic double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n    var timeGradient = 1.0;\n    for (var termination : solverTerminationList) {\n        var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n        if (nextTimeGradient >= 0.0) {\n            timeGradient = Math.min(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid withConstructionHeuristicAndLocalSearch() { // CH ignores unimproved time spent termination.\n    var solverScope = spy(new SolverScope<TestdataSolution>());\n    var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n    var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n    var clock = mock(Clock.class);\n\n    var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n            SimpleScore.of(7), clock);\n\n    doReturn(START_TIME_MILLIS).when(clock).millis();\n    doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n    doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n    termination.solvingStarted(solverScope);\n    termination.phaseStarted(phaseScope);\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 1001).when(clock).millis();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    var lsPhaseScope = spy(new LocalSearchPhaseScope<>(solverScope, 0));\n    var lsStepScope = spy(new LocalSearchStepScope<>(lsPhaseScope));\n\n    // second step - score has improved, but not beyond the threshold\n    doReturn(START_TIME_MILLIS + 1501).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1501).when(lsPhaseScope).getStartingSystemTimeMillis();\n    doReturn(START_TIME_MILLIS + 1501).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(true).when(lsStepScope).getBestScoreImproved();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(5))).when(solverScope).getBestScore();\n    termination.phaseStarted(lsPhaseScope);\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.5, withPrecision(0.0));\n\n    // third step - score has improved beyond the threshold\n    doReturn(START_TIME_MILLIS + 1502).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1502).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(10))).when(solverScope).getBestScore();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.501, withPrecision(0.0));\n\n    doReturn(START_TIME_MILLIS + 2001).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    // fourth step - no more improvements\n    doReturn(START_TIME_MILLIS + 2502).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    termination.solvingEnded(solverScope);\n}"}, {"turn_num": 3, "instruction": "Ensure that the final returned timeGradient is bounded below by 0.0 implicitly by using `Math.min` starting from 1.0, so even if all gradients were negative (and thus ignored), the result defaults to 1.0 without needing explicit clamping.", "solution": "@Override\npublic double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n    var timeGradient = 1.0;\n    for (var termination : solverTerminationList) {\n        var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n        if (nextTimeGradient >= 0.0) {\n            timeGradient = Math.min(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid withConstructionHeuristicAndLocalSearch() { // CH ignores unimproved time spent termination.\n    var solverScope = spy(new SolverScope<TestdataSolution>());\n    var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n    var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n    var clock = mock(Clock.class);\n\n    var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n            SimpleScore.of(7), clock);\n\n    doReturn(START_TIME_MILLIS).when(clock).millis();\n    doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n    doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n    termination.solvingStarted(solverScope);\n    termination.phaseStarted(phaseScope);\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 1001).when(clock).millis();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    var lsPhaseScope = spy(new LocalSearchPhaseScope<>(solverScope, 0));\n    var lsStepScope = spy(new LocalSearchStepScope<>(lsPhaseScope));\n\n    // second step - score has improved, but not beyond the threshold\n    doReturn(START_TIME_MILLIS + 1501).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1501).when(lsPhaseScope).getStartingSystemTimeMillis();\n    doReturn(START_TIME_MILLIS + 1501).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(true).when(lsStepScope).getBestScoreImproved();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(5))).when(solverScope).getBestScore();\n    termination.phaseStarted(lsPhaseScope);\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.5, withPrecision(0.0));\n\n    // third step - score has improved beyond the threshold\n    doReturn(START_TIME_MILLIS + 1502).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1502).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(10))).when(solverScope).getBestScore();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.001, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.501, withPrecision(0.0));\n\n    doReturn(START_TIME_MILLIS + 2001).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.5, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    // fourth step - no more improvements\n    doReturn(START_TIME_MILLIS + 2502).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(1.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isTrue();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    termination.solvingEnded(solverScope);\n}"}], "metadata": {"source": "mrgbench", "func": "    @Override\n    public double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n        var timeGradient = 1.0;\n        for (var termination : solverTerminationList) {\n            var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.min(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTermination.java", "func_name": "calculateSolverTimeGradient", "context": "package ai.timefold.solver.core.impl.solver.termination;\n\nimport java.util.List;\n\nimport ai.timefold.solver.core.impl.phase.scope.AbstractPhaseScope;\nimport ai.timefold.solver.core.impl.solver.scope.SolverScope;\nimport ai.timefold.solver.core.impl.solver.thread.ChildThreadType;\n\nimport org.jspecify.annotations.NullMarked;\n\n@NullMarked\nfinal class AndCompositeTermination<Solution_>\n        extends AbstractCompositeTermination<Solution_>\n        implements ChildThreadSupportingTermination<Solution_, SolverScope<Solution_>> {\n\n    public AndCompositeTermination(List<Termination<Solution_>> terminationList) {\n        super(terminationList);\n    }\n\n    @SafeVarargs\n    public AndCompositeTermination(Termination<Solution_>... terminations) {\n        super(terminations);\n    }\n\n    /**\n     * @return true if all terminations are terminated.\n     */\n    @Override\n    public boolean isSolverTerminated(SolverScope<Solution_> solverScope) {\n        for (var termination : solverTerminationList) {\n            if (!termination.isSolverTerminated(solverScope)) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * @return true if all supported terminations are terminated.\n     */\n    @Override\n    public boolean isPhaseTerminated(AbstractPhaseScope<Solution_> phaseScope) {\n        for (var termination : phaseTerminationList) {\n            if (termination.isApplicableTo(phaseScope.getClass()) && !termination.isPhaseTerminated(phaseScope)) {\n                return false;\n            }\n        }\n        return true;\n    }", "func_start": 57, "func_end": 67, "body_len": 10, "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java", "test_start": 174, "test_end": 261, "test_code": "    @Test\n    void withConstructionHeuristicAndLocalSearch() { // CH ignores unimproved time spent termination.\n        var solverScope = spy(new SolverScope<TestdataSolution>());\n        var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n        var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n        var clock = mock(Clock.class);\n\n        var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n                SimpleScore.of(7), clock);\n\n        doReturn(START_TIME_MILLIS).when(clock).millis();\n        doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n        doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n        termination.solvingStarted(solverScope);\n        termination.phaseStarted(phaseScope);\n        termination.stepEnded(stepScope);\n\n        // CH time has not yet run out\n        doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        termination.stepEnded(stepScope);\n\n        // CH time has not yet run out\n        doReturn(START_TIME_MILLIS + 1001).when(clock).millis();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        termination.phaseEnded(phaseScope);\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        var lsPhaseScope = spy(new LocalSearchPhaseScope<>(solverScope, 0));\n        var lsStepScope = spy(new LocalSearchStepScope<>(lsPhaseScope));\n\n        // second step - score has improved, but not beyond the threshold\n        doReturn(START_TIME_MILLIS + 1501).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 1501).when(lsPhaseScope).getStartingSystemTimeMillis();\n        doReturn(START_TIME_MILLIS + 1501).when(solverScope).getBestSolutionTimeMillis();\n        doReturn(true).when(lsStepScope).getBestScoreImproved();\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(5))).when(solverScope).getBestScore();\n        termination.phaseStarted(lsPhaseScope);\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.5, withPrecision(0.0));\n\n        // third step - score has improved beyond the threshold\n        doReturn(START_TIME_MILLIS + 1502).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 1502).when(solverScope).getBestSolutionTimeMillis();\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(10))).when(solverScope).getBestScore();\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.001, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.501, withPrecision(0.0));\n\n        doReturn(START_TIME_MILLIS + 2001).when(clock).millis();\n        termination.stepEnded(lsStepScope);\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.5, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n        // fourth step - no more improvements\n        doReturn(START_TIME_MILLIS + 2502).when(clock).millis();\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(1.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isTrue();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n        termination.phaseEnded(phaseScope);\n        termination.solvingEnded(solverScope);\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.termination.UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest#withConstructionHeuristicAndLocalSearch\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1059}
{"hash_id": "e5d0241661065416a4e451e1", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `calculatePhaseTimeGradient` that takes an `AbstractPhaseScope<Solution_>` as input and returns a double representing the minimum time gradient among all terminations in the `phaseTerminationList`. Initialize the minimum time gradient to 1.0, iterate through each termination, compute its time gradient using `calculatePhaseTimeGradient(phaseScope)`, and update the minimum if the computed value is smaller. Return the final minimum. You should write self-contained code starting with: ```java\n<code>```", "solution": "@Override\npublic double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n    var timeGradient = 1.0;\n    for (var termination : phaseTerminationList) {\n        var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n        timeGradient = Math.min(timeGradient, nextTimeGradient);\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid phaseTermination() {\n    ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n    when(scoreDefinition.getLevelsSize()).thenReturn(1);\n    var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n    AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n    when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n}"}, {"turn_num": 2, "instruction": "Modify the method to skip any termination that is not applicable to the given phase scope by checking `isApplicableTo(phaseScope.getClass())` before calculating its time gradient.", "solution": "@Override\npublic double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n    var timeGradient = 1.0;\n    for (var termination : phaseTerminationList) {\n        if (!termination.isApplicableTo(phaseScope.getClass())) {\n            continue;\n        }\n        var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n        if (nextTimeGradient >= 0.0) {\n            timeGradient = Math.min(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid phaseTermination() {\n    ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n    when(scoreDefinition.getLevelsSize()).thenReturn(1);\n    var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n    AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n    when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n}"}, {"turn_num": 3, "instruction": "Ensure the method only considers non-negative time gradients (values >= 0.0) when updating the minimum, as negative values (including -1.0) indicate unsupported or invalid gradients and should be ignored.", "solution": "@Override\npublic double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n    var timeGradient = 1.0;\n    for (var termination : phaseTerminationList) {\n        if (!termination.isApplicableTo(phaseScope.getClass())) {\n            continue;\n        }\n        var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n        if (nextTimeGradient >= 0.0) {\n            timeGradient = Math.min(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid phaseTermination() {\n    ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n    when(scoreDefinition.getLevelsSize()).thenReturn(1);\n    var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n    AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n    when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-900))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n}"}, {"turn_num": 4, "instruction": "Ensure the method returns 1.0 if no applicable termination produces a valid (non-negative) time gradient, preserving the initial value as the fallback result.", "solution": "@Override\npublic double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n    var timeGradient = 1.0;\n    for (var termination : phaseTerminationList) {\n        if (!termination.isApplicableTo(phaseScope.getClass())) {\n            continue;\n        }\n        var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n        if (nextTimeGradient >= 0.0) {\n            timeGradient = Math.min(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid phaseTermination() {\n    ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n    when(scoreDefinition.getLevelsSize()).thenReturn(1);\n    var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n    AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n    when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-900))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n}"}], "metadata": {"source": "mrgbench", "func": "    @Override\n    public double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n        var timeGradient = 1.0;\n        for (var termination : phaseTerminationList) {\n            if (!termination.isApplicableTo(phaseScope.getClass())) {\n                continue;\n            }\n            var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.min(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTermination.java", "func_name": "calculatePhaseTimeGradient", "context": "package ai.timefold.solver.core.impl.solver.termination;\n\nimport java.util.List;\n\nimport ai.timefold.solver.core.impl.phase.scope.AbstractPhaseScope;\nimport ai.timefold.solver.core.impl.solver.scope.SolverScope;\nimport ai.timefold.solver.core.impl.solver.thread.ChildThreadType;\n\nimport org.jspecify.annotations.NullMarked;\n\n@NullMarked\nfinal class AndCompositeTermination<Solution_>\n        extends AbstractCompositeTermination<Solution_>\n        implements ChildThreadSupportingTermination<Solution_, SolverScope<Solution_>> {\n\n    public AndCompositeTermination(List<Termination<Solution_>> terminationList) {\n        super(terminationList);\n    }\n\n    @SafeVarargs\n    public AndCompositeTermination(Termination<Solution_>... terminations) {\n        super(terminations);\n    }\n\n    /**\n     * @return true if all terminations are terminated.\n     */\n    @Override\n    public boolean isSolverTerminated(SolverScope<Solution_> solverScope) {\n        for (var termination : solverTerminationList) {\n            if (!termination.isSolverTerminated(solverScope)) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * @return true if all supported terminations are terminated.\n     */\n    @Override\n    public boolean isPhaseTerminated(AbstractPhaseScope<Solution_> phaseScope) {\n        for (var termination : phaseTerminationList) {\n            if (termination.isApplicableTo(phaseScope.getClass()) && !termination.isPhaseTerminated(phaseScope)) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * Calculates the minimum timeGradient of all Terminations.\n     * Not supported timeGradients (-1.0) are ignored.\n     *\n     * @return the minimum timeGradient of the Terminations.\n     */\n    @Override\n    public double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n        var timeGradient = 1.0;\n        for (var termination : solverTerminationList) {\n            var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.min(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }", "func_start": 75, "func_end": 88, "body_len": 13, "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java", "test_start": 62, "test_end": 94, "test_code": "    @Test\n    void phaseTermination() {\n        ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n        when(scoreDefinition.getLevelsSize()).thenReturn(1);\n        var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n        AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n        when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-900))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.termination.BestScoreTerminationTest#phaseTermination\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1060}
{"hash_id": "cb7b2e7a05a1dcf87e87211d", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `calculateSolverTimeGradient` that takes a `SolverScope<Solution_>` object and returns a double. The function should iterate over a list of solver terminations (`solverTerminationList`), call `calculateSolverTimeGradient` on each termination, and return the maximum value among them. You should write self-contained code starting with: ```java\n@Override\npublic double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n    // Your code here\n}```", "solution": "@Override\npublic double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n    var timeGradient = 0.0;\n    for (var termination : solverTerminationList) {\n        var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n        if (nextTimeGradient >= 0.0) {\n            timeGradient = Math.max(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid withConstructionHeuristicAndLocalSearch() { // CH ignores unimproved time spent termination.\n    var solverScope = spy(new SolverScope<TestdataSolution>());\n    var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n    var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n    var clock = mock(Clock.class);\n\n    var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n            SimpleScore.of(7), clock);\n\n    doReturn(START_TIME_MILLIS).when(clock).millis();\n    doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n    doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n    termination.solvingStarted(solverScope);\n    termination.phaseStarted(phaseScope);\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 1001).when(clock).millis();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    var lsPhaseScope = spy(new LocalSearchPhaseScope<>(solverScope, 0));\n    var lsStepScope = spy(new LocalSearchStepScope<>(lsPhaseScope));\n\n    // second step - score has improved, but not beyond the threshold\n    doReturn(START_TIME_MILLIS + 1501).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1501).when(lsPhaseScope).getStartingSystemTimeMillis();\n    doReturn(START_TIME_MILLIS + 1501).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(true).when(lsStepScope).getBestScoreImproved();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(5))).when(solverScope).getBestScore();\n    termination.phaseStarted(lsPhaseScope);\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.5, withPrecision(0.0));\n\n    // third step - score has improved beyond the threshold\n    doReturn(START_TIME_MILLIS + 1502).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1502).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(10))).when(solverScope).getBestScore();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.001, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.501, withPrecision(0.0));\n\n    doReturn(START_TIME_MILLIS + 2001).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.5, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    // fourth step - no more improvements\n    doReturn(START_TIME_MILLIS + 2502).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(1.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isTrue();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    termination.solvingEnded(solverScope);\n}"}, {"turn_num": 2, "instruction": "Modify the function to ignore any termination that returns a negative time gradient (specifically -1.0), treating such values as unsupported and not considering them in the maximum calculation.", "solution": "@Override\npublic double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n    var timeGradient = 0.0;\n    for (var termination : solverTerminationList) {\n        var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n        // Ignore unsupported terminations that return -1.0\n        if (nextTimeGradient != -1.0 && nextTimeGradient >= 0.0) {\n            timeGradient = Math.max(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid withConstructionHeuristicAndLocalSearch() { // CH ignores unimproved time spent termination.\n    var solverScope = spy(new SolverScope<TestdataSolution>());\n    var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n    var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n    var clock = mock(Clock.class);\n\n    var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n            SimpleScore.of(7), clock);\n\n    doReturn(START_TIME_MILLIS).when(clock).millis();\n    doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n    doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n    termination.solvingStarted(solverScope);\n    termination.phaseStarted(phaseScope);\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    // Test that -1.0 is properly ignored - during CH phase, this termination returns 0.0\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 1001).when(clock).millis();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    var lsPhaseScope = spy(new LocalSearchPhaseScope<>(solverScope, 0));\n    var lsStepScope = spy(new LocalSearchStepScope<>(lsPhaseScope));\n\n    // second step - score has improved, but not beyond the threshold\n    doReturn(START_TIME_MILLIS + 1501).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1501).when(lsPhaseScope).getStartingSystemTimeMillis();\n    doReturn(START_TIME_MILLIS + 1501).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(true).when(lsStepScope).getBestScoreImproved();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(5))).when(solverScope).getBestScore();\n    termination.phaseStarted(lsPhaseScope);\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.5, withPrecision(0.0));\n\n    // third step - score has improved beyond the threshold\n    doReturn(START_TIME_MILLIS + 1502).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1502).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(10))).when(solverScope).getBestScore();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.001, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.501, withPrecision(0.0));\n\n    doReturn(START_TIME_MILLIS + 2001).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.5, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    // fourth step - no more improvements\n    doReturn(START_TIME_MILLIS + 2502).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(1.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isTrue();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    termination.solvingEnded(solverScope);\n}"}, {"turn_num": 3, "instruction": "Use `Math.max` to update the maximum time gradient and initialize the result to 0.0 to ensure correct behavior when no positive gradients are present. Ensure the function returns 0.0 if all terminations return negative gradients, indicating no valid time gradient was found.", "solution": "@Override\npublic double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n    var timeGradient = 0.0;\n    for (var termination : solverTerminationList) {\n        var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n        if (nextTimeGradient >= 0.0) {\n            timeGradient = Math.max(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid withConstructionHeuristicAndLocalSearch() { // CH ignores unimproved time spent termination.\n    var solverScope = spy(new SolverScope<TestdataSolution>());\n    var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n    var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n    var clock = mock(Clock.class);\n\n    var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n            SimpleScore.of(7), clock);\n\n    doReturn(START_TIME_MILLIS).when(clock).millis();\n    doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n    doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n    termination.solvingStarted(solverScope);\n    termination.phaseStarted(phaseScope);\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.stepEnded(stepScope);\n\n    // CH time has not yet run out\n    doReturn(START_TIME_MILLIS + 1001).when(clock).millis();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n    var lsPhaseScope = spy(new LocalSearchPhaseScope<>(solverScope, 0));\n    var lsStepScope = spy(new LocalSearchStepScope<>(lsPhaseScope));\n\n    // second step - score has improved, but not beyond the threshold\n    doReturn(START_TIME_MILLIS + 1501).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1501).when(lsPhaseScope).getStartingSystemTimeMillis();\n    doReturn(START_TIME_MILLIS + 1501).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(true).when(lsStepScope).getBestScoreImproved();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(5))).when(solverScope).getBestScore();\n    termination.phaseStarted(lsPhaseScope);\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.5, withPrecision(0.0));\n\n    // third step - score has improved beyond the threshold\n    doReturn(START_TIME_MILLIS + 1502).when(clock).millis();\n    doReturn(START_TIME_MILLIS + 1502).when(solverScope).getBestSolutionTimeMillis();\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(10))).when(solverScope).getBestScore();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.001, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.501, withPrecision(0.0));\n\n    doReturn(START_TIME_MILLIS + 2001).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.5, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    // fourth step - no more improvements\n    doReturn(START_TIME_MILLIS + 2502).when(clock).millis();\n    termination.stepEnded(lsStepScope);\n\n    assertThat(termination.isPhaseTerminated(lsPhaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(1.0, withPrecision(0.0));\n    assertThat(termination.isSolverTerminated(solverScope)).isTrue();\n    assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n    termination.phaseEnded(phaseScope);\n    termination.solvingEnded(solverScope);\n}"}], "metadata": {"source": "mrgbench", "func": "    @Override\n    public double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n        var timeGradient = 0.0;\n        for (var termination : solverTerminationList) {\n            var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.max(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTermination.java", "func_name": "calculateSolverTimeGradient", "context": "package ai.timefold.solver.core.impl.solver.termination;\n\nimport java.util.List;\n\nimport ai.timefold.solver.core.impl.phase.scope.AbstractPhaseScope;\nimport ai.timefold.solver.core.impl.solver.scope.SolverScope;\nimport ai.timefold.solver.core.impl.solver.thread.ChildThreadType;\n\nimport org.jspecify.annotations.NullMarked;\n\n@NullMarked\nfinal class OrCompositeTermination<Solution_>\n        extends AbstractCompositeTermination<Solution_>\n        implements ChildThreadSupportingTermination<Solution_, SolverScope<Solution_>> {\n\n    public OrCompositeTermination(List<Termination<Solution_>> terminationList) {\n        super(terminationList);\n    }\n\n    @SafeVarargs\n    public OrCompositeTermination(Termination<Solution_>... terminations) {\n        super(terminations);\n    }\n\n    /**\n     * @param solverScope never null\n     * @return true if any one of the terminations is terminated.\n     */\n    @Override\n    public boolean isSolverTerminated(SolverScope<Solution_> solverScope) {\n        for (var termination : solverTerminationList) {\n            if (termination.isSolverTerminated(solverScope)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    /**\n     * @return true if any one of the supported terminations is terminated.\n     */\n    @Override\n    public boolean isPhaseTerminated(AbstractPhaseScope<Solution_> phaseScope) {\n        for (var termination : phaseTerminationList) {\n            if (!termination.isApplicableTo(phaseScope.getClass())) {\n                continue;\n            }\n            if (termination.isPhaseTerminated(phaseScope)) {\n                return true;\n            }\n        }\n        return false;\n    }", "func_start": 61, "func_end": 71, "body_len": 10, "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java", "test_start": 174, "test_end": 261, "test_code": "    @Test\n    void withConstructionHeuristicAndLocalSearch() { // CH ignores unimproved time spent termination.\n        var solverScope = spy(new SolverScope<TestdataSolution>());\n        var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n        var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n        var clock = mock(Clock.class);\n\n        var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n                SimpleScore.of(7), clock);\n\n        doReturn(START_TIME_MILLIS).when(clock).millis();\n        doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n        doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n        termination.solvingStarted(solverScope);\n        termination.phaseStarted(phaseScope);\n        termination.stepEnded(stepScope);\n\n        // CH time has not yet run out\n        doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        termination.stepEnded(stepScope);\n\n        // CH time has not yet run out\n        doReturn(START_TIME_MILLIS + 1001).when(clock).millis();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        termination.phaseEnded(phaseScope);\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        var lsPhaseScope = spy(new LocalSearchPhaseScope<>(solverScope, 0));\n        var lsStepScope = spy(new LocalSearchStepScope<>(lsPhaseScope));\n\n        // second step - score has improved, but not beyond the threshold\n        doReturn(START_TIME_MILLIS + 1501).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 1501).when(lsPhaseScope).getStartingSystemTimeMillis();\n        doReturn(START_TIME_MILLIS + 1501).when(solverScope).getBestSolutionTimeMillis();\n        doReturn(true).when(lsStepScope).getBestScoreImproved();\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(5))).when(solverScope).getBestScore();\n        termination.phaseStarted(lsPhaseScope);\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.5, withPrecision(0.0));\n\n        // third step - score has improved beyond the threshold\n        doReturn(START_TIME_MILLIS + 1502).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 1502).when(solverScope).getBestSolutionTimeMillis();\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(10))).when(solverScope).getBestScore();\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.001, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.501, withPrecision(0.0));\n\n        doReturn(START_TIME_MILLIS + 2001).when(clock).millis();\n        termination.stepEnded(lsStepScope);\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.5, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n        // fourth step - no more improvements\n        doReturn(START_TIME_MILLIS + 2502).when(clock).millis();\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(1.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isTrue();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n        termination.phaseEnded(phaseScope);\n        termination.solvingEnded(solverScope);\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.termination.UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest#withConstructionHeuristicAndLocalSearch\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1061}
{"hash_id": "06cff9f66a61a3ff7ba32db9", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `calculatePhaseTimeGradient(phaseScope)` that takes a phase scope object and returns the maximum time gradient from the `phaseTerminationList`. Assume all terminations are applicable and all time gradients are valid. You should write self-contained code starting with: ```java\n<code>```", "solution": "public double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n    var timeGradient = 0.0;\n    for (var termination : phaseTerminationList) {\n        var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n        timeGradient = Math.max(timeGradient, nextTimeGradient);\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid phaseTermination() {\n    ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n    when(scoreDefinition.getLevelsSize()).thenReturn(1);\n    var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n    AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n    when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n}"}, {"turn_num": 2, "instruction": "Modify the function to skip any termination that is not applicable to the given phase scope class by checking `termination.isApplicableTo(phaseScope.getClass())`.", "solution": "public double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n    var timeGradient = 0.0;\n    for (var termination : phaseTerminationList) {\n        if (!termination.isApplicableTo(phaseScope.getClass())) {\n            continue;\n        }\n        var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n        timeGradient = Math.max(timeGradient, nextTimeGradient);\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid phaseTermination() {\n    ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n    when(scoreDefinition.getLevelsSize()).thenReturn(1);\n    var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n    AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n    when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n    // Test that termination works when applicable\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n\n    // Test with different score to verify gradient calculation\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n    assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n}"}, {"turn_num": 3, "instruction": "Ignore any time gradient value that is negative, as unsupported gradients are indicated by -1.0; only consider non-negative time gradients. If no valid (non-negative) time gradients are found from applicable terminations, return 0.0.", "solution": "public double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n    var timeGradient = 0.0;\n    for (var termination : phaseTerminationList) {\n        if (!termination.isApplicableTo(phaseScope.getClass())) {\n            continue;\n        }\n        var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n        if (nextTimeGradient >= 0.0) {\n            timeGradient = Math.max(timeGradient, nextTimeGradient);\n        }\n    }\n    return timeGradient;\n}", "test": "@Test\nvoid phaseTermination() {\n    ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n    when(scoreDefinition.getLevelsSize()).thenReturn(1);\n    var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n    AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n    when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n    // Test negative gradient handling - when score is worse than starting, gradient should be 0.0\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1200))).when(phaseScope).getBestScore();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n    \n    // Test normal positive gradient behavior\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n    \n    // Test when target is reached\n    doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n    assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n}"}], "metadata": {"source": "mrgbench", "func": "    @Override\n    public double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n        var timeGradient = 0.0;\n        for (var termination : phaseTerminationList) {\n            if (!termination.isApplicableTo(phaseScope.getClass())) {\n                continue;\n            }\n            var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.max(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTermination.java", "func_name": "calculatePhaseTimeGradient", "context": "package ai.timefold.solver.core.impl.solver.termination;\n\nimport java.util.List;\n\nimport ai.timefold.solver.core.impl.phase.scope.AbstractPhaseScope;\nimport ai.timefold.solver.core.impl.solver.scope.SolverScope;\nimport ai.timefold.solver.core.impl.solver.thread.ChildThreadType;\n\nimport org.jspecify.annotations.NullMarked;\n\n@NullMarked\nfinal class OrCompositeTermination<Solution_>\n        extends AbstractCompositeTermination<Solution_>\n        implements ChildThreadSupportingTermination<Solution_, SolverScope<Solution_>> {\n\n    public OrCompositeTermination(List<Termination<Solution_>> terminationList) {\n        super(terminationList);\n    }\n\n    @SafeVarargs\n    public OrCompositeTermination(Termination<Solution_>... terminations) {\n        super(terminations);\n    }\n\n    /**\n     * @param solverScope never null\n     * @return true if any one of the terminations is terminated.\n     */\n    @Override\n    public boolean isSolverTerminated(SolverScope<Solution_> solverScope) {\n        for (var termination : solverTerminationList) {\n            if (termination.isSolverTerminated(solverScope)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    /**\n     * @return true if any one of the supported terminations is terminated.\n     */\n    @Override\n    public boolean isPhaseTerminated(AbstractPhaseScope<Solution_> phaseScope) {\n        for (var termination : phaseTerminationList) {\n            if (!termination.isApplicableTo(phaseScope.getClass())) {\n                continue;\n            }\n            if (termination.isPhaseTerminated(phaseScope)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    /**\n     * Calculates the maximum timeGradient of all Terminations.\n     * Not supported timeGradients (-1.0) are ignored.\n     *\n     * @return the maximum timeGradient of the terminations.\n     */\n    @Override\n    public double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n        var timeGradient = 0.0;\n        for (var termination : solverTerminationList) {\n            var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.max(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }", "func_start": 79, "func_end": 92, "body_len": 13, "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java", "test_start": 62, "test_end": 94, "test_code": "    @Test\n    void phaseTermination() {\n        ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n        when(scoreDefinition.getLevelsSize()).thenReturn(1);\n        var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n        AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n        when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-900))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.termination.BestScoreTerminationTest#phaseTermination\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1062}
{"hash_id": "73b5560db41ec359450f1ff1", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `getCyclesForPermutation` that takes a `KOptDescriptor` object and computes the number of disjoint cycles in a permutation defined by its `removedEdgeIndexToTourOrder` array. Assume the permutation maps each index `i` to `removedEdgeIndexToTourOrder[i]`, and use a simple loop with a visited array to follow cycles. Return the total cycle count and an array mapping each index to its cycle number. You should write self-contained code starting with: ```java\nstatic KOptCycle getCyclesForPermutation(KOptDescriptor<?> kOptDescriptor) {```", "solution": "static KOptCycle getCyclesForPermutation(KOptDescriptor<?> kOptDescriptor) {\n    var removedEdgeIndexToTourOrder = kOptDescriptor.removedEdgeIndexToTourOrder();\n    var visited = new boolean[removedEdgeIndexToTourOrder.length];\n    var indexToCycle = new int[removedEdgeIndexToTourOrder.length];\n    var cycleCount = 0;\n    \n    for (int i = 0; i < removedEdgeIndexToTourOrder.length; i++) {\n        if (!visited[i]) {\n            int current = i;\n            while (!visited[current]) {\n                visited[current] = true;\n                indexToCycle[current] = cycleCount;\n                current = removedEdgeIndexToTourOrder[current];\n            }\n            cycleCount++;\n        }\n    }\n    \n    return new KOptCycle(cycleCount, indexToCycle);\n}", "test": "@Test\nvoid testGetCyclesForPermutationThreeCycle() {\n    List<TestdataListValue> originalTour = List.of(v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12);\n    List<TestdataListValue> removedEdges = List.of(v1, v2,\n            v3, v4,\n            v5, v6,\n            v7, v8,\n            v9, v10,\n            v11, v12);\n    List<TestdataListValue> addedEdges = List.of(v1, v4,\n            v5, v12,\n            v3, v6,\n            v7, v2,\n            v8, v10,\n            v11, v9);\n\n    KOptDescriptor<TestdataListValue> kOptDescriptor = fromRemovedAndAddedEdges(originalTour,\n            removedEdges,\n            addedEdges);\n    KOptCycle cycleInfo = KOptUtils.getCyclesForPermutation(kOptDescriptor);\n    \n    // Test that we can detect cycles in the permutation\n    assertThat(cycleInfo.cycleCount()).isGreaterThan(0);\n    assertThat(cycleInfo.indexToCycleIdentifier()).hasSize(13);\n}"}, {"turn_num": 2, "instruction": "Modify the function to use the `addedEdgeToOtherEndpoint` and `inverseRemovedEdgeIndexToTourOrder` mappings to correctly define the permutation traversal. Instead of directly using `removedEdgeIndexToTourOrder` as the permutation, alternate between using the tour order and added edge connections to find the next endpoint. Use a BitSet to track unvisited indices for efficiency.", "solution": "static KOptCycle getCyclesForPermutation(KOptDescriptor<?> kOptDescriptor) {\n    var cycleCount = 0;\n    var removedEdgeIndexToTourOrder = kOptDescriptor.removedEdgeIndexToTourOrder();\n    var addedEdgeToOtherEndpoint = kOptDescriptor.addedEdgeToOtherEndpoint();\n    var inverseRemovedEdgeIndexToTourOrder = kOptDescriptor.inverseRemovedEdgeIndexToTourOrder();\n\n    var indexToCycle = new int[removedEdgeIndexToTourOrder.length];\n    var remaining = new BitSet(removedEdgeIndexToTourOrder.length);\n    remaining.set(1, removedEdgeIndexToTourOrder.length, true);\n\n    while (!remaining.isEmpty()) {\n        var currentEndpoint = remaining.nextSetBit(0);\n        while (remaining.get(currentEndpoint)) {\n            indexToCycle[currentEndpoint] = cycleCount;\n            remaining.clear(currentEndpoint);\n\n            // Go to the endpoint connected to this one by an added edge\n            var currentEndpointTourIndex = removedEdgeIndexToTourOrder[currentEndpoint];\n            var nextEndpointTourIndex = addedEdgeToOtherEndpoint[currentEndpointTourIndex];\n            currentEndpoint = inverseRemovedEdgeIndexToTourOrder[nextEndpointTourIndex];\n\n            indexToCycle[currentEndpoint] = cycleCount;\n            remaining.clear(currentEndpoint);\n\n            // Go to the endpoint after the added edge\n            currentEndpoint = currentEndpoint ^ 1;\n        }\n        cycleCount++;\n    }\n    return new KOptCycle(cycleCount, indexToCycle);\n}", "test": "@Test\nvoid testGetCyclesForPermutationThreeCycle() {\n    List<TestdataListValue> originalTour = List.of(v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12);\n    List<TestdataListValue> removedEdges = List.of(v1, v2,\n            v3, v4,\n            v5, v6,\n            v7, v8,\n            v9, v10,\n            v11, v12);\n    List<TestdataListValue> addedEdges = List.of(v1, v4,\n            v5, v12,\n            v3, v6,\n            v7, v2,\n            v8, v10,\n            v11, v9);\n\n    KOptDescriptor<TestdataListValue> kOptDescriptor = fromRemovedAndAddedEdges(originalTour,\n            removedEdges,\n            addedEdges);\n    KOptCycle cycleInfo = KOptUtils.getCyclesForPermutation(kOptDescriptor);\n    assertThat(cycleInfo.cycleCount()).isEqualTo(3);\n\n    // Verify BitSet usage starting from index 1 and alternating traversal produces correct cycle assignments\n    assertThat(cycleInfo.indexToCycleIdentifier()).containsExactly(0, 0, 1, 1, 0, 0, 1, 1, 2, 2, 2, 2, 0);\n}"}, {"turn_num": 3, "instruction": "Ensure the traversal alternates between following an added edge and moving to the paired endpoint via XOR with 1. After visiting an endpoint and its connected endpoint through the added edge, move to the next endpoint in the pair by applying `currentEndpoint = currentEndpoint ^ 1` to correctly simulate the k-opt move structure.", "solution": "static KOptCycle getCyclesForPermutation(KOptDescriptor<?> kOptDescriptor) {\n    var cycleCount = 0;\n    var removedEdgeIndexToTourOrder = kOptDescriptor.removedEdgeIndexToTourOrder();\n    var addedEdgeToOtherEndpoint = kOptDescriptor.addedEdgeToOtherEndpoint();\n    var inverseRemovedEdgeIndexToTourOrder = kOptDescriptor.inverseRemovedEdgeIndexToTourOrder();\n\n    var indexToCycle = new int[removedEdgeIndexToTourOrder.length];\n    var remaining = new BitSet(removedEdgeIndexToTourOrder.length);\n    remaining.set(1, removedEdgeIndexToTourOrder.length, true);\n\n    while (!remaining.isEmpty()) {\n        var currentEndpoint = remaining.nextSetBit(0);\n        while (remaining.get(currentEndpoint)) {\n            indexToCycle[currentEndpoint] = cycleCount;\n            remaining.clear(currentEndpoint);\n\n            // Go to the endpoint connected to this one by an added edge\n            var currentEndpointTourIndex = removedEdgeIndexToTourOrder[currentEndpoint];\n            var nextEndpointTourIndex = addedEdgeToOtherEndpoint[currentEndpointTourIndex];\n            currentEndpoint = inverseRemovedEdgeIndexToTourOrder[nextEndpointTourIndex];\n\n            indexToCycle[currentEndpoint] = cycleCount;\n            remaining.clear(currentEndpoint);\n\n            // Move to the paired endpoint via XOR with 1\n            currentEndpoint = currentEndpoint ^ 1;\n        }\n        cycleCount++;\n    }\n    return new KOptCycle(cycleCount, indexToCycle);\n}", "test": "@Test\nvoid testGetCyclesForPermutationThreeCycle() {\n    List<TestdataListValue> originalTour = List.of(v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12);\n    List<TestdataListValue> removedEdges = List.of(v1, v2,\n            v3, v4,\n            v5, v6,\n            v7, v8,\n            v9, v10,\n            v11, v12);\n    List<TestdataListValue> addedEdges = List.of(v1, v4,\n            v5, v12,\n            v3, v6,\n            v7, v2,\n            v8, v10,\n            v11, v9);\n\n    KOptDescriptor<TestdataListValue> kOptDescriptor = fromRemovedAndAddedEdges(originalTour,\n            removedEdges,\n            addedEdges);\n    KOptCycle cycleInfo = KOptUtils.getCyclesForPermutation(kOptDescriptor);\n    assertThat(cycleInfo.cycleCount()).isEqualTo(3);\n\n    // Cycles with XOR alternation:\n    // v1 -> v4 -> v5 -> v12\n    // v2 -> v7 -> v6 -> v3\n    // v8 -> v10 -> v11 -> v9\n    assertThat(cycleInfo.indexToCycleIdentifier()).containsExactly(0, 0, 1, 1, 0, 0, 1, 1, 2, 2, 2, 2, 0);\n}"}, {"turn_num": 4, "instruction": "Guarantee that all indices are processed by correctly initializing the BitSet to cover the full range from 1 to the length of `removedEdgeIndexToTourOrder`, and ensure that each step of the cycle traversal updates the current endpoint using the correct sequence: current endpoint → tour order → added edge → inverse tour order → next endpoint.", "solution": "static KOptCycle getCyclesForPermutation(KOptDescriptor<?> kOptDescriptor) {\n    var cycleCount = 0;\n    var removedEdgeIndexToTourOrder = kOptDescriptor.removedEdgeIndexToTourOrder();\n    var addedEdgeToOtherEndpoint = kOptDescriptor.addedEdgeToOtherEndpoint();\n    var inverseRemovedEdgeIndexToTourOrder = kOptDescriptor.inverseRemovedEdgeIndexToTourOrder();\n\n    var indexToCycle = new int[removedEdgeIndexToTourOrder.length];\n    var remaining = new BitSet(removedEdgeIndexToTourOrder.length);\n    remaining.set(0, removedEdgeIndexToTourOrder.length, true);\n\n    while (!remaining.isEmpty()) {\n        var currentEndpoint = remaining.nextSetBit(0);\n        while (remaining.get(currentEndpoint)) {\n            indexToCycle[currentEndpoint] = cycleCount;\n            remaining.clear(currentEndpoint);\n\n            // Follow the sequence: current endpoint → tour order → added edge → inverse tour order → next endpoint\n            var currentEndpointTourIndex = removedEdgeIndexToTourOrder[currentEndpoint];\n            var nextEndpointTourIndex = addedEdgeToOtherEndpoint[currentEndpointTourIndex];\n            currentEndpoint = inverseRemovedEdgeIndexToTourOrder[nextEndpointTourIndex];\n\n            indexToCycle[currentEndpoint] = cycleCount;\n            remaining.clear(currentEndpoint);\n\n            // Move to the paired endpoint to complete the cycle step\n            currentEndpoint = currentEndpoint ^ 1;\n        }\n        cycleCount++;\n    }\n    return new KOptCycle(cycleCount, indexToCycle);\n}", "test": "@Test\nvoid testGetCyclesForPermutationThreeCycle() {\n    List<TestdataListValue> originalTour = List.of(v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12);\n    List<TestdataListValue> removedEdges = List.of(v1, v2,\n            v3, v4,\n            v5, v6,\n            v7, v8,\n            v9, v10,\n            v11, v12);\n    List<TestdataListValue> addedEdges = List.of(v1, v4,\n            v5, v12,\n            v3, v6,\n            v7, v2,\n            v8, v10,\n            v11, v9);\n\n    KOptDescriptor<TestdataListValue> kOptDescriptor = fromRemovedAndAddedEdges(originalTour,\n            removedEdges,\n            addedEdges);\n    KOptCycle cycleInfo = KOptUtils.getCyclesForPermutation(kOptDescriptor);\n    assertThat(cycleInfo.cycleCount()).isEqualTo(3);\n\n    // Verify that the BitSet initialization covers the full range from 0 to length\n    // and the traversal sequence correctly processes all indices using the complete cycle path\n    assertThat(cycleInfo.indexToCycleIdentifier()).containsExactly(0, 0, 1, 1, 0, 0, 1, 1, 2, 2, 2, 2, 0);\n}"}], "metadata": {"source": "mrgbench", "func": "    static KOptCycle getCyclesForPermutation(KOptDescriptor<?> kOptDescriptor) {\n        var cycleCount = 0;\n        var removedEdgeIndexToTourOrder = kOptDescriptor.removedEdgeIndexToTourOrder();\n        var addedEdgeToOtherEndpoint = kOptDescriptor.addedEdgeToOtherEndpoint();\n        var inverseRemovedEdgeIndexToTourOrder = kOptDescriptor.inverseRemovedEdgeIndexToTourOrder();\n\n        var indexToCycle = new int[removedEdgeIndexToTourOrder.length];\n        var remaining = new BitSet(removedEdgeIndexToTourOrder.length);\n        remaining.set(1, removedEdgeIndexToTourOrder.length, true);\n\n        while (!remaining.isEmpty()) {\n            var currentEndpoint = remaining.nextSetBit(0);\n            while (remaining.get(currentEndpoint)) {\n                indexToCycle[currentEndpoint] = cycleCount;\n                remaining.clear(currentEndpoint);\n\n                // Go to the endpoint connected to this one by an added edge\n                var currentEndpointTourIndex = removedEdgeIndexToTourOrder[currentEndpoint];\n                var nextEndpointTourIndex = addedEdgeToOtherEndpoint[currentEndpointTourIndex];\n                currentEndpoint = inverseRemovedEdgeIndexToTourOrder[nextEndpointTourIndex];\n\n                indexToCycle[currentEndpoint] = cycleCount;\n                remaining.clear(currentEndpoint);\n\n                // Go to the endpoint after the added edge\n                currentEndpoint = currentEndpoint ^ 1;\n            }\n            cycleCount++;\n        }\n        return new KOptCycle(cycleCount, indexToCycle);\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtils.java", "func_name": "getCyclesForPermutation", "context": "package ai.timefold.solver.core.impl.heuristic.selector.move.generic.list.kopt;\n\nimport java.util.ArrayList;\nimport java.util.BitSet;\nimport java.util.List;\nimport java.util.function.Function;\n\nimport ai.timefold.solver.core.api.function.TriPredicate;\nimport ai.timefold.solver.core.impl.domain.variable.ListVariableStateSupply;\nimport ai.timefold.solver.core.impl.domain.variable.index.IndexVariableSupply;\nimport ai.timefold.solver.core.impl.util.Pair;\n\nimport org.apache.commons.math3.util.CombinatoricsUtils;\n\nfinal class KOptUtils {\n\n    private KOptUtils() {\n    }", "func_start": 46, "func_end": 76, "body_len": 30, "test_file": "core/src/test/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtilsTest.java", "test_start": 122, "test_end": 149, "test_code": "    @Test\n    void testGetCyclesForPermutationThreeCycle() {\n        List<TestdataListValue> originalTour = List.of(v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12);\n        List<TestdataListValue> removedEdges = List.of(v1, v2,\n                v3, v4,\n                v5, v6,\n                v7, v8,\n                v9, v10,\n                v11, v12);\n        List<TestdataListValue> addedEdges = List.of(v1, v4,\n                v5, v12,\n                v3, v6,\n                v7, v2,\n                v8, v10,\n                v11, v9);\n\n        KOptDescriptor<TestdataListValue> kOptDescriptor = fromRemovedAndAddedEdges(originalTour,\n                removedEdges,\n                addedEdges);\n        KOptCycle cycleInfo = KOptUtils.getCyclesForPermutation(kOptDescriptor);\n        assertThat(cycleInfo.cycleCount()).isEqualTo(3);\n\n        // Cycles:\n        // v1 -> v4 -> v5 -> v12\n        // v2 -> v7 -> v6 -> v3\n        // v8 -> v10 -> v11 -> v9\n        assertThat(cycleInfo.indexToCycleIdentifier()).containsExactly(0, 0, 1, 1, 0, 0, 1, 1, 2, 2, 2, 2, 0);\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.heuristic.selector.move.generic.list.kopt.KOptUtilsTest#testGetCyclesForPermutationThreeCycle\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1063}
{"hash_id": "dcb5dab97ac7ba282aeed44a", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `diff` in the `ScoreAnalysis` class that takes another `ScoreAnalysis` object and returns a new `ScoreAnalysis` representing the difference (this - other), computing differences in score and constraint weights. Include all constraints from both objects in the result, regardless of whether the difference is zero. You should write self-contained code starting with: ```java\npublic @NonNull ScoreAnalysis<Score_> diff(@NonNull ScoreAnalysis<Score_> other) {```", "solution": "public @NonNull ScoreAnalysis<Score_> diff(@NonNull ScoreAnalysis<Score_> other) {\n    var result = Stream.concat(constraintMap.keySet().stream(),\n            other.constraintMap.keySet().stream())\n            .distinct()\n            .collect(Collectors.toMap(\n                    constraintRef -> constraintRef,\n                    constraintRef -> {\n                        var constraintAnalysis = getConstraintAnalysis(constraintRef);\n                        var otherConstraintAnalysis = other.getConstraintAnalysis(constraintRef);\n                        return ConstraintAnalysis.diff(constraintRef, constraintAnalysis, otherConstraintAnalysis);\n                    },\n                    (existing, replacement) -> existing,\n                    HashMap::new));\n    return new ScoreAnalysis<>(score.subtract(other.score()), result, isSolutionInitialized);\n}", "test": "@Test\nvoid empty() {\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n    var scoreAnalysis2 = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n\n    var diff = scoreAnalysis.diff(scoreAnalysis2);\n    assertSoftly(softly -> {\n        softly.assertThat(diff.score()).isEqualTo(SimpleScore.of(0));\n        softly.assertThat(diff.constraintMap()).isEmpty();\n    });\n}"}, {"turn_num": 2, "instruction": "Modify the `diff` function to exclude constraint analyses from the result if both the weight difference and score difference are zero. Enhance the `diff` function to also consider detailed match analyses: include a constraint in the result if there is a non-zero difference in match details, detected via non-empty match diffs from `ConstraintAnalysis.diff`.", "solution": "public @NonNull ScoreAnalysis<Score_> diff(@NonNull ScoreAnalysis<Score_> other) {\n    var result = Stream.concat(constraintMap.keySet().stream(),\n            other.constraintMap.keySet().stream())\n            .distinct()\n            .flatMap(constraintRef -> {\n                var constraintAnalysis = getConstraintAnalysis(constraintRef);\n                var otherConstraintAnalysis = other.getConstraintAnalysis(constraintRef);\n                var diff = ConstraintAnalysis.diff(constraintRef, constraintAnalysis, otherConstraintAnalysis);\n                \n                // Exclude if both weight and score differences are zero\n                if (diff.weight().isZero() && diff.score().isZero()) {\n                    // Check if there are match differences\n                    if (diff.matches() != null && !diff.matches().isEmpty()) {\n                        return Stream.of(diff);\n                    } else {\n                        return Stream.empty();\n                    }\n                }\n                \n                return Stream.of(diff);\n            })\n            .collect(Collectors.toMap(\n                    ConstraintAnalysis::constraintRef,\n                    Function.identity(),\n                    (existing, replacement) -> existing,\n                    HashMap::new));\n    return new ScoreAnalysis<>(score.subtract(other.score()), result, isSolutionInitialized);\n}", "test": "@Test\nvoid empty() {\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n    var scoreAnalysis2 = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n\n    var diff = scoreAnalysis.diff(scoreAnalysis2);\n    assertSoftly(softly -> {\n        softly.assertThat(diff.score()).isEqualTo(SimpleScore.of(0));\n        softly.assertThat(diff.constraintMap()).isEmpty();\n    });\n\n    var summary = scoreAnalysis.summarize();\n    assertThat(summary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                    \"\"\");\n}"}, {"turn_num": 3, "instruction": "Ensure the `diff` function excludes constraints where match data is unchanged or absent, and only includes match differences when they are meaningful. If one `ScoreAnalysis` has match data and the other does not, throw an exception to enforce compatibility.", "solution": "public @NonNull ScoreAnalysis<Score_> diff(@NonNull ScoreAnalysis<Score_> other) {\n    var result = Stream.concat(constraintMap.keySet().stream(),\n            other.constraintMap.keySet().stream())\n            .distinct()\n            .flatMap(constraintRef -> {\n                var constraintAnalysis = getConstraintAnalysis(constraintRef);\n                var otherConstraintAnalysis = other.getConstraintAnalysis(constraintRef);\n                var diff = ConstraintAnalysis.diff(constraintRef, constraintAnalysis, otherConstraintAnalysis);\n                \n                // Check for compatibility: if one has match data and the other doesn't, throw exception\n                boolean thisHasMatches = constraintAnalysis != null && constraintAnalysis.matches() != null;\n                boolean otherHasMatches = otherConstraintAnalysis != null && otherConstraintAnalysis.matches() != null;\n                \n                if (thisHasMatches != otherHasMatches) {\n                    throw new IllegalArgumentException(\"Incompatible ScoreAnalysis: one has match data while the other does not for constraint: \" + constraintRef);\n                }\n                \n                // Exclude if both weight and score differences are zero\n                if (diff.weight().isZero() && diff.score().isZero()) {\n                    // Only include if there are meaningful match differences\n                    if (diff.matches() != null && !diff.matches().isEmpty()) {\n                        return Stream.of(diff);\n                    } else {\n                        return Stream.empty();\n                    }\n                }\n                \n                return Stream.of(diff);\n            })\n            .collect(Collectors.toMap(\n                    ConstraintAnalysis::constraintRef,\n                    Function.identity(),\n                    (existing, replacement) -> existing,\n                    HashMap::new));\n    return new ScoreAnalysis<>(score.subtract(other.score()), result, isSolutionInitialized);\n}", "test": "@Test\nvoid empty() {\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n    var scoreAnalysis2 = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n\n    var diff = scoreAnalysis.diff(scoreAnalysis2);\n    assertSoftly(softly -> {\n        softly.assertThat(diff.score()).isEqualTo(SimpleScore.of(0));\n        softly.assertThat(diff.constraintMap()).isEmpty();\n    });\n\n    var summary = scoreAnalysis.summarize();\n    assertThat(summary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                    \"\"\");\n}"}, {"turn_num": 4, "instruction": "Preserve the `isSolutionInitialized` flag from `this` in the resulting `ScoreAnalysis` object, and ensure the final result does not bloat by omitting constraints with no meaningful changes in weight, score, or match details.", "solution": "public @NonNull ScoreAnalysis<Score_> diff(@NonNull ScoreAnalysis<Score_> other) {\n    var result = Stream.concat(constraintMap.keySet().stream(),\n            other.constraintMap.keySet().stream())\n            .distinct()\n            .flatMap(constraintRef -> {\n                var constraintAnalysis = getConstraintAnalysis(constraintRef);\n                var otherConstraintAnalysis = other.getConstraintAnalysis(constraintRef);\n                var diff = ConstraintAnalysis.diff(constraintRef, constraintAnalysis, otherConstraintAnalysis);\n                // The following code implements logic to decide which information the user needs to see,\n                // and which is information we can safely discard.\n                // This is done so that the diff (which is likely to be serialized into JSON) is not bloated.\n                if (!diff.weight().isZero() || !diff.score().isZero()) { // Guaranteed change.\n                    return Stream.of(diff);\n                }\n                // Figuring out whether constraint matches changed is tricky.\n                // Can't use constraint weight; weight diff on the same constraint is zero if weight unchanged.\n                // Can't use matchCount; matchCount diff can be zero if one match was added and another removed.\n                // To detect if the constraint matches changed, we use the actual match diff.\n                if (diff.matches() == null) {\n                    // If it is null, either justifications are disabled,\n                    // or constraint matching is disabled altogether.\n                    // This means we don't have enough information to make smarter decisions.\n                    if (diff.matchCount() == 0) {\n                        // Returning this makes no practical sense.\n                        // The result would be constraint name + zero weight + zero score + zero match count.\n                        return Stream.empty();\n                    } else {\n                        return Stream.of(diff);\n                    }\n                } else if (!diff.matches().isEmpty()) {\n                    // We actually have constraint matches, and they are meaningfully different.\n                    return Stream.of(diff);\n                } else {\n                    // This will be empty only if all matches are exactly the same.\n                    return Stream.empty();\n                }\n            })\n            .collect(Collectors.toMap(\n                    ConstraintAnalysis::constraintRef,\n                    Function.identity(),\n                    (constraintRef, otherConstraintRef) -> constraintRef,\n                    HashMap::new));\n    return new ScoreAnalysis<>(score.subtract(other.score()), result, isSolutionInitialized);\n}", "test": "@Test\nvoid empty() {\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n    var scoreAnalysis2 = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n\n    var diff = scoreAnalysis.diff(scoreAnalysis2);\n    assertSoftly(softly -> {\n        softly.assertThat(diff.score()).isEqualTo(SimpleScore.of(0));\n        softly.assertThat(diff.constraintMap()).isEmpty();\n    });\n\n    var summary = scoreAnalysis.summarize();\n    assertThat(summary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                    \"\"\");\n}"}], "metadata": {"source": "mrgbench", "func": "    public @NonNull ScoreAnalysis<Score_> diff(@NonNull ScoreAnalysis<Score_> other) {\n        var result = Stream.concat(constraintMap.keySet().stream(),\n                other.constraintMap.keySet().stream())\n                .distinct()\n                .flatMap(constraintRef -> {\n                    var constraintAnalysis = getConstraintAnalysis(constraintRef);\n                    var otherConstraintAnalysis = other.getConstraintAnalysis(constraintRef);\n                    var diff = ConstraintAnalysis.diff(constraintRef, constraintAnalysis, otherConstraintAnalysis);\n                    // The following code implements logic to decide which information the user needs to see,\n                    // and which is information we can safely discard.\n                    // This is done so that the diff (which is likely to be serialized into JSON) is not bloated.\n                    if (!diff.weight().isZero() || !diff.score().isZero()) { // Guaranteed change.\n                        return Stream.of(diff);\n                    }\n                    // Figuring out whether constraint matches changed is tricky.\n                    // Can't use constraint weight; weight diff on the same constraint is zero if weight unchanged.\n                    // Can't use matchCount; matchCount diff can be zero if one match was added and another removed.\n                    // To detect if the constraint matches changed, we use the actual match diff.\n                    if (diff.matches() == null) {\n                        // If it is null, either justifications are disabled,\n                        // or constraint matching is disabled altogether.\n                        // This means we don't have enough information to make smarter decisions.\n                        if (diff.matchCount() == 0) {\n                            // Returning this makes no practical sense.\n                            // The result would be constraint name + zero weight + zero score + zero match count.\n                            return Stream.empty();\n                        } else {\n                            return Stream.of(diff);\n                        }\n                    } else if (!diff.matches().isEmpty()) {\n                        // We actually have constraint matches, and they are meaningfully different.\n                        return Stream.of(diff);\n                    } else {\n                        // This will be empty only if all matches are exactly the same.\n                        return Stream.empty();\n                    }\n                })\n                .collect(Collectors.toMap(\n                        ConstraintAnalysis::constraintRef,\n                        Function.identity(),\n                        (constraintRef, otherConstraintRef) -> constraintRef,\n                        HashMap::new));\n        return new ScoreAnalysis<>(score.subtract(other.score()), result, isSolutionInitialized);\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysis.java", "func_name": "diff", "context": "package ai.timefold.solver.core.api.score.analysis;\n\nimport static java.util.Comparator.comparing;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.LinkedHashMap;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.function.Function;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport ai.timefold.solver.core.api.score.Score;\nimport ai.timefold.solver.core.api.score.ScoreExplanation;\nimport ai.timefold.solver.core.api.score.constraint.ConstraintRef;\nimport ai.timefold.solver.core.api.score.stream.Constraint;\nimport ai.timefold.solver.core.api.score.stream.ConstraintJustification;\nimport ai.timefold.solver.core.api.solver.ScoreAnalysisFetchPolicy;\nimport ai.timefold.solver.core.api.solver.SolutionManager;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * Represents the breakdown of a {@link Score} into individual {@link ConstraintAnalysis} instances,\n * one for each constraint.\n * Compared to {@link ScoreExplanation}, this is JSON-friendly and faster to generate.\n *\n * <p>\n * In order to be fully serializable to JSON, {@link MatchAnalysis} instances must be serializable to JSON\n * and that requires any implementations of {@link ConstraintJustification} to be serializable to JSON.\n * This is the responsibility of the user.\n *\n * <p>\n * For deserialization from JSON, the user needs to provide the deserializer themselves.\n * This is due to the fact that, once the {@link ScoreAnalysis} is received over the wire,\n * we no longer know which {@link Score} type or {@link ConstraintJustification} type was used.\n * The user has all of that information in their domain model,\n * and so they are the correct party to provide the deserializer.\n *\n * <p>\n * Note: the constructors of this record are off-limits.\n * We ask users to use exclusively {@link SolutionManager#analyze(Object)} to obtain instances of this record.\n *\n * @param score Score of the solution being analyzed.\n * @param constraintMap for each constraint identified by its {@link Constraint#getConstraintRef()},\n *        the {@link ConstraintAnalysis} that describes the impact of that constraint on the overall score.\n *        <p>\n *        Zero-weight constraints are never included, they are excluded from score calculation in the first place.\n *        Otherwise constraints are always included, even if they have no matches,\n *        unless the score analysis represents a diff between two other analyses.\n * \n *        <p>\n *        In the case of a diff:\n * \n *        <ul>\n *        <li>If the constraint weight diff is non-zero,\n *        or if the score diff for the constraint is non-zero,\n *        the constraint diff will be included.</li>\n *        <li>\n *        Otherwise if constraint matching is disabled ({@link ScoreAnalysisFetchPolicy#FETCH_SHALLOW})\n *        or if only match counts are available ({@link ScoreAnalysisFetchPolicy#FETCH_MATCH_COUNT}),\n *        constraint diff will only be included if it has a non-zero match count diff.\n *        </li>\n *        <li>\n *        Otherwise (when constraint matching is fully enabled with {@link ScoreAnalysisFetchPolicy#FETCH_ALL})\n *        the constraint diff will not be included if the diff of its constraint matches is empty.\n *        (In other words: when diffing, the analysis for a particular constraint won't be available\n *        if we can guarantee that the constraint matches are identical in both analyses.)\n *        </li>\n *        </ul>\n * \n *        <p>\n *        Entries in the map have a stable iteration order; items are ordered first by {@link ConstraintAnalysis#weight()},\n *        then by {@link ConstraintAnalysis#constraintRef()}.\n * @param isSolutionInitialized Whether the solution was fully initialized at the time of analysis.\n *\n * @param <Score_>\n */\npublic record ScoreAnalysis<Score_ extends Score<Score_>>(@NonNull Score_ score,\n        @NonNull Map<ConstraintRef, ConstraintAnalysis<Score_>> constraintMap,\n        boolean isSolutionInitialized) {\n\n    @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n    private static final Comparator<ConstraintAnalysis<?>> REVERSED_WEIGHT_COMPARATOR =\n            Comparator.<ConstraintAnalysis<?>, Score> comparing(ConstraintAnalysis::weight)\n                    .reversed();\n    private static final Comparator<ConstraintAnalysis<?>> MAP_COMPARATOR =\n            REVERSED_WEIGHT_COMPARATOR.thenComparing(ConstraintAnalysis::constraintRef);\n\n    static final int DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT = 3;\n\n    /**\n     * As defined by {@link #ScoreAnalysis(Score, Map, boolean)},\n     * with the final argument set to true.\n     */\n    public ScoreAnalysis(@NonNull Score_ score, @NonNull Map<ConstraintRef, ConstraintAnalysis<Score_>> constraintMap) {\n        this(score, constraintMap, true);\n    }\n\n    public ScoreAnalysis {\n        Objects.requireNonNull(score, \"score\");\n        Objects.requireNonNull(constraintMap, \"constraintMap\");\n        // Ensure consistent order and no external interference.\n        constraintMap = Collections.unmodifiableMap(constraintMap.values()\n                .stream()\n                .sorted(MAP_COMPARATOR)\n                .collect(Collectors.toMap(\n                        ConstraintAnalysis::constraintRef,\n                        Function.identity(),\n                        (constraintAnalysis, otherConstraintAnalysis) -> constraintAnalysis,\n                        LinkedHashMap::new)));\n    }\n\n    /**\n     * Performs a lookup on {@link #constraintMap()}.\n     * Equivalent to {@code constraintMap().get(constraintRef)}.\n     *\n     * @return null if no constraint matches of such constraint are present\n     */\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull ConstraintRef constraintRef) {\n        return constraintMap.get(constraintRef);\n    }\n\n    /**\n     * As defined by {@link #getConstraintAnalysis(ConstraintRef)}\n     * where the arguments are first composed into a singular constraint ID.\n     *\n     * @return null if no constraint matches of such constraint are present\n     * @deprecated Use {@link #getConstraintAnalysis(String)} instead.\n     */\n    @Deprecated(forRemoval = true, since = \"1.13.0\")\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull String constraintPackage,\n            @NonNull String constraintName) {\n        return getConstraintAnalysis(ConstraintRef.of(constraintPackage, constraintName));\n    }\n\n    /**\n     * As defined by {@link #getConstraintAnalysis(ConstraintRef)}.\n     *\n     * @return null if no constraint matches of such constraint are present\n     * @throws IllegalStateException if multiple constraints with the same name are present,\n     *         which is possible if they are in different constraint packages.\n     *         Constraint packages are deprecated, we recommend avoiding them and instead naming constraints uniquely.\n     *         If you must use constraint packages, see {@link #getConstraintAnalysis(String, String)}\n     *         (also deprecated) and reach out to us to discuss your use case.\n     */\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull String constraintName) {\n        var constraintAnalysisList = constraintMap.entrySet()\n                .stream()\n                .filter(entry -> entry.getKey().constraintName().equals(constraintName))\n                .map(Map.Entry::getValue)\n                .toList();\n        return switch (constraintAnalysisList.size()) {\n            case 0 -> null;\n            case 1 -> constraintAnalysisList.get(0);\n            default -> throw new IllegalStateException(\"\"\"\n                    Multiple constraints with the same name (%s) are present in the score analysis.\n                    This may be caused by the use of multiple constraint packages, a deprecated feature.\n                    Please avoid using constraint packages and keep constraint names unique.\"\"\"\n                    .formatted(constraintName));\n        };\n    }", "func_start": 193, "func_end": 236, "body_len": 43, "test_file": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java", "test_start": 25, "test_end": 42, "test_code": "    @Test\n    void empty() {\n        var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n        var scoreAnalysis2 = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n\n        var diff = scoreAnalysis.diff(scoreAnalysis2);\n        assertSoftly(softly -> {\n            softly.assertThat(diff.score()).isEqualTo(SimpleScore.of(0));\n            softly.assertThat(diff.constraintMap()).isEmpty();\n        });\n\n        var summary = scoreAnalysis.summarize();\n        assertThat(summary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                        \"\"\");\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.api.score.analysis.ScoreAnalysisTest#empty\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1064}
{"hash_id": "ed043aa2b7edbdb82b5c8faf", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `summarize()` that returns a string describing the score of a solution by listing each constraint's name and score, along with its matches' scores and justifications. You should write self-contained code starting with: ```java\npublic @NonNull String summarize() {```", "solution": "public @NonNull String summarize() {\n    StringBuilder summary = new StringBuilder();\n    summary.append(\"Explanation of score (%s):\\n\".formatted(score));\n    summary.append(\"    Constraint matches:\\n\");\n    \n    constraintAnalyses().stream()\n            .sorted(comparing(ConstraintAnalysis::score))\n            .forEach(constraint -> {\n                var matches = constraint.matches();\n                if (matches == null) {\n                    throw new IllegalArgumentException(\n                            \"The constraint matches must be non-null.\\n\" +\n                            \"Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\");\n                }\n                if (matches.isEmpty()) {\n                    summary.append(\n                            \"        %s: constraint (%s) has no matches.\\n\".formatted(\n                                    constraint.score().toShortString(),\n                                    constraint.constraintRef().constraintName()));\n                } else {\n                    summary.append(\n                            \"        %s: constraint (%s) has %s matches:\\n\".formatted(\n                                    constraint.score().toShortString(),\n                                    constraint.constraintRef().constraintName(), \n                                    matches.size()));\n                    matches.stream()\n                            .sorted(comparing(MatchAnalysis::score))\n                            .forEach(match -> summary\n                                    .append(\"            %s: justified with (%s)\\n\".formatted(\n                                            match.score().toShortString(),\n                                            match.justification())));\n                }\n            });\n    \n    return summary.toString();\n}", "test": "@Test\nvoid summarizeUninitializedSolution() {\n    var constraintPackage = \"constraintPackage\";\n    var constraintName1 = \"constraint1\";\n    var constraintName2 = \"constraint2\";\n    var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n    var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n    var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n    var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n    var constraintAnalysisMap = Map.of(\n            constraintMatchTotal.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n            constraintMatchTotal2.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n    // Single constraint analysis\n    var constraintSummary = constraintAnalysisMap.get(constraintMatchTotal.getConstraintRef()).summarize();\n    assertThat(constraintSummary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                            0: constraint (constraint1) has no matches.\n                    \"\"\");\n\n    // Complete score analysis\n    var summary = scoreAnalysis.summarize();\n    assertThat(summary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                            0: constraint (constraint1) has no matches.\n                            0: constraint (constraint2) has no matches.\n                    \"\"\");\n}"}, {"turn_num": 2, "instruction": "Sort the constraints and their matches in ascending order by score using the score() method. Include the total number of matches per constraint in the output, and if a constraint has no matches, explicitly state that it has no matches instead of listing an empty list.", "solution": "@SuppressWarnings(\"java:S3457\")\npublic @NonNull String summarize() {\n    StringBuilder summary = new StringBuilder();\n    summary.append(\"\"\"\n            Explanation of score (%s):\n                Constraint matches:\n            \"\"\".formatted(score));\n    \n    constraintAnalyses().stream()\n            .sorted(comparing(ConstraintAnalysis::score))\n            .forEach(constraint -> {\n                var matches = constraint.matches();\n                if (matches == null) {\n                    throw new IllegalArgumentException(\"\"\"\n                            The constraint matches must be non-null.\n                            Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\n                            \"\"\");\n                }\n                if (matches.isEmpty()) {\n                    summary.append(\n                            \"%8s%s: constraint (%s) has no matches.\\n\".formatted(\" \", constraint.score().toShortString(),\n                                    constraint.constraintRef().constraintName()));\n                } else {\n                    summary.append(\n                            \"%8s%s: constraint (%s) has %s matches:\\n\".formatted(\" \", constraint.score().toShortString(),\n                                    constraint.constraintRef().constraintName(), matches.size()));\n                    matches.stream()\n                            .sorted(comparing(MatchAnalysis::score))\n                            .forEach(match -> summary\n                                    .append(\"%12s%s: justified with (%s)\\n\".formatted(\" \", match.score().toShortString(),\n                                            match.justification())));\n                }\n            });\n    \n    return summary.toString();\n}", "test": "@Test\nvoid summarizeUninitializedSolution() {\n    var constraintPackage = \"constraintPackage\";\n    var constraintName1 = \"constraint1\";\n    var constraintName2 = \"constraint2\";\n    var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n    var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n    var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n    var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n    var constraintAnalysisMap = Map.of(\n            constraintMatchTotal.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n            constraintMatchTotal2.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n    // Complete score analysis - verify sorting by score and match count display\n    var summary = scoreAnalysis.summarize();\n    assertThat(summary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                            0: constraint (constraint1) has no matches.\n                            0: constraint (constraint2) has no matches.\n                    \"\"\");\n}"}, {"turn_num": 3, "instruction": "Limit the number of matches displayed per constraint to DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT, and if there are more matches than the limit, append an ellipsis ('...') after the last listed match.", "solution": "@SuppressWarnings(\"java:S3457\")\npublic @NonNull String summarize() {\n    StringBuilder summary = new StringBuilder();\n    summary.append(\"\"\"\n            Explanation of score (%s):\n                Constraint matches:\n            \"\"\".formatted(score));\n    \n    constraintAnalyses().stream()\n            .sorted(comparing(ConstraintAnalysis::score))\n            .forEach(constraint -> {\n                var matches = constraint.matches();\n                if (matches == null) {\n                    throw new IllegalArgumentException(\"\"\"\n                            The constraint matches must be non-null.\n                            Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\n                            \"\"\");\n                }\n                if (matches.isEmpty()) {\n                    summary.append(\n                            \"%8s%s: constraint (%s) has no matches.\\n\".formatted(\" \", constraint.score().toShortString(),\n                                    constraint.constraintRef().constraintName()));\n                } else {\n                    summary.append(\n                            \"%8s%s: constraint (%s) has %s matches:\\n\".formatted(\" \", constraint.score().toShortString(),\n                                    constraint.constraintRef().constraintName(), matches.size()));\n                    matches.stream()\n                            .sorted(comparing(MatchAnalysis::score))\n                            .limit(DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT)\n                            .forEach(match -> summary\n                                    .append(\"%12s%s: justified with (%s)\\n\".formatted(\" \", match.score().toShortString(),\n                                            match.justification())));\n                    if (matches.size() > DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT) {\n                        summary.append(\"%12s%s\\n\".formatted(\" \", \"...\"));\n                    }\n                }\n            });\n    \n    return summary.toString();\n}", "test": "@Test\nvoid summarizeUninitializedSolution() {\n    var constraintPackage = \"constraintPackage\";\n    var constraintName1 = \"constraint1\";\n    var constraintName2 = \"constraint2\";\n    var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n    var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n    var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n    var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n    var constraintAnalysisMap = Map.of(\n            constraintMatchTotal.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n            constraintMatchTotal2.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n    // Single constraint analysis\n    var constraintSummary = constraintAnalysisMap.get(constraintMatchTotal.getConstraintRef()).summarize();\n    assertThat(constraintSummary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                            0: constraint (constraint1) has no matches.\n                    \"\"\");\n\n    // Complete score analysis\n    var summary = scoreAnalysis.summarize();\n    assertThat(scoreAnalysis.getConstraintAnalysis(constraintName1).matchCount()).isZero();\n    assertThat(summary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                            0: constraint (constraint1) has no matches.\n                            0: constraint (constraint2) has no matches.\n                    \"\"\");\n}"}, {"turn_num": 4, "instruction": "Throw an IllegalArgumentException if any constraint's matches are null, with a message suggesting to use ScoreAnalysisFetchPolicy.FETCH_ALL, and ensure the overall score is included at the beginning of the summary in the format 'Explanation of score (X):'.", "solution": "@SuppressWarnings(\"java:S3457\")\npublic @NonNull String summarize() {\n    StringBuilder summary = new StringBuilder();\n    summary.append(\"\"\"\n            Explanation of score (%s):\n                Constraint matches:\n            \"\"\".formatted(score));\n    \n    constraintAnalyses().stream()\n            .sorted(comparing(ConstraintAnalysis::score))\n            .forEach(constraint -> {\n                var matches = constraint.matches();\n                if (matches == null) {\n                    throw new IllegalArgumentException(\"\"\"\n                            The constraint matches must be non-null.\n                            Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\n                            \"\"\");\n                }\n                if (matches.isEmpty()) {\n                    summary.append(\n                            \"%8s%s: constraint (%s) has no matches.\\n\".formatted(\" \", constraint.score().toShortString(),\n                                    constraint.constraintRef().constraintName()));\n                } else {\n                    summary.append(\n                            \"%8s%s: constraint (%s) has %s matches:\\n\".formatted(\" \", constraint.score().toShortString(),\n                                    constraint.constraintRef().constraintName(), matches.size()));\n                    matches.stream()\n                            .sorted(comparing(MatchAnalysis::score))\n                            .limit(DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT)\n                            .forEach(match -> summary\n                                    .append(\"%12s%s: justified with (%s)\\n\".formatted(\" \", match.score().toShortString(),\n                                            match.justification())));\n                    if (matches.size() > DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT) {\n                        summary.append(\"%12s%s\\n\".formatted(\" \", \"...\"));\n                    }\n                }\n            });\n    \n    return summary.toString();\n}", "test": "@Test\nvoid summarizeUninitializedSolution() {\n    var constraintPackage = \"constraintPackage\";\n    var constraintName1 = \"constraint1\";\n    var constraintName2 = \"constraint2\";\n    var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n    var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n    var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n    var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n    var constraintAnalysisMap = Map.of(\n            constraintMatchTotal.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n            constraintMatchTotal2.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n    // Test IllegalArgumentException when matches are null\n    var constraintAnalysisWithNullMatches = getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_SHALLOW);\n    assertThatThrownBy(() -> constraintAnalysisWithNullMatches.summarize())\n            .isInstanceOf(IllegalArgumentException.class)\n            .hasMessageContaining(\"The constraint matches must be non-null\")\n            .hasMessageContaining(\"Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL\");\n\n    // Test score format in summary\n    var summary = scoreAnalysis.summarize();\n    assertThat(summary)\n            .startsWith(\"Explanation of score (0):\");\n}"}], "metadata": {"source": "mrgbench", "func": "    @SuppressWarnings(\"java:S3457\")\n    public @NonNull String summarize() {\n        StringBuilder summary = new StringBuilder();\n        summary.append(\"\"\"\n                Explanation of score (%s):\n                    Constraint matches:\n                \"\"\".formatted(score));\n        Comparator<ConstraintAnalysis<Score_>> constraintsScoreComparator = comparing(ConstraintAnalysis::score);\n        Comparator<MatchAnalysis<Score_>> matchScoreComparator = comparing(MatchAnalysis::score);\n\n        constraintAnalyses().stream()\n                .sorted(constraintsScoreComparator)\n                .forEach(constraint -> {\n                    var matches = constraint.matches();\n                    if (matches == null) {\n                        throw new IllegalArgumentException(\"\"\"\n                                The constraint matches must be non-null.\n                                Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\n                                \"\"\");\n                    }\n                    if (matches.isEmpty()) {\n                        summary.append(\n                                \"%8s%s: constraint (%s) has no matches.\\n\".formatted(\" \", constraint.score().toShortString(),\n                                        constraint.constraintRef().constraintName()));\n                    } else {\n                        summary.append(\n                                \"%8s%s: constraint (%s) has %s matches:\\n\".formatted(\" \", constraint.score().toShortString(),\n                                        constraint.constraintRef().constraintName(), matches.size()));\n                    }\n                    matches.stream()\n                            .sorted(matchScoreComparator)\n                            .limit(DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT)\n                            .forEach(match -> summary\n                                    .append(\"%12s%s: justified with (%s)\\n\".formatted(\" \", match.score().toShortString(),\n                                            match.justification())));\n                    if (matches.size() > DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT) {\n                        summary.append(\"%12s%s\\n\".formatted(\" \", \"...\"));\n                    }\n                });\n\n        return summary.toString();\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysis.java", "func_name": "summarize", "context": "package ai.timefold.solver.core.api.score.analysis;\n\nimport static java.util.Comparator.comparing;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.LinkedHashMap;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.function.Function;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport ai.timefold.solver.core.api.score.Score;\nimport ai.timefold.solver.core.api.score.ScoreExplanation;\nimport ai.timefold.solver.core.api.score.constraint.ConstraintRef;\nimport ai.timefold.solver.core.api.score.stream.Constraint;\nimport ai.timefold.solver.core.api.score.stream.ConstraintJustification;\nimport ai.timefold.solver.core.api.solver.ScoreAnalysisFetchPolicy;\nimport ai.timefold.solver.core.api.solver.SolutionManager;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * Represents the breakdown of a {@link Score} into individual {@link ConstraintAnalysis} instances,\n * one for each constraint.\n * Compared to {@link ScoreExplanation}, this is JSON-friendly and faster to generate.\n *\n * <p>\n * In order to be fully serializable to JSON, {@link MatchAnalysis} instances must be serializable to JSON\n * and that requires any implementations of {@link ConstraintJustification} to be serializable to JSON.\n * This is the responsibility of the user.\n *\n * <p>\n * For deserialization from JSON, the user needs to provide the deserializer themselves.\n * This is due to the fact that, once the {@link ScoreAnalysis} is received over the wire,\n * we no longer know which {@link Score} type or {@link ConstraintJustification} type was used.\n * The user has all of that information in their domain model,\n * and so they are the correct party to provide the deserializer.\n *\n * <p>\n * Note: the constructors of this record are off-limits.\n * We ask users to use exclusively {@link SolutionManager#analyze(Object)} to obtain instances of this record.\n *\n * @param score Score of the solution being analyzed.\n * @param constraintMap for each constraint identified by its {@link Constraint#getConstraintRef()},\n *        the {@link ConstraintAnalysis} that describes the impact of that constraint on the overall score.\n *        <p>\n *        Zero-weight constraints are never included, they are excluded from score calculation in the first place.\n *        Otherwise constraints are always included, even if they have no matches,\n *        unless the score analysis represents a diff between two other analyses.\n * \n *        <p>\n *        In the case of a diff:\n * \n *        <ul>\n *        <li>If the constraint weight diff is non-zero,\n *        or if the score diff for the constraint is non-zero,\n *        the constraint diff will be included.</li>\n *        <li>\n *        Otherwise if constraint matching is disabled ({@link ScoreAnalysisFetchPolicy#FETCH_SHALLOW})\n *        or if only match counts are available ({@link ScoreAnalysisFetchPolicy#FETCH_MATCH_COUNT}),\n *        constraint diff will only be included if it has a non-zero match count diff.\n *        </li>\n *        <li>\n *        Otherwise (when constraint matching is fully enabled with {@link ScoreAnalysisFetchPolicy#FETCH_ALL})\n *        the constraint diff will not be included if the diff of its constraint matches is empty.\n *        (In other words: when diffing, the analysis for a particular constraint won't be available\n *        if we can guarantee that the constraint matches are identical in both analyses.)\n *        </li>\n *        </ul>\n * \n *        <p>\n *        Entries in the map have a stable iteration order; items are ordered first by {@link ConstraintAnalysis#weight()},\n *        then by {@link ConstraintAnalysis#constraintRef()}.\n * @param isSolutionInitialized Whether the solution was fully initialized at the time of analysis.\n *\n * @param <Score_>\n */\npublic record ScoreAnalysis<Score_ extends Score<Score_>>(@NonNull Score_ score,\n        @NonNull Map<ConstraintRef, ConstraintAnalysis<Score_>> constraintMap,\n        boolean isSolutionInitialized) {\n\n    @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n    private static final Comparator<ConstraintAnalysis<?>> REVERSED_WEIGHT_COMPARATOR =\n            Comparator.<ConstraintAnalysis<?>, Score> comparing(ConstraintAnalysis::weight)\n                    .reversed();\n    private static final Comparator<ConstraintAnalysis<?>> MAP_COMPARATOR =\n            REVERSED_WEIGHT_COMPARATOR.thenComparing(ConstraintAnalysis::constraintRef);\n\n    static final int DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT = 3;\n\n    /**\n     * As defined by {@link #ScoreAnalysis(Score, Map, boolean)},\n     * with the final argument set to true.\n     */\n    public ScoreAnalysis(@NonNull Score_ score, @NonNull Map<ConstraintRef, ConstraintAnalysis<Score_>> constraintMap) {\n        this(score, constraintMap, true);\n    }\n\n    public ScoreAnalysis {\n        Objects.requireNonNull(score, \"score\");\n        Objects.requireNonNull(constraintMap, \"constraintMap\");\n        // Ensure consistent order and no external interference.\n        constraintMap = Collections.unmodifiableMap(constraintMap.values()\n                .stream()\n                .sorted(MAP_COMPARATOR)\n                .collect(Collectors.toMap(\n                        ConstraintAnalysis::constraintRef,\n                        Function.identity(),\n                        (constraintAnalysis, otherConstraintAnalysis) -> constraintAnalysis,\n                        LinkedHashMap::new)));\n    }\n\n    /**\n     * Performs a lookup on {@link #constraintMap()}.\n     * Equivalent to {@code constraintMap().get(constraintRef)}.\n     *\n     * @return null if no constraint matches of such constraint are present\n     */\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull ConstraintRef constraintRef) {\n        return constraintMap.get(constraintRef);\n    }\n\n    /**\n     * As defined by {@link #getConstraintAnalysis(ConstraintRef)}\n     * where the arguments are first composed into a singular constraint ID.\n     *\n     * @return null if no constraint matches of such constraint are present\n     * @deprecated Use {@link #getConstraintAnalysis(String)} instead.\n     */\n    @Deprecated(forRemoval = true, since = \"1.13.0\")\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull String constraintPackage,\n            @NonNull String constraintName) {\n        return getConstraintAnalysis(ConstraintRef.of(constraintPackage, constraintName));\n    }\n\n    /**\n     * As defined by {@link #getConstraintAnalysis(ConstraintRef)}.\n     *\n     * @return null if no constraint matches of such constraint are present\n     * @throws IllegalStateException if multiple constraints with the same name are present,\n     *         which is possible if they are in different constraint packages.\n     *         Constraint packages are deprecated, we recommend avoiding them and instead naming constraints uniquely.\n     *         If you must use constraint packages, see {@link #getConstraintAnalysis(String, String)}\n     *         (also deprecated) and reach out to us to discuss your use case.\n     */\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull String constraintName) {\n        var constraintAnalysisList = constraintMap.entrySet()\n                .stream()\n                .filter(entry -> entry.getKey().constraintName().equals(constraintName))\n                .map(Map.Entry::getValue)\n                .toList();\n        return switch (constraintAnalysisList.size()) {\n            case 0 -> null;\n            case 1 -> constraintAnalysisList.get(0);\n            default -> throw new IllegalStateException(\"\"\"\n                    Multiple constraints with the same name (%s) are present in the score analysis.\n                    This may be caused by the use of multiple constraint packages, a deprecated feature.\n                    Please avoid using constraint packages and keep constraint names unique.\"\"\"\n                    .formatted(constraintName));\n        };\n    }\n\n    /**\n     * Compare this {@link ScoreAnalysis} to another {@link ScoreAnalysis}\n     * and retrieve the difference between them.\n     * The comparison is in the direction of {@code this - other}.\n     * <p>\n     * Example: if {@code this} has a score of 100 and {@code other} has a score of 90,\n     * the returned {@link ScoreAnalysis#score} will be 10.\n     * If this and other were inverted, the score would have been -10.\n     * The same applies to all other properties of {@link ScoreAnalysis}.\n     *\n     * <p>\n     * In order to properly diff {@link MatchAnalysis} against each other,\n     * we rely on the user implementing {@link ConstraintJustification} equality correctly.\n     * In other words, the diff will consider two justifications equal if the user says they are equal,\n     * and it expects the hash code to be consistent with equals.\n     *\n     * <p>\n     * If one {@link ScoreAnalysis} provides {@link MatchAnalysis} and the other doesn't, exception is thrown.\n     * Such {@link ScoreAnalysis} instances are mutually incompatible.\n     * \n     * <p>\n     * If {@code this} came from a fully initialized solution,\n     * {@link #isSolutionInitialized} will be true.\n     * False otherwise.\n     */\n    public @NonNull ScoreAnalysis<Score_> diff(@NonNull ScoreAnalysis<Score_> other) {\n        var result = Stream.concat(constraintMap.keySet().stream(),\n                other.constraintMap.keySet().stream())\n                .distinct()\n                .flatMap(constraintRef -> {\n                    var constraintAnalysis = getConstraintAnalysis(constraintRef);\n                    var otherConstraintAnalysis = other.getConstraintAnalysis(constraintRef);\n                    var diff = ConstraintAnalysis.diff(constraintRef, constraintAnalysis, otherConstraintAnalysis);\n                    // The following code implements logic to decide which information the user needs to see,\n                    // and which is information we can safely discard.\n                    // This is done so that the diff (which is likely to be serialized into JSON) is not bloated.\n                    if (!diff.weight().isZero() || !diff.score().isZero()) { // Guaranteed change.\n                        return Stream.of(diff);\n                    }\n                    // Figuring out whether constraint matches changed is tricky.\n                    // Can't use constraint weight; weight diff on the same constraint is zero if weight unchanged.\n                    // Can't use matchCount; matchCount diff can be zero if one match was added and another removed.\n                    // To detect if the constraint matches changed, we use the actual match diff.\n                    if (diff.matches() == null) {\n                        // If it is null, either justifications are disabled,\n                        // or constraint matching is disabled altogether.\n                        // This means we don't have enough information to make smarter decisions.\n                        if (diff.matchCount() == 0) {\n                            // Returning this makes no practical sense.\n                            // The result would be constraint name + zero weight + zero score + zero match count.\n                            return Stream.empty();\n                        } else {\n                            return Stream.of(diff);\n                        }\n                    } else if (!diff.matches().isEmpty()) {\n                        // We actually have constraint matches, and they are meaningfully different.\n                        return Stream.of(diff);\n                    } else {\n                        // This will be empty only if all matches are exactly the same.\n                        return Stream.empty();\n                    }\n                })\n                .collect(Collectors.toMap(\n                        ConstraintAnalysis::constraintRef,\n                        Function.identity(),\n                        (constraintRef, otherConstraintRef) -> constraintRef,\n                        HashMap::new));\n        return new ScoreAnalysis<>(score.subtract(other.score()), result, isSolutionInitialized);\n    }\n\n    /**\n     * Returns individual {@link ConstraintAnalysis} instances that make up this {@link ScoreAnalysis}.\n     *\n     * @return equivalent to {@code constraintMap().values()}\n     */\n    public Collection<ConstraintAnalysis<Score_>> constraintAnalyses() {\n        return constraintMap.values();\n    }", "func_start": 260, "func_end": 301, "body_len": 41, "test_file": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java", "test_start": 110, "test_end": 146, "test_code": "    @Test\n    void summarizeUninitializedSolution() {\n        var constraintPackage = \"constraintPackage\";\n        var constraintName1 = \"constraint1\";\n        var constraintName2 = \"constraint2\";\n        var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n        var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n        var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n        var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n        var constraintAnalysisMap = Map.of(\n                constraintMatchTotal.getConstraintRef(),\n                getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n                constraintMatchTotal2.getConstraintRef(),\n                getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n        var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n        // Single constraint analysis\n        var constraintSummary = constraintAnalysisMap.get(constraintMatchTotal.getConstraintRef()).summarize();\n        assertThat(constraintSummary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                                0: constraint (constraint1) has no matches.\n                        \"\"\");\n\n        // Complete score analysis\n        var summary = scoreAnalysis.summarize();\n        assertThat(scoreAnalysis.getConstraintAnalysis(constraintName1).matchCount()).isZero();\n        assertThat(summary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                                0: constraint (constraint1) has no matches.\n                                0: constraint (constraint2) has no matches.\n                        \"\"\");\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.api.score.analysis.ScoreAnalysisTest#summarizeUninitializedSolution\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1065}
{"hash_id": "4651056ca39b311b1f771f62", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a method `summarize()` that returns a string describing the score and listing all constraint matches with their scores and justifications. The output should include the overall score, the number of matches, and for each match, its score and justification, formatted clearly. You should write self-contained code starting with: ```java\n<code>```", "solution": "public @NonNull String summarize() {\n    var summary = new StringBuilder();\n    summary.append(\"Explanation of score (%s):\\n\".formatted(score));\n    summary.append(\"    Constraint matches:\\n\");\n    \n    var constraintMatches = matches();\n    if (constraintMatches == null) {\n        throw new IllegalArgumentException(\n                \"The constraint matches must be non-null. \" +\n                \"Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\");\n    }\n    \n    if (constraintMatches.isEmpty()) {\n        summary.append(\"        %s: constraint (%s) has no matches.\\n\"\n                .formatted(score().toShortString(), constraintRef().constraintName()));\n    } else {\n        summary.append(\"        %s: constraint (%s) has %s matches:\\n\"\n                .formatted(score().toShortString(), constraintRef().constraintName(), constraintMatches.size()));\n        for (var match : constraintMatches) {\n            summary.append(\"            %s: justified with (%s)\\n\"\n                    .formatted(match.score().toShortString(), match.justification()));\n        }\n    }\n    \n    return summary.toString();\n}", "test": "@Test\nvoid summarizeUninitializedSolution() {\n    var constraintPackage = \"constraintPackage\";\n    var constraintName1 = \"constraint1\";\n    var constraintName2 = \"constraint2\";\n    var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n    var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n    var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n    var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n    var constraintAnalysisMap = Map.of(\n            constraintMatchTotal.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n            constraintMatchTotal2.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n    // Single constraint analysis\n    var constraintSummary = constraintAnalysisMap.get(constraintMatchTotal.getConstraintRef()).summarize();\n    assertThat(constraintSummary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                            0: constraint (constraint1) has no matches.\n                    \"\"\");\n\n    // Complete score analysis\n    var summary = scoreAnalysis.summarize();\n    assertThat(scoreAnalysis.getConstraintAnalysis(constraintName1).matchCount()).isZero();\n    assertThat(summary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                            0: constraint (constraint1) has no matches.\n                            0: constraint (constraint2) has no matches.\n                    \"\"\");\n}"}, {"turn_num": 2, "instruction": "Modify the method to sort the constraint matches by score in ascending order before displaying them. Limit the number of displayed matches to a default maximum (e.g., 5) and append an ellipsis if there are more matches than the limit.", "solution": "public @NonNull String summarize() {\n    var summary = new StringBuilder();\n    summary.append(\"Explanation of score (%s):\\n\".formatted(score));\n    summary.append(\"    Constraint matches:\\n\");\n    \n    var constraintMatches = matches();\n    if (constraintMatches == null) {\n        throw new IllegalArgumentException(\n                \"The constraint matches must be non-null. \" +\n                \"Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\");\n    }\n    \n    if (constraintMatches.isEmpty()) {\n        summary.append(\"        %s: constraint (%s) has no matches.\\n\"\n                .formatted(score().toShortString(), constraintRef().constraintName()));\n    } else {\n        summary.append(\"        %s: constraint (%s) has %s matches:\\n\"\n                .formatted(score().toShortString(), constraintRef().constraintName(), constraintMatches.size()));\n        \n        Comparator<MatchAnalysis<Score_>> matchScoreComparator = comparing(MatchAnalysis::score);\n        constraintMatches.stream()\n                .sorted(matchScoreComparator)\n                .limit(5)\n                .forEach(match -> summary.append(\"            %s: justified with (%s)\\n\"\n                        .formatted(match.score().toShortString(), match.justification())));\n        \n        if (constraintMatches.size() > 5) {\n            summary.append(\"            ...\\n\");\n        }\n    }\n    \n    return summary.toString();\n}", "test": "@Test\nvoid summarizeUninitializedSolution() {\n    var constraintPackage = \"constraintPackage\";\n    var constraintName1 = \"constraint1\";\n    var constraintName2 = \"constraint2\";\n    var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n    var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n    var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n    var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n    var constraintAnalysisMap = Map.of(\n            constraintMatchTotal.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n            constraintMatchTotal2.getConstraintRef(),\n            getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n    var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n    // Single constraint analysis\n    var constraintSummary = constraintAnalysisMap.get(constraintMatchTotal.getConstraintRef()).summarize();\n    assertThat(constraintSummary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                            0: constraint (constraint1) has no matches.\n                    \"\"\");\n\n    // Complete score analysis\n    var summary = scoreAnalysis.summarize();\n    assertThat(scoreAnalysis.getConstraintAnalysis(constraintName1).matchCount()).isZero();\n    assertThat(summary)\n            .isEqualTo(\"\"\"\n                    Explanation of score (0):\n                        Constraint matches:\n                            0: constraint (constraint1) has no matches.\n                            0: constraint (constraint2) has no matches.\n                    \"\"\");\n}"}, {"turn_num": 3, "instruction": "Handle the case where there are no constraint matches by explicitly stating that the constraint has no matches, including the score and constraint name. Throw an IllegalArgumentException if the constraint matches are null, with a message indicating that the data must be non-null and suggesting the use of ScoreAnalysisFetchPolicy.FETCH_ALL to obtain it.", "solution": "public @NonNull String summarize() {\n    var summary = new StringBuilder();\n    summary.append(\"Explanation of score (%s):\\n\".formatted(score));\n    summary.append(\"    Constraint matches:\\n\");\n    \n    var constraintMatches = matches();\n    if (constraintMatches == null) {\n        throw new IllegalArgumentException(\n                \"The constraint matches must be non-null. \" +\n                \"Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\");\n    }\n    \n    if (constraintMatches.isEmpty()) {\n        summary.append(\"        %s: constraint (%s) has no matches.\\n\"\n                .formatted(score().toShortString(), constraintRef().constraintName()));\n    } else {\n        summary.append(\"        %s: constraint (%s) has %s matches:\\n\"\n                .formatted(score().toShortString(), constraintRef().constraintName(), constraintMatches.size()));\n        \n        Comparator<MatchAnalysis<Score_>> matchScoreComparator = comparing(MatchAnalysis::score);\n        constraintMatches.stream()\n                .sorted(matchScoreComparator)\n                .limit(5)\n                .forEach(match -> summary.append(\"            %s: justified with (%s)\\n\"\n                        .formatted(match.score().toShortString(), match.justification())));\n        \n        if (constraintMatches.size() > 5) {\n            summary.append(\"            ...\\n\");\n        }\n    }\n    \n    return summary.toString();\n}", "test": "@Test\nvoid summarizeUninitializedSolution() {\n    var constraintPackage = \"constraintPackage\";\n    var constraintName1 = \"constraint1\";\n    var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n\n    // Test IllegalArgumentException for null constraint matches\n    var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n    var constraintAnalysisWithNullMatches = getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_SHALLOW);\n    \n    var exception = assertThatThrownBy(() -> constraintAnalysisWithNullMatches.summarize())\n            .isInstanceOf(IllegalArgumentException.class);\n    \n    exception.hasMessage(\"The constraint matches must be non-null. Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\");\n\n    // Test format for empty constraint matches with score and constraint name\n    var constraintAnalysisWithEmptyMatches = getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL);\n    var constraintSummary = constraintAnalysisWithEmptyMatches.summarize();\n    \n    assertThat(constraintSummary).contains(\"0: constraint (constraint1) has no matches\");\n}"}], "metadata": {"source": "mrgbench", "func": "    @SuppressWarnings(\"java:S3457\")\n    public @NonNull String summarize() {\n        var summary = new StringBuilder();\n        summary.append(\"\"\"\n                Explanation of score (%s):\n                    Constraint matches:\n                \"\"\".formatted(score));\n        Comparator<MatchAnalysis<Score_>> matchScoreComparator = comparing(MatchAnalysis::score);\n\n        var constraintMatches = matches();\n        if (constraintMatches == null) {\n            throw new IllegalArgumentException(\"\"\"\n                    The constraint matches must be non-null.\n                    Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\n                    \"\"\");\n        }\n        if (constraintMatches.isEmpty()) {\n            summary.append(\n                    \"%8s%s: constraint (%s) has no matches.\\n\".formatted(\" \", score().toShortString(),\n                            constraintRef().constraintName()));\n        } else {\n            summary.append(\"%8s%s: constraint (%s) has %s matches:\\n\".formatted(\" \", score().toShortString(),\n                    constraintRef().constraintName(), constraintMatches.size()));\n        }\n        constraintMatches.stream()\n                .sorted(matchScoreComparator)\n                .limit(DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT)\n                .forEach(match -> summary.append(\"%12S%s: justified with (%s)\\n\".formatted(\" \", match.score().toShortString(),\n                        match.justification())));\n        if (constraintMatches.size() > DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT) {\n            summary.append(\"%12s%s\\n\".formatted(\" \", \"...\"));\n        }\n\n        return summary.toString();\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/api/score/analysis/ConstraintAnalysis.java", "func_name": "summarize", "context": "package ai.timefold.solver.core.api.score.analysis;\n\nimport static ai.timefold.solver.core.api.score.analysis.ScoreAnalysis.DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT;\nimport static java.util.Comparator.comparing;\n\nimport java.util.Comparator;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Stream;\n\nimport ai.timefold.solver.core.api.score.Score;\nimport ai.timefold.solver.core.api.score.calculator.ConstraintMatchAwareIncrementalScoreCalculator;\nimport ai.timefold.solver.core.api.score.constraint.ConstraintRef;\nimport ai.timefold.solver.core.api.score.stream.ConstraintJustification;\nimport ai.timefold.solver.core.api.solver.SolutionManager;\nimport ai.timefold.solver.core.impl.score.constraint.DefaultConstraintMatchTotal;\nimport ai.timefold.solver.core.impl.util.CollectionUtils;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * Note: Users should never create instances of this type directly.\n * It is available transitively via {@link SolutionManager#analyze(Object)}.\n *\n * @param <Score_>\n * @param matches null if analysis not available;\n *        empty if constraint has no matches, but still non-zero constraint weight;\n *        non-empty if constraint has matches.\n *        This is a {@link List} to simplify access to individual elements,\n *        but it contains no duplicates just like {@link HashSet} wouldn't.\n * @param matchCount\n *        <ul>\n *        <li>For regular constraint analysis:\n *        -1 if analysis not available,\n *        0 if constraint has no matches,\n *        positive if constraint has matches.\n *        Equal to the size of the {@link #matches} list.</li>\n *        <li>For a {@link ScoreAnalysis#diff(ScoreAnalysis) diff of constraint analyses}:\n *        positive if the constraint has more matches in the new analysis,\n *        zero if the number of matches is the same in both,\n *        negative otherwise.\n *        Need not be equal to the size of the {@link #matches} list.</li>\n *        </ul>\n */\npublic record ConstraintAnalysis<Score_ extends Score<Score_>>(@NonNull ConstraintRef constraintRef, @NonNull Score_ weight,\n        @NonNull Score_ score, @Nullable List<MatchAnalysis<Score_>> matches, int matchCount) {\n\n    public ConstraintAnalysis(@NonNull ConstraintRef constraintRef, @NonNull Score_ weight, @NonNull Score_ score,\n            @Nullable List<MatchAnalysis<Score_>> matches) {\n        this(constraintRef, weight, score, matches, matches == null ? -1 : matches.size());\n    }\n\n    public ConstraintAnalysis {\n        Objects.requireNonNull(constraintRef);\n        /*\n         * Null only possible in ConstraintMatchAwareIncrementalScoreCalculator and/or tests.\n         * Easy doesn't support constraint analysis at all.\n         * CS always provides constraint weights.\n         */\n        Objects.requireNonNull(weight, () -> \"\"\"\n                The constraint weight must be non-null.\n                Maybe use a non-deprecated %s constructor in your %s implementation?\"\"\"\n                .formatted(DefaultConstraintMatchTotal.class.getSimpleName(),\n                        ConstraintMatchAwareIncrementalScoreCalculator.class.getSimpleName()));\n        Objects.requireNonNull(score);\n    }\n\n    @NonNull\n    ConstraintAnalysis<Score_> negate() {\n        // Only used to compute diff; use semantics for non-diff.\n        // A negative match count is only allowed within these semantics when matches == null.\n        if (matches == null) {\n            // At this point, matchCount is already negative, as matches == null.\n            return new ConstraintAnalysis<>(constraintRef, weight.negate(), score.negate(), null, matchCount);\n        } else {\n            // Within these semantics, match count == list size.\n            var negatedMatchAnalysesList = matches.stream()\n                    .map(MatchAnalysis::negate)\n                    .toList();\n            return new ConstraintAnalysis<>(constraintRef, weight.negate(), score.negate(), negatedMatchAnalysesList,\n                    matchCount);\n        }\n    }\n\n    static <Score_ extends Score<Score_>> @NonNull ConstraintAnalysis<Score_> diff(\n            @NonNull ConstraintRef constraintRef, @Nullable ConstraintAnalysis<Score_> constraintAnalysis,\n            @Nullable ConstraintAnalysis<Score_> otherConstraintAnalysis) {\n        if (constraintAnalysis == null) {\n            if (otherConstraintAnalysis == null) {\n                throw new IllegalStateException(\n                        \"Impossible state: none of the score explanations provided constraint matches for a constraint (%s).\"\n                                .formatted(constraintRef));\n            }\n            // No need to compute diff; this constraint is not present in this score explanation.\n            return otherConstraintAnalysis.negate();\n        } else if (otherConstraintAnalysis == null) {\n            // No need to compute diff; this constraint is not present in the other score explanation.\n            return constraintAnalysis;\n        }\n        var matchAnalyses = constraintAnalysis.matches();\n        var otherMatchAnalyses = otherConstraintAnalysis.matches();\n        if ((matchAnalyses == null && otherMatchAnalyses != null) || (matchAnalyses != null && otherMatchAnalyses == null)) {\n            throw new IllegalStateException(\n                    \"Impossible state: One of the score analyses (%s, %s) provided no match analysis for a constraint (%s).\"\n                            .formatted(constraintAnalysis, otherConstraintAnalysis, constraintRef));\n        }\n        // Compute the diff.\n        var constraintWeightDifference = constraintAnalysis.weight().subtract(otherConstraintAnalysis.weight());\n        var scoreDifference = constraintAnalysis.score().subtract(otherConstraintAnalysis.score());\n        if (matchAnalyses == null) {\n            var leftHasMatchCount = hasMatchCount(constraintAnalysis);\n            var rightHasMatchCount = hasMatchCount(otherConstraintAnalysis);\n            if ((!leftHasMatchCount && rightHasMatchCount) || (leftHasMatchCount && !rightHasMatchCount)) {\n                throw new IllegalStateException(\n                        \"Impossible state: One of the score analyses (%s, %s) provided no match count for a constraint (%s).\"\n                                .formatted(constraintAnalysis, otherConstraintAnalysis, constraintRef));\n            }\n            return new ConstraintAnalysis<>(constraintRef, constraintWeightDifference, scoreDifference, null,\n                    getMatchCount(constraintAnalysis, otherConstraintAnalysis));\n        }\n        var matchAnalysisMap = mapMatchesToJustifications(matchAnalyses);\n        var otherMatchAnalysisMap = mapMatchesToJustifications(otherMatchAnalyses);\n        var matchAnalysesList = Stream.concat(matchAnalysisMap.keySet().stream(), otherMatchAnalysisMap.keySet().stream())\n                .distinct()\n                .flatMap(justification -> {\n                    var matchAnalysis = matchAnalysisMap.get(justification);\n                    var otherMatchAnalysis = otherMatchAnalysisMap.get(justification);\n                    if (matchAnalysis == null) {\n                        if (otherMatchAnalysis == null) {\n                            throw new IllegalStateException(\n                                    \"Impossible state: none of the match analyses provided for a constraint (%s).\"\n                                            .formatted(constraintRef));\n                        }\n                        // No need to compute diff; this match is not present in this score explanation.\n                        return Stream.of(otherMatchAnalysis.negate());\n                    } else if (otherMatchAnalysis == null) {\n                        // No need to compute diff; this match is not present in the other score explanation.\n                        return Stream.of(matchAnalysis);\n                    } else if (!matchAnalysis.equals(otherMatchAnalysis)) { // Compute the diff.\n                        return Stream.of(new MatchAnalysis<>(constraintRef,\n                                matchAnalysis.score().subtract(otherMatchAnalysis.score()), justification));\n                    } else { // There is no difference; skip entirely.\n                        return Stream.empty();\n                    }\n                }).toList();\n        return new ConstraintAnalysis<>(constraintRef, constraintWeightDifference, scoreDifference, matchAnalysesList,\n                getMatchCount(constraintAnalysis, otherConstraintAnalysis));\n    }\n\n    private static boolean hasMatchCount(ConstraintAnalysis<?> analysis) {\n        return analysis.matchCount >= 0;\n    }\n\n    private static int getMatchCount(ConstraintAnalysis<?> analysis, ConstraintAnalysis<?> otherAnalysis) {\n        return analysis.matchCount() - otherAnalysis.matchCount();\n    }\n\n    private static <Score_ extends Score<Score_>> Map<ConstraintJustification, MatchAnalysis<Score_>>\n            mapMatchesToJustifications(List<MatchAnalysis<Score_>> matchAnalyses) {\n        Map<ConstraintJustification, MatchAnalysis<Score_>> matchAnalysisMap =\n                CollectionUtils.newLinkedHashMap(matchAnalyses.size());\n        for (var matchAnalysis : matchAnalyses) {\n            var previous = matchAnalysisMap.put(matchAnalysis.justification(), matchAnalysis);\n            if (previous != null) {\n                // Match analysis for the same justification should have been merged already.\n                throw new IllegalStateException(\n                        \"Impossible state: multiple constraint matches (%s, %s) have the same justification (%s).\"\n                                .formatted(previous, matchAnalysis, matchAnalysis.justification()));\n            }\n        }\n        return matchAnalysisMap;\n    }\n\n    /**\n     * Return package name of the constraint that this analysis is for.\n     *\n     * @return equal to {@code constraintRef.packageName()}\n     * @deprecated Do not rely on constraint package in user code.\n     */\n    @Deprecated(forRemoval = true, since = \"1.13.0\")\n    public String constraintPackage() {\n        return constraintRef.packageName();\n    }\n\n    /**\n     * Return name of the constraint that this analysis is for.\n     *\n     * @return equal to {@code constraintRef.constraintName()}\n     */\n    public @NonNull String constraintName() {\n        return constraintRef.constraintName();\n    }", "func_start": 201, "func_end": 235, "body_len": 34, "test_file": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java", "test_start": 110, "test_end": 146, "test_code": "    @Test\n    void summarizeUninitializedSolution() {\n        var constraintPackage = \"constraintPackage\";\n        var constraintName1 = \"constraint1\";\n        var constraintName2 = \"constraint2\";\n        var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n        var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n        var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n        var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n        var constraintAnalysisMap = Map.of(\n                constraintMatchTotal.getConstraintRef(),\n                getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n                constraintMatchTotal2.getConstraintRef(),\n                getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n        var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n        // Single constraint analysis\n        var constraintSummary = constraintAnalysisMap.get(constraintMatchTotal.getConstraintRef()).summarize();\n        assertThat(constraintSummary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                                0: constraint (constraint1) has no matches.\n                        \"\"\");\n\n        // Complete score analysis\n        var summary = scoreAnalysis.summarize();\n        assertThat(scoreAnalysis.getConstraintAnalysis(constraintName1).matchCount()).isZero();\n        assertThat(summary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                                0: constraint (constraint1) has no matches.\n                                0: constraint (constraint2) has no matches.\n                        \"\"\");\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.api.score.analysis.ScoreAnalysisTest#summarizeUninitializedSolution\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1066}
{"hash_id": "67d3cc15a7fcfae7fc274087", "total_turn": 4, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `format` that takes a double `number`, a non-null `DecimalFormat` object, and a non-null `Locale` object, and returns a formatted string representation of the number using the given DecimalFormat with its current settings. You should write self-contained code starting with: ```java\nprivate static @NonNull String format(double number, @NonNull DecimalFormat decimalFormat, @NonNull Locale locale) {\n}```", "solution": "private static @NonNull String format(double number, @NonNull DecimalFormat decimalFormat, @NonNull Locale locale) {\n    return decimalFormat.format(number);\n}", "test": "@Test\n@Timeout(60)\nvoid submitMoreProblemsThanCpus_allGetSolved() throws InterruptedException, ExecutionException {\n    // Use twice the amount of problems than available processors.\n    var problemCount = Runtime.getRuntime().availableProcessors() * 2;\n    try (var solverManager = createSolverManagerTestableByDifferentConsumers()) {\n        assertSolveWithoutConsumer(problemCount, solverManager);\n        assertSolveWithConsumer(problemCount, solverManager, true);\n        assertSolveWithConsumer(problemCount, solverManager, false);\n    }\n}"}, {"turn_num": 2, "instruction": "Ensure that if the input Locale is the same as the constant FORMATTER_LOCALE, the function directly formats the number without changing the DecimalFormat's symbols to avoid unnecessary operations.", "solution": "private static @NonNull String format(double number, @NonNull DecimalFormat decimalFormat, @NonNull Locale locale) {\n    if (locale.equals(FORMATTER_LOCALE)) {\n        return decimalFormat.format(number);\n    }\n    try { // Slow path for corner cases where input locale doesn't match the default locale.\n        decimalFormat.setDecimalFormatSymbols(DecimalFormatSymbols.getInstance(locale));\n        return decimalFormat.format(number);\n    } finally {\n        decimalFormat.setDecimalFormatSymbols(DecimalFormatSymbols.getInstance(FORMATTER_LOCALE));\n    }\n}", "test": "@Test\n@Timeout(60)\nvoid submitMoreProblemsThanCpus_allGetSolved() throws InterruptedException, ExecutionException {\n    // Use twice the amount of problems than available processors.\n    var problemCount = Runtime.getRuntime().availableProcessors() * 2;\n    try (var solverManager = createSolverManagerTestableByDifferentConsumers()) {\n        assertSolveWithoutConsumer(problemCount, solverManager);\n        assertSolveWithConsumer(problemCount, solverManager, true);\n        assertSolveWithConsumer(problemCount, solverManager, false);\n    }\n}"}, {"turn_num": 3, "instruction": "Modify the function to use DecimalFormatSymbols from the provided Locale when formatting the number, by setting the DecimalFormat's symbols to those of the input locale before formatting.", "solution": "private static @NonNull String format(double number, @NonNull DecimalFormat decimalFormat, @NonNull Locale locale) {\n    decimalFormat.setDecimalFormatSymbols(DecimalFormatSymbols.getInstance(locale));\n    return decimalFormat.format(number);\n}", "test": "@Test\n@Timeout(60)\nvoid submitMoreProblemsThanCpus_allGetSolved() throws InterruptedException, ExecutionException {\n    // Use twice the amount of problems than available processors.\n    var problemCount = Runtime.getRuntime().availableProcessors() * 2;\n    try (var solverManager = createSolverManagerTestableByDifferentConsumers()) {\n        assertSolveWithoutConsumer(problemCount, solverManager);\n        assertSolveWithConsumer(problemCount, solverManager, true);\n        assertSolveWithConsumer(problemCount, solverManager, false);\n    }\n}"}, {"turn_num": 4, "instruction": "After formatting the number, the function must restore the DecimalFormat's symbols to use those of FORMATTER_LOCALE, regardless of whether an exception occurs during formatting, using a try-finally block to ensure cleanup.", "solution": "private static @NonNull String format(double number, @NonNull DecimalFormat decimalFormat, @NonNull Locale locale) {\n    decimalFormat.setDecimalFormatSymbols(DecimalFormatSymbols.getInstance(locale));\n    try {\n        return decimalFormat.format(number);\n    } finally {\n        decimalFormat.setDecimalFormatSymbols(DecimalFormatSymbols.getInstance(FORMATTER_LOCALE));\n    }\n}", "test": "@Test\n@Timeout(60)\nvoid submitMoreProblemsThanCpus_allGetSolved() throws InterruptedException, ExecutionException {\n    // Use twice the amount of problems than available processors.\n    var problemCount = Runtime.getRuntime().availableProcessors() * 2;\n    try (var solverManager = createSolverManagerTestableByDifferentConsumers()) {\n        assertSolveWithoutConsumer(problemCount, solverManager);\n        assertSolveWithConsumer(problemCount, solverManager, true);\n        assertSolveWithConsumer(problemCount, solverManager, false);\n    }\n}"}], "metadata": {"source": "mrgbench", "func": "    private static @NonNull String format(double number, @NonNull DecimalFormat decimalFormat, @NonNull Locale locale) {\n        if (locale.equals(FORMATTER_LOCALE)) {\n            return decimalFormat.format(number);\n        }\n        try { // Slow path for corner cases where input locale doesn't match the default locale.\n            decimalFormat.setDecimalFormatSymbols(DecimalFormatSymbols.getInstance(locale));\n            return decimalFormat.format(number);\n        } finally {\n            decimalFormat.setDecimalFormatSymbols(DecimalFormatSymbols.getInstance(FORMATTER_LOCALE));\n        }\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/api/solver/ProblemSizeStatistics.java", "func_name": "format", "context": "package ai.timefold.solver.core.api.solver;\n\nimport java.text.DecimalFormat;\nimport java.text.DecimalFormatSymbols;\nimport java.util.Locale;\n\nimport ai.timefold.solver.core.impl.util.MathUtils;\n\nimport org.jspecify.annotations.NonNull;\n\n/**\n * The statistics of a given problem submitted to a {@link Solver}.\n *\n * @param entityCount The number of genuine entities defined by the problem.\n * @param variableCount The number of genuine variables defined by the problem.\n * @param approximateValueCount The estimated number of values defined by the problem.\n *        Can be larger than the actual value count.\n * @param approximateProblemSizeLog The estimated log_10 of the problem's search space size.\n */\npublic record ProblemSizeStatistics(long entityCount,\n        long variableCount,\n        long approximateValueCount,\n        double approximateProblemSizeLog) {\n\n    private static final Locale FORMATTER_LOCALE = Locale.getDefault();\n    private static final DecimalFormat BASIC_FORMATTER = new DecimalFormat(\"#,###\");\n\n    // Exponent should not use grouping, unlike basic\n    private static final DecimalFormat EXPONENT_FORMATTER = new DecimalFormat(\"#\");\n    private static final DecimalFormat SIGNIFICANT_FIGURE_FORMATTER = new DecimalFormat(\"0.######\");\n\n    /**\n     * Return the {@link #approximateProblemSizeLog} as a fixed point integer.\n     */\n    public long approximateProblemScaleLogAsFixedPointLong() {\n        return Math.round(approximateProblemSizeLog * MathUtils.LOG_PRECISION);\n    }\n\n    public String approximateProblemScaleAsFormattedString() {\n        return approximateProblemScaleAsFormattedString(Locale.getDefault());\n    }\n\n    String approximateProblemScaleAsFormattedString(Locale locale) {\n        if (Double.isNaN(approximateProblemSizeLog) || Double.isInfinite(approximateProblemSizeLog)) {\n            return \"0\";\n        }\n\n        if (approximateProblemSizeLog < 10) { // log_10(10_000_000_000) = 10\n            return \"%s\".formatted(format(Math.pow(10d, approximateProblemSizeLog), BASIC_FORMATTER, locale));\n        }\n        // The actual number will often be too large to fit in a double, so cannot use normal\n        // formatting.\n        // Separate the exponent into its integral and fractional parts\n        // Use the integral part as the power of 10, and the fractional part as the significant digits.\n        double exponentPart = Math.floor(approximateProblemSizeLog);\n        double remainderPartAsExponent = approximateProblemSizeLog - exponentPart;\n        double remainderPart = Math.pow(10, remainderPartAsExponent);\n        return \"%s × 10^%s\".formatted(\n                format(remainderPart, SIGNIFICANT_FIGURE_FORMATTER, locale),\n                format(exponentPart, EXPONENT_FORMATTER, locale));\n    }", "func_start": 72, "func_end": 82, "body_len": 10, "test_file": "core/src/test/java/ai/timefold/solver/core/api/solver/SolverManagerTest.java", "test_start": 934, "test_end": 944, "test_code": "    @Test\n    @Timeout(60)\n    void submitMoreProblemsThanCpus_allGetSolved() throws InterruptedException, ExecutionException {\n        // Use twice the amount of problems than available processors.\n        var problemCount = Runtime.getRuntime().availableProcessors() * 2;\n        try (var solverManager = createSolverManagerTestableByDifferentConsumers()) {\n            assertSolveWithoutConsumer(problemCount, solverManager);\n            assertSolveWithConsumer(problemCount, solverManager, true);\n            assertSolveWithConsumer(problemCount, solverManager, false);\n        }\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.api.solver.SolverManagerTest#submitMoreProblemsThanCpus_allGetSolved\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1067}
{"hash_id": "6a7812356745fb480384dc14", "total_turn": 2, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `meldProperty` that takes two nullable objects of the same type and merges them: if both are null, return null; if one is null, return the other; if both are non-null, return the first one. The function should be generic to work with any type. You should write self-contained code starting with: ```java\n<T> T meldProperty(T a, T b)```", "solution": "public static <T> @Nullable T meldProperty(@Nullable T a, @Nullable T b) {\n    if (a == null && b == null) {\n        return null;\n    }\n    if (a == null) {\n        return b;\n    }\n    if (b == null) {\n        return a;\n    }\n    return a;\n}", "test": "@Test\nvoid meldProperty() {\n    assertThat(ConfigUtils.<Integer> meldProperty(null, null)).isNull();\n    assertThat(ConfigUtils.<Integer> meldProperty(1, null)).isEqualTo(1);\n    assertThat(ConfigUtils.<Integer> meldProperty(null, 10)).isEqualTo(10);\n    assertThat(ConfigUtils.<Integer> meldProperty(1, 10)).isEqualTo(1);\n}"}, {"turn_num": 2, "instruction": "When both inputs are non-null, instead of returning the first one, delegate the merging to another method `mergeProperty` to combine the two values. Add proper nullability annotations to indicate that the parameters and return value can be null.", "solution": "public static <T> @Nullable T meldProperty(@Nullable T a, @Nullable T b) {\n    if (a == null && b == null) {\n        return null;\n    }\n    if (a == null) {\n        return b;\n    }\n    if (b == null) {\n        return a;\n    }\n    return mergeProperty(a, b);\n}", "test": "@Test\nvoid meldProperty() {\n    assertThat(ConfigUtils.<Integer> meldProperty(null, null)).isNull();\n    assertThat(ConfigUtils.<Integer> meldProperty(1, null)).isEqualTo(1);\n    assertThat(ConfigUtils.<Integer> meldProperty(null, 10)).isEqualTo(10);\n    assertThat(ConfigUtils.<Integer> meldProperty(1, 10)).isEqualTo(ConfigUtils.mergeProperty(1, 10));\n}"}], "metadata": {"source": "mrgbench", "func": "    public static <T> @Nullable T meldProperty(@Nullable T a, @Nullable T b) {\n        if (a == null && b == null) {\n            return null;\n        }\n        if (a == null) {\n            return b;\n        }\n        if (b == null) {\n            return a;\n        }\n        return mergeProperty(a, b);\n    }", "repo": "timefold-solver", "file_path": "core/src/main/java/ai/timefold/solver/core/config/util/ConfigUtils.java", "func_name": "meldProperty", "context": "package ai.timefold.solver.core.config.util;\n\nimport static ai.timefold.solver.core.impl.domain.common.accessor.MemberAccessorFactory.MemberAccessorType.FIELD_OR_READ_METHOD;\nimport static ai.timefold.solver.core.impl.domain.solution.cloner.DeepCloningUtils.IMMUTABLE_CLASSES;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.AnnotatedElement;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Modifier;\nimport java.lang.reflect.ParameterizedType;\nimport java.lang.reflect.Type;\nimport java.lang.reflect.WildcardType;\nimport java.math.BigDecimal;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.EnumSet;\nimport java.util.LinkedHashMap;\nimport java.util.LinkedHashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.function.Supplier;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport ai.timefold.solver.core.api.domain.common.DomainAccessType;\nimport ai.timefold.solver.core.api.domain.lookup.PlanningId;\nimport ai.timefold.solver.core.config.AbstractConfig;\nimport ai.timefold.solver.core.impl.domain.common.AlphabeticMemberComparator;\nimport ai.timefold.solver.core.impl.domain.common.ReflectionHelper;\nimport ai.timefold.solver.core.impl.domain.common.accessor.MemberAccessor;\nimport ai.timefold.solver.core.impl.domain.common.accessor.MemberAccessorFactory;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\npublic class ConfigUtils {\n\n    private static final AlphabeticMemberComparator alphabeticMemberComparator = new AlphabeticMemberComparator();\n\n    /**\n     * Create a new instance of clazz from a config's property.\n     * <p>\n     * If the instantiation fails, the simple class name of {@code configBean} will be used as the owner of\n     * {@code propertyName}.\n     * <p>\n     * Intended usage:\n     *\n     * <pre>\n     * selectionFilter = ConfigUtils.newInstance(config, \"filterClass\", config.getFilterClass());\n     * </pre>\n     *\n     * @param configBean the bean holding the {@code clazz} to be instantiated\n     * @param propertyName {@code configBean}'s property holding {@code clazz}\n     * @param clazz {@code Class} representation of the type {@code T}\n     * @param <T> the new instance type\n     * @return new instance of clazz\n     */\n    public static <T> @NonNull T newInstance(@Nullable Object configBean, @NonNull String propertyName,\n            @NonNull Class<T> clazz) {\n        return newInstance(() -> (configBean == null ? \"?\" : configBean.getClass().getSimpleName()), propertyName, clazz);\n    }\n\n    /**\n     * Create a new instance of clazz from a general source.\n     * <p>\n     * If the instantiation fails, the result of {@code ownerDescriptor} will be used to describe the owner of\n     * {@code propertyName}.\n     *\n     * @param ownerDescriptor describes the owner of {@code propertyName}\n     * @param propertyName property holding the {@code clazz}\n     * @param clazz {@code Class} representation of the type {@code T}\n     * @param <T> the new instance type\n     * @return new instance of clazz\n     */\n    public static <T> @NonNull T newInstance(@NonNull Supplier<String> ownerDescriptor, @NonNull String propertyName,\n            @NonNull Class<T> clazz) {\n        try {\n            return clazz.getDeclaredConstructor().newInstance();\n        } catch (NoSuchMethodException | InstantiationException | IllegalAccessException | InvocationTargetException e) {\n            // Inner classes include local, anonymous and non-static member classes\n            throw new IllegalArgumentException(\"The %s's %s (%s) does not have a public no-arg constructor%s\"\n                    .formatted(ownerDescriptor.get(), propertyName, clazz.getName(),\n                            ((clazz.isLocalClass() || clazz.isAnonymousClass() || clazz.isMemberClass())\n                                    && !Modifier.isStatic(clazz.getModifiers()) ? \" because it is an inner class.\" : \".\")),\n                    e);\n        }\n    }\n\n    public static void applyCustomProperties(@NonNull Object bean, @NonNull String beanClassPropertyName,\n            @Nullable Map<@NonNull String, @NonNull String> customProperties, @NonNull String customPropertiesPropertyName) {\n        if (customProperties == null) {\n            return;\n        }\n        var beanClass = bean.getClass();\n        customProperties.forEach((propertyName, valueString) -> {\n            var setterMethod = ReflectionHelper.getSetterMethod(beanClass, propertyName);\n            if (setterMethod == null) {\n                throw new IllegalStateException(\n                        \"\"\"\n                                The custom property %s (%s) in the %s cannot be set on the %s (%s) because that class has no public setter for that property.\n                                Maybe add a public setter for that custom property (%s) on that class (%s).\n                                Maybe don't configure that custom property %s (%s) in the %s.\"\"\"\n                                .formatted(propertyName, valueString, customPropertiesPropertyName, beanClassPropertyName,\n                                        beanClass, propertyName, beanClass.getSimpleName(), propertyName, valueString,\n                                        customPropertiesPropertyName));\n            }\n            var propertyType = setterMethod.getParameterTypes()[0];\n            Object typedValue;\n            try {\n                if (propertyType.equals(String.class)) {\n                    typedValue = valueString;\n                } else if (propertyType.equals(Boolean.TYPE) || propertyType.equals(Boolean.class)) {\n                    typedValue = Boolean.parseBoolean(valueString);\n                } else if (propertyType.equals(Integer.TYPE) || propertyType.equals(Integer.class)) {\n                    typedValue = Integer.parseInt(valueString);\n                } else if (propertyType.equals(Long.TYPE) || propertyType.equals(Long.class)) {\n                    typedValue = Long.parseLong(valueString);\n                } else if (propertyType.equals(Float.TYPE) || propertyType.equals(Float.class)) {\n                    typedValue = Float.parseFloat(valueString);\n                } else if (propertyType.equals(Double.TYPE) || propertyType.equals(Double.class)) {\n                    typedValue = Double.parseDouble(valueString);\n                } else if (propertyType.equals(BigDecimal.class)) {\n                    typedValue = new BigDecimal(valueString);\n                } else if (propertyType.isEnum()) {\n                    typedValue = Enum.valueOf((Class<? extends Enum>) propertyType, valueString);\n                } else {\n                    throw new IllegalStateException(\n                            \"The custom property %s (%s) in the %s has an unsupported propertyType (%s) for value (%s).\"\n                                    .formatted(propertyName, valueString, customPropertiesPropertyName, propertyType,\n                                            valueString));\n                }\n            } catch (NumberFormatException e) {\n                throw new IllegalStateException(\n                        \"The custom property %s (%s) in the %s cannot be parsed to the propertyType (%s) of the setterMethod (%s).\"\n                                .formatted(propertyName, valueString, customPropertiesPropertyName, propertyType,\n                                        setterMethod));\n            }\n            try {\n                setterMethod.invoke(bean, typedValue);\n            } catch (IllegalAccessException e) {\n                throw new IllegalStateException(\n                        \"The custom property %s (%s) in the %s has a setterMethod (%s) on the beanClass (%s) that cannot be called for the typedValue (%s).\"\n                                .formatted(propertyName, valueString, customPropertiesPropertyName, setterMethod, beanClass,\n                                        typedValue),\n                        e);\n            } catch (InvocationTargetException e) {\n                throw new IllegalStateException(\n                        \"The custom property %s (%s) in the %s has a setterMethod (%s) on the beanClass (%s) that throws an exception for the typedValue (%s).\"\n                                .formatted(propertyName, valueString, customPropertiesPropertyName, setterMethod, beanClass,\n                                        typedValue),\n                        e.getCause());\n            }\n        });\n    }\n\n    public static <Config_ extends AbstractConfig<Config_>> @Nullable Config_ inheritConfig(@Nullable Config_ original,\n            @Nullable Config_ inherited) {\n        if (inherited != null) {\n            if (original == null) {\n                original = inherited.copyConfig();\n            } else {\n                original.inherit(inherited);\n            }\n        }\n        return original;\n    }\n\n    public static <Config_ extends AbstractConfig<Config_>> @Nullable List<Config_> inheritMergeableListConfig(\n            @Nullable List<Config_> originalList, @Nullable List<Config_> inheritedList) {\n        if (inheritedList != null) {\n            List<Config_> mergedList = new ArrayList<>(inheritedList.size()\n                    + (originalList == null ? 0 : originalList.size()));\n            // The inheritedList should be before the originalList\n            for (var inherited : inheritedList) {\n                var copy = inherited.copyConfig();\n                mergedList.add(copy);\n            }\n            if (originalList != null) {\n                mergedList.addAll(originalList);\n            }\n            originalList = mergedList;\n        }\n        return originalList;\n    }\n\n    public static <T> @Nullable T inheritOverwritableProperty(@Nullable T original, @Nullable T inherited) {\n        if (original != null) {\n            // Original overwrites inherited\n            return original;\n        } else {\n            return inherited;\n        }\n    }\n\n    public static <T> @Nullable List<T> inheritMergeableListProperty(@Nullable List<T> originalList,\n            @Nullable List<T> inheritedList) {\n        if (inheritedList == null) {\n            return originalList;\n        } else if (originalList == null) {\n            // Shallow clone due to modifications after calling inherit\n            return new ArrayList<>(inheritedList);\n        } else {\n            // The inheritedList should be before the originalList\n            List<T> mergedList = new ArrayList<>(inheritedList);\n            mergedList.addAll(originalList);\n            return mergedList;\n        }\n    }\n\n    public static <E extends Enum<E>> @Nullable Set<E> inheritMergeableEnumSetProperty(@Nullable Set<E> originalSet,\n            @Nullable Set<E> inheritedSet) {\n        if (inheritedSet == null) {\n            return originalSet;\n        } else if (originalSet == null) {\n            return EnumSet.copyOf(inheritedSet);\n        } else {\n            var newSet = EnumSet.copyOf(originalSet);\n            newSet.addAll(inheritedSet);\n            return newSet;\n        }\n    }\n\n    public static <T> @Nullable List<T> inheritUniqueMergeableListProperty(@Nullable List<T> originalList,\n            @Nullable List<T> inheritedList) {\n        if (inheritedList == null) {\n            return originalList;\n        } else if (originalList == null) {\n            // Shallow clone due to modifications after calling inherit\n            return new ArrayList<>(inheritedList);\n        } else {\n            // The inheritedMap should be before the originalMap\n            Set<T> mergedSet = new LinkedHashSet<>(inheritedList);\n            mergedSet.addAll(originalList);\n            return new ArrayList<>(mergedSet);\n        }\n    }\n\n    public static <K, T> @Nullable Map<K, T> inheritMergeableMapProperty(@Nullable Map<K, T> originalMap,\n            @Nullable Map<K, T> inheritedMap) {\n        if (inheritedMap == null) {\n            return originalMap;\n        } else if (originalMap == null) {\n            return inheritedMap;\n        } else {\n            Map<K, T> mergedMap = new LinkedHashMap<>(inheritedMap);\n            mergedMap.putAll(originalMap);\n            return mergedMap;\n        }\n    }\n\n    public static <T> @Nullable T mergeProperty(@Nullable T a, @Nullable T b) {\n        return Objects.equals(a, b) ? a : null;\n    }", "func_start": 284, "func_end": 295, "body_len": 11, "test_file": "core/src/test/java/ai/timefold/solver/core/config/util/ConfigUtilsTest.java", "test_start": 31, "test_end": 37, "test_code": "    @Test\n    void meldProperty() {\n        assertThat(ConfigUtils.<Integer> meldProperty(null, null)).isNull();\n        assertThat(ConfigUtils.<Integer> meldProperty(1, null)).isEqualTo(1);\n        assertThat(ConfigUtils.<Integer> meldProperty(null, 10)).isEqualTo(10);\n        assertThat(ConfigUtils.<Integer> meldProperty(1, 10)).isEqualTo(ConfigUtils.mergeProperty(1, 10));\n    }", "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.config.util.ConfigUtilsTest#meldProperty\" -Dspotless.skip=true", "language": "java"}, "mt_id": 1068}
{"hash_id": "9f6ab586a4a1eda4f1828d39", "total_turn": 3, "turn_datas": [{"turn_num": 1, "instruction": "Write a function `camelToUnderline(String string)` that converts a camelCase string into a lowercase underscore-separated string by inserting an underscore before each uppercase letter and then converting the whole string to lowercase. If the input is null or empty, return an empty string. You should write self-contained code starting with: ```java\npublic static String camelToUnderline(String string) {```", "solution": "public static String camelToUnderline(String string) {\n    if (string == null || string.isEmpty()) {\n        return \"\";\n    }\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < string.length(); i++) {\n        char c = string.charAt(i);\n        if (Character.isUpperCase(c) && i > 0) {\n            sb.append('_');\n        }\n        sb.append(Character.toLowerCase(c));\n    }\n    return sb.toString();\n}", "test": "@Test\npublic void underlineToCamel() {\n    // Test basic camelCase conversion\n    String camelCase = \"aaBb\";\n    String result = StringUtil.camelToUnderline(camelCase);\n    Assert.assertEquals(\"aa_bb\", result);\n    \n    // Test null input\n    Assert.assertEquals(\"\", StringUtil.camelToUnderline(null));\n    \n    // Test empty input\n    Assert.assertEquals(\"\", StringUtil.camelToUnderline(\"\"));\n    \n    // Test single character\n    Assert.assertEquals(\"a\", StringUtil.camelToUnderline(\"a\"));\n    Assert.assertEquals(\"a\", StringUtil.camelToUnderline(\"A\"));\n}"}, {"turn_num": 2, "instruction": "Do not insert an underscore at the beginning of the string, even if the first character is uppercase. Only insert an underscore before an uppercase letter if the previous character is a lowercase letter, not another uppercase letter or an underscore.", "solution": "public static String camelToUnderline(String string) {\n    if (string == null || string.isEmpty()) {\n        return \"\";\n    }\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < string.length(); i++) {\n        char c = string.charAt(i);\n        if (Character.isUpperCase(c) && i > 0) {\n            char prev = string.charAt(i - 1);\n            if (Character.isLowerCase(prev)) {\n                sb.append('_');\n            }\n        }\n        sb.append(Character.toLowerCase(c));\n    }\n    return sb.toString();\n}", "test": "@Test\npublic void underlineToCamel() {\n    // Test basic camelCase conversion\n    String camelCase = \"aaBb\";\n    String result = StringUtil.camelToUnderline(camelCase);\n    Assert.assertEquals(\"aa_bb\", result);\n    \n    // Test consecutive uppercase letters\n    String consecutiveUpper = \"XMLParser\";\n    String result2 = StringUtil.camelToUnderline(consecutiveUpper);\n    Assert.assertEquals(\"xmlparser\", result2);\n    \n    // Test starting with uppercase\n    String startUpper = \"TestCase\";\n    String result3 = StringUtil.camelToUnderline(startUpper);\n    Assert.assertEquals(\"test_case\", result3);\n}"}, {"turn_num": 3, "instruction": "Ensure that consecutive uppercase letters are treated as part of the same word unless preceded by a lowercase letter, so no underscore is inserted between them.", "solution": "public static String camelToUnderline(String string) {\n    if (string == null || string.isEmpty()) {\n        return \"\";\n    }\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < string.length(); i++) {\n        char c = string.charAt(i);\n        if (Character.isUpperCase(c) && i > 0) {\n            char prev = string.charAt(i - 1);\n            // Insert underscore if previous char is lowercase\n            if (Character.isLowerCase(prev)) {\n                sb.append('_');\n            }\n            // Insert underscore if previous char is uppercase and current char starts a new word\n            // This happens when we have consecutive uppercase letters followed by a mixed case word\n            else if (Character.isUpperCase(prev) && i < string.length() - 1) {\n                char next = string.charAt(i + 1);\n                if (Character.isLowerCase(next)) {\n                    sb.append('_');\n                }\n            }\n        }\n        sb.append(Character.toLowerCase(c));\n    }\n    return sb.toString();\n}", "test": "@Test\npublic void underlineToCamel() {\n    // Test consecutive uppercase letters - should stay together\n    String testConsecutiveUpper = \"XMLParser\";\n    String result = StringUtil.camelToUnderline(testConsecutiveUpper);\n    Assert.assertEquals(\"xml_parser\", result);\n    \n    // Test uppercase after lowercase - should insert underscore\n    String testLowerToUpper = \"camelCase\";\n    String result2 = StringUtil.camelToUnderline(testLowerToUpper);\n    Assert.assertEquals(\"camel_case\", result2);\n    \n    // Test mixed case: should handle consecutive uppercase followed by mixed case\n    String testMixed = \"parseXMLData\";\n    String result3 = StringUtil.camelToUnderline(testMixed);\n    Assert.assertEquals(\"parse_xml_data\", result3);\n}"}], "metadata": {"source": "mrgbench", "func": "    public static String camelToUnderline(String string) {\n        if (noText(string)) {\n            return \"\";\n        }\n        int strLen = string.length();\n        StringBuilder sb = new StringBuilder(strLen);\n        for (int i = 0; i < strLen; i++) {\n            char c = string.charAt(i);\n            if (Character.isUpperCase(c) && i > 0) {\n                char prev = string.charAt(i - 1);\n                if (!Character.isUpperCase(prev) && prev != '_') {\n                    sb.append('_');\n                }\n            }\n            sb.append(Character.toLowerCase(c));\n        }\n        return sb.toString();\n    }", "repo": "mybatis-flex", "file_path": "mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java", "func_name": "camelToUnderline", "context": "/*\n *  Copyright (c) 2022-2025, Mybatis-Flex (fuhai999@gmail.com).\n *  <p>\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *  <p>\n *  http://www.apache.org/licenses/LICENSE-2.0\n *  <p>\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\npackage com.mybatisflex.core.util;\n\n\nimport com.mybatisflex.core.exception.FlexExceptions;\n\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.function.Function;\nimport java.util.regex.Pattern;\n\npublic class StringUtil {\n\n    private StringUtil() {\n    }\n\n    /**\n     * @see org.apache.ibatis.reflection.property.PropertyNamer#methodToProperty(String)\n     */\n    public static String methodToProperty(String name) {\n        if (name.startsWith(\"is\")) {\n            name = name.substring(2);\n        } else if (name.startsWith(\"get\") || name.startsWith(\"set\")) {\n            name = name.substring(3);\n        } else {\n            throw FlexExceptions.wrap(\"Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.\", name);\n        }\n        if (!name.isEmpty()) {\n            name = name.substring(0, 1).toLowerCase(Locale.ENGLISH).concat(name.substring(1));\n        }\n        return name;\n    }\n\n\n    /**\n     * 第一个字符转换为小写\n     *\n     * @param string\n     */\n    public static String firstCharToLowerCase(String string) {\n        char firstChar = string.charAt(0);\n        if (firstChar >= 'A' && firstChar <= 'Z') {\n            char[] chars = string.toCharArray();\n            chars[0] += ('a' - 'A');\n            return new String(chars);\n        }\n        return string;\n    }\n\n\n    /**\n     * 第一个字符转换为大写\n     *\n     * @param string\n     */\n    public static String firstCharToUpperCase(String string) {\n        char firstChar = string.charAt(0);\n        if (firstChar >= 'a' && firstChar <= 'z') {\n            char[] chars = string.toCharArray();\n            chars[0] -= ('a' - 'A');\n            return new String(chars);\n        }\n        return string;\n    }\n", "func_start": 86, "func_end": 103, "body_len": 17, "test_file": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java", "test_start": 26, "test_end": 36, "test_code": "    @Test\n    public void underlineToCamel() {\n        String testString1 = \"aa_bb_\";\n        String underlineToCamel = StringUtil.underlineToCamel(testString1);\n        System.out.println(underlineToCamel);\n        Assert.assertEquals(\"aaBb\", underlineToCamel);\n\n        String underline = StringUtil.camelToUnderline(underlineToCamel);\n        System.out.println(underline);\n        Assert.assertEquals(\"aa_bb\", underline);\n    }", "test_instruction": "mvn test -pl mybatis-flex-core -Dtest=\"com.mybatisflex.coretest.StringUtilTest#underlineToCamel\"", "language": "java"}, "mt_id": 1069}
