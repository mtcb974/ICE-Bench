[
  {
    "task-id": "langchain4j-langchain4j/src/main/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoader.java-loadDocuments",
    "project": "langchain4j",
    "file_path": "langchain4j/src/main/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoader.java",
    "func_name": "loadDocuments",
    "context": "package dev.langchain4j.data.document.loader;\n\nimport static dev.langchain4j.data.document.source.FileSystemSource.from;\nimport static dev.langchain4j.internal.Exceptions.illegalArgument;\nimport static dev.langchain4j.internal.Utils.getOrDefault;\nimport static java.nio.file.Files.isDirectory;\nimport static java.nio.file.Files.isRegularFile;\n\nimport dev.langchain4j.data.document.BlankDocumentException;\nimport dev.langchain4j.data.document.Document;\nimport dev.langchain4j.data.document.DocumentLoader;\nimport dev.langchain4j.data.document.DocumentParser;\nimport dev.langchain4j.data.document.parser.TextDocumentParser;\nimport dev.langchain4j.spi.data.document.parser.DocumentParserFactory;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.PathMatcher;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.stream.Stream;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class FileSystemDocumentLoader {\n\n    private static final Logger log = LoggerFactory.getLogger(FileSystemDocumentLoader.class);\n\n    private static final DocumentParser DEFAULT_DOCUMENT_PARSER =\n            getOrDefault(DocumentParserLoader.loadDocumentParser(), TextDocumentParser::new);\n\n    private FileSystemDocumentLoader() {}\n\n    /**\n     * Loads a {@link Document} from the specified file {@link Path}.\n     * <br>\n     * The file is parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath       The path to the file.\n     * @param documentParser The parser to be used for parsing text from the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(Path filePath, DocumentParser documentParser) {\n        if (!isRegularFile(filePath)) {\n            throw illegalArgument(\"'%s' is not a file\", filePath);\n        }\n\n        return DocumentLoader.load(from(filePath), documentParser);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file {@link Path}.\n     * <br>\n     * The file is parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath The path to the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(Path filePath) {\n        return loadDocument(filePath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file path.\n     * <br>\n     * The file is parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath       The path to the file.\n     * @param documentParser The parser to be used for parsing text from the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(String filePath, DocumentParser documentParser) {\n        return loadDocument(Paths.get(filePath), documentParser);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file path.\n     * <br>\n     * The file is parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath The path to the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(String filePath) {\n        return loadDocument(filePath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath) {\n        return loadDocuments(directoryPath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath, DocumentParser documentParser) {\n        return loadDocuments(Paths.get(directoryPath), documentParser);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath) {\n        return loadDocuments(directoryPath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param pathMatcher    Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                       For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                       will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                       When traversing the directory, each file path is converted from absolute to relative\n     *                       (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                       Thus, {@code pathMatcher} should use relative patterns.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(\n            Path directoryPath, PathMatcher pathMatcher, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, pathMatcher, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @param pathMatcher   Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                      For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                      will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                      When traversing the directory, each file path is converted from absolute to relative\n     *                      (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                      Thus, {@code pathMatcher} should use relative patterns.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath, PathMatcher pathMatcher) {\n        return loadDocuments(directoryPath, pathMatcher, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param pathMatcher    Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                       For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                       will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                       When traversing the directory, each file path is converted from absolute to relative\n     *                       (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                       Thus, {@code pathMatcher} should use relative patterns.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(\n            String directoryPath, PathMatcher pathMatcher, DocumentParser documentParser) {\n        return loadDocuments(Paths.get(directoryPath), pathMatcher, documentParser);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @param pathMatcher   Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                      For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                      will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                      When traversing the directory, each file path is converted from absolute to relative\n     *                      (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                      Thus, {@code pathMatcher} should use relative patterns.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath, PathMatcher pathMatcher) {\n        return loadDocuments(directoryPath, pathMatcher, DEFAULT_DOCUMENT_PARSER);\n    }",
    "func": "    public static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }",
    "comment": "/**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     */",
    "test_funcs": "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_load_documents_including_unknown_document_types langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::load_bad_directory langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_load_matching_documents",
    "test_class": "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::FileSystemDocumentLoaderTest",
    "func_start": 122,
    "func_end": 132,
    "body_len": 10,
    "instruction": "Write a function `loadDocuments(Path directoryPath, DocumentParser documentParser)` that takes a directory path and a document parser as inputs, validates that the path is a directory, lists all files in it (non-recursively), parses each file into a `Document` using the provided parser, skips any files that fail to load, and returns a list of successfully parsed `Document` objects. If the path is not a directory, throw an `IllegalArgumentException`. Any `IOException` encountered while listing the directory should be wrapped and rethrown as a `RuntimeException`.",
    "func_name_with_file_path": "langchain4j/src/main/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoader.java::loadDocuments",
    "test_funcs_split": [
      "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_load_documents_including_unknown_document_types",
      "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::load_bad_directory",
      "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_load_matching_documents"
    ],
    "test_start": 72,
    "test_end": 126,
    "test_file": "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java",
    "test_instruction": "mvn test -pl langchain4j -Dtest=\"dev.langchain4j.data.document.loader.FileSystemDocumentLoaderTest#should_load_documents_including_unknown_document_types\"",
    "test_code": "    @Test\n    void should_load_documents_including_unknown_document_types() {\n\n        // given\n        Path resourceDirectory = resourceDirectory();\n\n        // when\n        List<Document> documents = loadDocuments(resourceDirectory, new TextDocumentParser());\n\n        // then\n        List<String> fileNames = documents.stream()\n                .map(document -> document.metadata().getString(Document.FILE_NAME))\n                .toList();\n        assertThat(fileNames)\n                .containsExactlyInAnyOrder(\n                        \"miles-of-smiles-terms-of-use.txt\",\n                        \"test-file.banana\",\n                        \"test-file-iso-8859-1.txt\",\n                        \"test-file-utf8.txt\",\n                        \"chefs-prompt-based-on-ingredients-in-root.txt\");\n\n        // when-then\n        assertThat(loadDocuments(resourceDirectory.toString(), new TextDocumentParser()))\n                .isEqualTo(documents);\n\n        assertThat(loadDocuments(resourceDirectory)).isEqualTo(documents);\n        assertThat(loadDocuments(resourceDirectory.toString())).isEqualTo(documents);\n\n        DocumentParser parserThatFailsOnFirstNonBlankDocument = new DocumentParser() {\n\n            private boolean first = true;\n            private final DocumentParser parser = new TextDocumentParser();\n\n            @Override\n            public Document parse(InputStream inputStream) {\n                if (first && isNotBlank(inputStream)) {\n                    first = false;\n                    throw new RuntimeException(\"fail first\");\n                }\n                return parser.parse(inputStream);\n            }\n\n            private boolean isNotBlank(InputStream inputStream) {\n                try {\n                    return inputStream.available() > 10; // rough approximation\n                } catch (IOException e) {\n                    throw new RuntimeException(e);\n                }\n            }\n        };\n\n        // when-then\n        assertThat(loadDocuments(resourceDirectory, parserThatFailsOnFirstNonBlankDocument))\n                .hasSize(documents.size() - 1); // -1 because first document fails\n    }"
  },
  {
    "task-id": "langchain4j-langchain4j/src/main/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoader.java-loadDocumentsRecursively",
    "project": "langchain4j",
    "file_path": "langchain4j/src/main/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoader.java",
    "func_name": "loadDocumentsRecursively",
    "context": "package dev.langchain4j.data.document.loader;\n\nimport static dev.langchain4j.data.document.source.FileSystemSource.from;\nimport static dev.langchain4j.internal.Exceptions.illegalArgument;\nimport static dev.langchain4j.internal.Utils.getOrDefault;\nimport static java.nio.file.Files.isDirectory;\nimport static java.nio.file.Files.isRegularFile;\n\nimport dev.langchain4j.data.document.BlankDocumentException;\nimport dev.langchain4j.data.document.Document;\nimport dev.langchain4j.data.document.DocumentLoader;\nimport dev.langchain4j.data.document.DocumentParser;\nimport dev.langchain4j.data.document.parser.TextDocumentParser;\nimport dev.langchain4j.spi.data.document.parser.DocumentParserFactory;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.PathMatcher;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.stream.Stream;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class FileSystemDocumentLoader {\n\n    private static final Logger log = LoggerFactory.getLogger(FileSystemDocumentLoader.class);\n\n    private static final DocumentParser DEFAULT_DOCUMENT_PARSER =\n            getOrDefault(DocumentParserLoader.loadDocumentParser(), TextDocumentParser::new);\n\n    private FileSystemDocumentLoader() {}\n\n    /**\n     * Loads a {@link Document} from the specified file {@link Path}.\n     * <br>\n     * The file is parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath       The path to the file.\n     * @param documentParser The parser to be used for parsing text from the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(Path filePath, DocumentParser documentParser) {\n        if (!isRegularFile(filePath)) {\n            throw illegalArgument(\"'%s' is not a file\", filePath);\n        }\n\n        return DocumentLoader.load(from(filePath), documentParser);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file {@link Path}.\n     * <br>\n     * The file is parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath The path to the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(Path filePath) {\n        return loadDocument(filePath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file path.\n     * <br>\n     * The file is parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath       The path to the file.\n     * @param documentParser The parser to be used for parsing text from the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(String filePath, DocumentParser documentParser) {\n        return loadDocument(Paths.get(filePath), documentParser);\n    }\n\n    /**\n     * Loads a {@link Document} from the specified file path.\n     * <br>\n     * The file is parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Returned {@code Document} contains all the textual information from the file.\n     *\n     * @param filePath The path to the file.\n     * @return document\n     * @throws IllegalArgumentException If specified path is not a file.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static Document loadDocument(String filePath) {\n        return loadDocument(filePath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath) {\n        return loadDocuments(directoryPath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath, DocumentParser documentParser) {\n        return loadDocuments(Paths.get(directoryPath), documentParser);\n    }\n\n    /**\n     * Loads {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath) {\n        return loadDocuments(directoryPath, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param pathMatcher    Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                       For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                       will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                       When traversing the directory, each file path is converted from absolute to relative\n     *                       (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                       Thus, {@code pathMatcher} should use relative patterns.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(\n            Path directoryPath, PathMatcher pathMatcher, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.list(directoryPath)) {\n            return loadDocuments(pathStream, pathMatcher, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @param pathMatcher   Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                      For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                      will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                      When traversing the directory, each file path is converted from absolute to relative\n     *                      (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                      Thus, {@code pathMatcher} should use relative patterns.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(Path directoryPath, PathMatcher pathMatcher) {\n        return loadDocuments(directoryPath, pathMatcher, DEFAULT_DOCUMENT_PARSER);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param pathMatcher    Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                       For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                       will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                       When traversing the directory, each file path is converted from absolute to relative\n     *                       (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                       Thus, {@code pathMatcher} should use relative patterns.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(\n            String directoryPath, PathMatcher pathMatcher, DocumentParser documentParser) {\n        return loadDocuments(Paths.get(directoryPath), pathMatcher, documentParser);\n    }\n\n    /**\n     * Loads matching {@link Document}s from the specified directory. Does not use recursion.\n     * <br>\n     * The files are parsed using the default {@link DocumentParser}.\n     * The default {@code DocumentParser} is loaded through SPI (see {@link DocumentParserFactory}).\n     * If no {@code DocumentParserFactory} is available in the classpath, a {@link TextDocumentParser} is used.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath The path to the directory with files.\n     * @param pathMatcher   Only files whose paths match the provided {@link PathMatcher} will be loaded.\n     *                      For example, using {@code FileSystems.getDefault().getPathMatcher(\"glob:*.txt\")}\n     *                      will load all files from {@code directoryPath} with a {@code txt} extension.\n     *                      When traversing the directory, each file path is converted from absolute to relative\n     *                      (relative to {@code directoryPath}) before being matched by a {@code pathMatcher}.\n     *                      Thus, {@code pathMatcher} should use relative patterns.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */\n    public static List<Document> loadDocuments(String directoryPath, PathMatcher pathMatcher) {\n        return loadDocuments(directoryPath, pathMatcher, DEFAULT_DOCUMENT_PARSER);\n    }",
    "func": "    public static List<Document> loadDocumentsRecursively(Path directoryPath, DocumentParser documentParser) {\n        if (!isDirectory(directoryPath)) {\n            throw illegalArgument(\"'%s' is not a directory\", directoryPath);\n        }\n\n        try (Stream<Path> pathStream = Files.walk(directoryPath)) {\n            return loadDocuments(pathStream, (path) -> true, directoryPath, documentParser);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }",
    "comment": "    /**\n     * Recursively loads {@link Document}s from the specified directory and its subdirectories.\n     * <br>\n     * The files are parsed using the specified {@link DocumentParser}.\n     * <br>\n     * Skips any {@code Document}s that fail to load.\n     *\n     * @param directoryPath  The path to the directory with files.\n     * @param documentParser The parser to be used for parsing text from each file.\n     * @return list of documents\n     * @throws IllegalArgumentException If specified path is not a directory.\n     * @see dev.langchain4j.data.document.source.FileSystemSource FileSystemSource\n     */",
    "test_funcs": "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_recursively_load_documents langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_recursively_load_matching_documents langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_recursively_load_matching_documents_with_glob_crossing_directory_boundaries langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_recursively_load_matching_documents_with_glob_specifying_concrete_directory",
    "test_class": "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::FileSystemDocumentLoaderTest",
    "func_start": 304,
    "func_end": 314,
    "body_len": 10,
    "instruction": "Write a function `loadDocumentsRecursively(Path directoryPath, DocumentParser documentParser)` that takes a directory path and a document parser as inputs, recursively walks through the directory and all its subdirectories, parses each file into a Document using the provided parser, and returns a list of successfully parsed Documents. The function should skip any files that fail to load, and throw an IllegalArgumentException if the given path is not a valid directory. Use Files.walk to traverse the directory tree and ensure proper resource management with try-with-resources.",
    "func_name_with_file_path": "langchain4j/src/main/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoader.java::loadDocumentsRecursively",
    "test_funcs_split": [
      "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_recursively_load_documents",
      "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_recursively_load_matching_documents",
      "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_recursively_load_matching_documents_with_glob_crossing_directory_boundaries",
      "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java::should_recursively_load_matching_documents_with_glob_specifying_concrete_directory"
    ],
    "test_start": 157,
    "test_end": 192,
    "test_file": "langchain4j/src/test/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoaderTest.java",
    "test_instruction": "mvn test -pl langchain4j -Dtest=\"dev.langchain4j.data.document.loader.FileSystemDocumentLoaderTest#should_recursively_load_documents\"",
    "test_code": "    @Test\n    void should_recursively_load_documents() {\n\n        // given\n        Path resourceDirectory = resourceDirectory();\n\n        // when\n        List<Document> documents = loadDocumentsRecursively(resourceDirectory, new TextDocumentParser());\n\n        // then\n        List<String> fileNames = documents.stream()\n                .map(document -> document.metadata().getString(Document.FILE_NAME))\n                .toList();\n        assertThat(fileNames)\n                .containsExactlyInAnyOrder(\n                        \"miles-of-smiles-terms-of-use.txt\",\n                        \"test-file.banana\",\n                        \"test-file-iso-8859-1.txt\",\n                        \"test-file-utf8.txt\",\n                        \"chefs-prompt-based-on-ingredients.txt\",\n                        \"chefs-prompt-system-message.txt\",\n                        \"chefs-prompt-based-on-ingredients-in-root.txt\",\n                        \"chefs-prompt-based-on-ingredients-in-subdirectory.txt\",\n                        \"test-file-2.banana\",\n                        \"file1.txt\",\n                        \"file2.txt\",\n                        \"test-file-3.banana\",\n                        \"test-file-4.banana\");\n\n        // when-then\n        assertThat(loadDocumentsRecursively(resourceDirectory.toString(), new TextDocumentParser()))\n                .isEqualTo(documents);\n\n        assertThat(loadDocumentsRecursively(resourceDirectory)).isEqualTo(documents);\n        assertThat(loadDocumentsRecursively(resourceDirectory.toString())).isEqualTo(documents);\n    }"
  },
  {
    "task-id": "langchain4j-langchain4j-core/src/main/java/dev/langchain4j/store/embedding/CosineSimilarity.java-between",
    "project": "langchain4j",
    "file_path": "langchain4j-core/src/main/java/dev/langchain4j/store/embedding/CosineSimilarity.java",
    "func_name": "between",
    "context": "package dev.langchain4j.store.embedding;\n\nimport dev.langchain4j.data.embedding.Embedding;\n\nimport static dev.langchain4j.internal.Exceptions.illegalArgument;\nimport static dev.langchain4j.internal.ValidationUtils.ensureNotNull;\n\n/**\n * Utility class for calculating cosine similarity between two vectors.\n */\npublic class CosineSimilarity {\n    private CosineSimilarity() {}\n\n    /**\n     * A small value to avoid division by zero.\n     */\n    public static final float EPSILON = 1e-8f;",
    "func": "    public static double between(Embedding embeddingA, Embedding embeddingB) {\n        ensureNotNull(embeddingA, \"embeddingA\");\n        ensureNotNull(embeddingB, \"embeddingB\");\n\n        float[] vectorA = embeddingA.vector();\n        float[] vectorB = embeddingB.vector();\n\n        if (vectorA.length != vectorB.length) {\n            throw illegalArgument(\"Length of vector a (%s) must be equal to the length of vector b (%s)\",\n                    vectorA.length, vectorB.length);\n        }\n\n        double dotProduct = 0.0;\n        double normA = 0.0;\n        double normB = 0.0;\n\n        for (int i = 0; i < vectorA.length; i++) {\n            dotProduct += vectorA[i] * vectorB[i];\n            normA += vectorA[i] * vectorA[i];\n            normB += vectorB[i] * vectorB[i];\n        }\n\n        // Avoid division by zero.\n        return dotProduct / Math.max(Math.sqrt(normA) * Math.sqrt(normB), EPSILON);\n    }",
    "comment": "    /**\n     * Calculates cosine similarity between two vectors.\n     * <p>\n     * Cosine similarity measures the cosine of the angle between two vectors, indicating their directional similarity.\n     * It produces a value in the range:\n     * <p>\n     * -1 indicates vectors are diametrically opposed (opposite directions).\n     * <p>\n     * 0 indicates vectors are orthogonal (no directional similarity).\n     * <p>\n     * 1 indicates vectors are pointing in the same direction (but not necessarily of the same magnitude).\n     * <p>\n     * Not to be confused with cosine distance ([0..2]), which quantifies how different two vectors are.\n     * <p>\n     * Embeddings of all-zeros vectors are considered orthogonal to all other vectors;\n     * including other all-zeros vectors.\n     *\n     * @param embeddingA first embedding vector\n     * @param embeddingB second embedding vector\n     * @return cosine similarity in the range [-1..1]\n     */",
    "test_funcs": "langchain4j-core/src/test/java/dev/langchain4j/store/embedding/CosineSimilarityTest.java::should_calculate_cosine_similarity langchain4j-core/src/test/java/dev/langchain4j/store/embedding/CosineSimilarityTest.java::test_bad langchain4j-core/src/test/java/dev/langchain4j/store/embedding/CosineSimilarityTest.java::test_zeros",
    "test_class": "langchain4j-core/src/test/java/dev/langchain4j/store/embedding/CosineSimilarityTest.java::CosineSimilarityTest",
    "func_start": 40,
    "func_end": 64,
    "body_len": 24,
    "instruction": "Write a static method `between` that takes two `Embedding` objects as parameters and returns a `double` representing the cosine similarity between their vector representations. The method should first validate that neither embedding is null, then compute the dot product and Euclidean norms of the two vectors. It should handle the case where either vector is a zero vector by ensuring division by zero is avoided using a small epsilon value. The cosine similarity must be calculated as the dot product divided by the product of the magnitudes of the two vectors, and the result should be in the range [-1, 1], with all-zero vectors treated as orthogonal to all other vectors (including other zero vectors).",
    "func_name_with_file_path": "langchain4j-core/src/main/java/dev/langchain4j/store/embedding/CosineSimilarity.java::between",
    "test_funcs_split": [
      "langchain4j-core/src/test/java/dev/langchain4j/store/embedding/CosineSimilarityTest.java::should_calculate_cosine_similarity",
      "langchain4j-core/src/test/java/dev/langchain4j/store/embedding/CosineSimilarityTest.java::test_bad",
      "langchain4j-core/src/test/java/dev/langchain4j/store/embedding/CosineSimilarityTest.java::test_zeros"
    ],
    "test_start": 28,
    "test_end": 35,
    "test_file": "langchain4j-core/src/test/java/dev/langchain4j/store/embedding/CosineSimilarityTest.java",
    "test_instruction": "mvn test -pl langchain4j-core -Dtest=\"dev.langchain4j.store.embedding.CosineSimilarityTest#should_calculate_cosine_similarity\"",
    "test_code": "    @Test\n    void should_calculate_cosine_similarity() {\n        Embedding embeddingA = Embedding.from(new float[] {1, -1, 1});\n        Embedding embeddingB = Embedding.from(new float[] {-1, 1, -1});\n\n        assertThat(CosineSimilarity.between(embeddingA, embeddingA)).isCloseTo(1, withPercentage(1));\n        assertThat(CosineSimilarity.between(embeddingA, embeddingB)).isCloseTo(-1, withPercentage(1));\n    }"
  },
  {
    "task-id": "langchain4j-langchain4j-core/src/main/java/dev/langchain4j/internal/Utils.java-areNotNullOrBlank",
    "project": "langchain4j",
    "file_path": "langchain4j-core/src/main/java/dev/langchain4j/internal/Utils.java",
    "func_name": "areNotNullOrBlank",
    "context": "package dev.langchain4j.internal;\n\nimport static dev.langchain4j.internal.Exceptions.illegalArgument;\nimport static dev.langchain4j.internal.ValidationUtils.ensureNotEmpty;\nimport static java.net.HttpURLConnection.HTTP_OK;\nimport static java.nio.charset.StandardCharsets.UTF_8;\nimport static java.util.Collections.unmodifiableList;\nimport static java.util.Collections.unmodifiableMap;\nimport static java.util.Collections.unmodifiableSet;\n\nimport dev.langchain4j.Internal;\nimport java.io.ByteArrayOutputStream;\nimport java.io.InputStream;\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Proxy;\nimport java.net.HttpURLConnection;\nimport java.net.URI;\nimport java.net.URL;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.HexFormat;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.function.Supplier;\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * Utility methods.\n */\n@Internal\npublic class Utils {\n\n    private Utils() {}\n\n    /**\n     * Returns the first non-null value from the provided array of values.\n     * If all values are null, an IllegalArgumentException is thrown.\n     *\n     * @param name   A non-null string representing the name associated with the values.\n     * @param values An array of potentially nullable values to search through.\n     * @return The first non-null value in the array.\n     * @throws IllegalArgumentException If all values are null or if the array is empty.\n     */\n    @SafeVarargs\n    @NonNull\n    public static <T> T firstNotNull(@NonNull String name, @Nullable T... values) {\n        ensureNotEmpty(values, name + \" values\");\n        for (T value : values) {\n            if (value != null) {\n                return value;\n            }\n        }\n        throw illegalArgument(\"At least one of the given '%s' values must be not null\", name);\n    }\n\n    /**\n     * Returns the given value if it is not {@code null}, otherwise returns the given default value.\n     *\n     * @param value        The value to return if it is not {@code null}.\n     * @param defaultValue The value to return if the value is {@code null}.\n     * @param <T>          The type of the value.\n     * @return the given value if it is not {@code null}, otherwise returns the given default value.\n     */\n    public static <T> T getOrDefault(T value, T defaultValue) {\n        return value != null ? value : defaultValue;\n    }\n\n    /**\n     * Returns the given list if it is not {@code null} and not empty, otherwise returns the given default list.\n     *\n     * @param list        The list to return if it is not {@code null} and not empty.\n     * @param defaultList The list to return if the list is {@code null} or empty.\n     * @param <T>         The type of the value.\n     * @return the given list if it is not {@code null} and not empty, otherwise returns the given default list.\n     */\n    public static <T> List<T> getOrDefault(List<T> list, List<T> defaultList) {\n        return isNullOrEmpty(list) ? defaultList : list;\n    }\n\n    /**\n     * Returns the given map if it is not {@code null} and not empty, otherwise returns the given default map.\n     *\n     * @param map        The map to return if it is not {@code null} and not empty.\n     * @param defaultMap The map to return if the map is {@code null} or empty.\n     * @return the given map if it is not {@code null} and not empty, otherwise returns the given default map.\n     */\n    public static <K, V> Map<K, V> getOrDefault(Map<K, V> map, Map<K, V> defaultMap) {\n        return isNullOrEmpty(map) ? defaultMap : map;\n    }\n\n    /**\n     * Returns the given value if it is not {@code null}, otherwise returns the value returned by the given supplier.\n     *\n     * @param value                The value to return if it is not {@code null}.\n     * @param defaultValueSupplier The supplier to call if the value is {@code null}.\n     * @param <T>                  The type of the value.\n     * @return the given value if it is not {@code null}, otherwise returns the value returned by the given supplier.\n     */\n    public static <T> T getOrDefault(@Nullable T value, Supplier<T> defaultValueSupplier) {\n        return value != null ? value : defaultValueSupplier.get();\n    }\n\n    /**\n     * Is the given string {@code null} or blank?\n     *\n     * @param string The string to check.\n     * @return true if the string is {@code null} or blank.\n     */\n    public static boolean isNullOrBlank(String string) {\n        return string == null || string.trim().isEmpty();\n    }\n\n    /**\n     * Is the given string {@code null} or empty (\"\")?\n     *\n     * @param string The string to check.\n     * @return true if the string is {@code null} or empty.\n     */\n    public static boolean isNullOrEmpty(String string) {\n        return string == null || string.isEmpty();\n    }\n\n    /**\n     * Is the given string not {@code null} and not blank?\n     *\n     * @param string The string to check.\n     * @return true if there's something in the string.\n     */\n    public static boolean isNotNullOrBlank(String string) {\n        return !isNullOrBlank(string);\n    }\n\n    /**\n     * Is the given string not {@code null} and not empty (\"\")?\n     *\n     * @param string The string to check.\n     * @return true if the given string is not {@code null} and not empty (\"\")?\n     */\n    public static boolean isNotNullOrEmpty(String string) {\n        return !isNullOrEmpty(string);\n    }",
    "func": "    public static boolean areNotNullOrBlank(String... strings) {\n        if (strings == null || strings.length == 0) {\n            return false;\n        }\n\n        for (String string : strings) {\n            if (isNullOrBlank(string)) {\n                return false;\n            }\n        }\n\n        return true;\n    }",
    "comment": "    /**\n     * Are all the given strings not {@code null} and not blank?\n     *\n     * @param strings The strings to check.\n     * @return {@code true} if every string is non-{@code null} and non-empty.\n     */",
    "test_funcs": "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_generate_tokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::userMessageWithTokens langchain4j-core/src/test/java/dev/langchain4j/internal/UtilsTest.java::test_areNotNullOrBlank langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::textWithTokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::systemMessageWithTokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::aiMessageWithTokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_system_message_with_tokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_repeat_n_times langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_ai_message_with_tokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_user_message_with_tokens",
    "test_class": "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::TestUtils langchain4j-core/src/test/java/dev/langchain4j/internal/UtilsTest.java::UtilsTest",
    "func_start": 159,
    "func_end": 171,
    "body_len": 12,
    "instruction": "Write a function `areNotNullOrBlank` that takes a variable number of string arguments and returns true only if none of the strings are null and none are blank (i.e., empty or containing only whitespace); if the input array is null or has no elements, the function should return false.",
    "func_name_with_file_path": "langchain4j-core/src/main/java/dev/langchain4j/internal/Utils.java::areNotNullOrBlank",
    "test_funcs_split": [
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_generate_tokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::userMessageWithTokens",
      "langchain4j-core/src/test/java/dev/langchain4j/internal/UtilsTest.java::test_areNotNullOrBlank",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::textWithTokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::systemMessageWithTokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::aiMessageWithTokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_system_message_with_tokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_repeat_n_times",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_ai_message_with_tokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_user_message_with_tokens"
    ],
    "test_start": 62,
    "test_end": 68,
    "test_file": "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java",
    "test_instruction": "mvn test -pl langchain4j -Dtest=\"dev.langchain4j.internal.TestUtils#should_generate_tokens\"",
    "test_code": "    @ParameterizedTest\n    @ValueSource(ints = {1, 2, 5, 10, 25, 50, 100, 250, 500, 1000})\n    void should_generate_tokens(int numberOfTokens) {\n        String text = textWithTokens(numberOfTokens);\n\n        assertThat(TOKEN_COUNT_ESTIMATOR.estimateTokenCountInText(text)).isEqualTo(numberOfTokens);\n    }"
  },
  {
    "task-id": "langchain4j-langchain4j-core/src/main/java/dev/langchain4j/internal/Utils.java-readBytes",
    "project": "langchain4j",
    "file_path": "langchain4j-core/src/main/java/dev/langchain4j/internal/Utils.java",
    "func_name": "readBytes",
    "context": "package dev.langchain4j.internal;\n\nimport static dev.langchain4j.internal.Exceptions.illegalArgument;\nimport static dev.langchain4j.internal.ValidationUtils.ensureNotEmpty;\nimport static java.net.HttpURLConnection.HTTP_OK;\nimport static java.nio.charset.StandardCharsets.UTF_8;\nimport static java.util.Collections.unmodifiableList;\nimport static java.util.Collections.unmodifiableMap;\nimport static java.util.Collections.unmodifiableSet;\n\nimport dev.langchain4j.Internal;\nimport java.io.ByteArrayOutputStream;\nimport java.io.InputStream;\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Proxy;\nimport java.net.HttpURLConnection;\nimport java.net.URI;\nimport java.net.URL;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.HexFormat;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.function.Supplier;\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * Utility methods.\n */\n@Internal\npublic class Utils {\n\n    private Utils() {}\n\n    /**\n     * Returns the first non-null value from the provided array of values.\n     * If all values are null, an IllegalArgumentException is thrown.\n     *\n     * @param name   A non-null string representing the name associated with the values.\n     * @param values An array of potentially nullable values to search through.\n     * @return The first non-null value in the array.\n     * @throws IllegalArgumentException If all values are null or if the array is empty.\n     */\n    @SafeVarargs\n    @NonNull\n    public static <T> T firstNotNull(@NonNull String name, @Nullable T... values) {\n        ensureNotEmpty(values, name + \" values\");\n        for (T value : values) {\n            if (value != null) {\n                return value;\n            }\n        }\n        throw illegalArgument(\"At least one of the given '%s' values must be not null\", name);\n    }\n\n    /**\n     * Returns the given value if it is not {@code null}, otherwise returns the given default value.\n     *\n     * @param value        The value to return if it is not {@code null}.\n     * @param defaultValue The value to return if the value is {@code null}.\n     * @param <T>          The type of the value.\n     * @return the given value if it is not {@code null}, otherwise returns the given default value.\n     */\n    public static <T> T getOrDefault(T value, T defaultValue) {\n        return value != null ? value : defaultValue;\n    }\n\n    /**\n     * Returns the given list if it is not {@code null} and not empty, otherwise returns the given default list.\n     *\n     * @param list        The list to return if it is not {@code null} and not empty.\n     * @param defaultList The list to return if the list is {@code null} or empty.\n     * @param <T>         The type of the value.\n     * @return the given list if it is not {@code null} and not empty, otherwise returns the given default list.\n     */\n    public static <T> List<T> getOrDefault(List<T> list, List<T> defaultList) {\n        return isNullOrEmpty(list) ? defaultList : list;\n    }\n\n    /**\n     * Returns the given map if it is not {@code null} and not empty, otherwise returns the given default map.\n     *\n     * @param map        The map to return if it is not {@code null} and not empty.\n     * @param defaultMap The map to return if the map is {@code null} or empty.\n     * @return the given map if it is not {@code null} and not empty, otherwise returns the given default map.\n     */\n    public static <K, V> Map<K, V> getOrDefault(Map<K, V> map, Map<K, V> defaultMap) {\n        return isNullOrEmpty(map) ? defaultMap : map;\n    }\n\n    /**\n     * Returns the given value if it is not {@code null}, otherwise returns the value returned by the given supplier.\n     *\n     * @param value                The value to return if it is not {@code null}.\n     * @param defaultValueSupplier The supplier to call if the value is {@code null}.\n     * @param <T>                  The type of the value.\n     * @return the given value if it is not {@code null}, otherwise returns the value returned by the given supplier.\n     */\n    public static <T> T getOrDefault(@Nullable T value, Supplier<T> defaultValueSupplier) {\n        return value != null ? value : defaultValueSupplier.get();\n    }\n\n    /**\n     * Is the given string {@code null} or blank?\n     *\n     * @param string The string to check.\n     * @return true if the string is {@code null} or blank.\n     */\n    public static boolean isNullOrBlank(String string) {\n        return string == null || string.trim().isEmpty();\n    }\n\n    /**\n     * Is the given string {@code null} or empty (\"\")?\n     *\n     * @param string The string to check.\n     * @return true if the string is {@code null} or empty.\n     */\n    public static boolean isNullOrEmpty(String string) {\n        return string == null || string.isEmpty();\n    }\n\n    /**\n     * Is the given string not {@code null} and not blank?\n     *\n     * @param string The string to check.\n     * @return true if there's something in the string.\n     */\n    public static boolean isNotNullOrBlank(String string) {\n        return !isNullOrBlank(string);\n    }\n\n    /**\n     * Is the given string not {@code null} and not empty (\"\")?\n     *\n     * @param string The string to check.\n     * @return true if the given string is not {@code null} and not empty (\"\")?\n     */\n    public static boolean isNotNullOrEmpty(String string) {\n        return !isNullOrEmpty(string);\n    }\n\n    /**\n     * Are all the given strings not {@code null} and not blank?\n     *\n     * @param strings The strings to check.\n     * @return {@code true} if every string is non-{@code null} and non-empty.\n     */\n    public static boolean areNotNullOrBlank(String... strings) {\n        if (strings == null || strings.length == 0) {\n            return false;\n        }\n\n        for (String string : strings) {\n            if (isNullOrBlank(string)) {\n                return false;\n            }\n        }\n\n        return true;\n    }\n\n    /**\n     * Is the collection {@code null} or empty?\n     *\n     * @param collection The collection to check.\n     * @return {@code true} if the collection is {@code null} or {@link Collection#isEmpty()}, otherwise {@code false}.\n     */\n    public static boolean isNullOrEmpty(Collection<?> collection) {\n        return collection == null || collection.isEmpty();\n    }\n\n    /**\n     * Is the iterable object {@code null} or empty?\n     *\n     * @param iterable The iterable object to check.\n     * @return {@code true} if the iterable object is {@code null} or there are no objects to iterate over, otherwise {@code false}.\n     */\n    public static boolean isNullOrEmpty(Iterable<?> iterable) {\n        return iterable == null || !iterable.iterator().hasNext();\n    }\n\n    /**\n     * Utility method to check if an array is null or has no elements.\n     *\n     * @param array the array to check\n     * @return {@code true} if the array is null or has no elements, otherwise {@code false}\n     */\n    public static boolean isNullOrEmpty(Object[] array) {\n        return array == null || array.length == 0;\n    }\n\n    /**\n     * Is the map object {@code null} or empty?\n     *\n     * @param map The iterable object to check.\n     * @return {@code true} if the map object is {@code null} or empty map, otherwise {@code false}.\n     */\n    public static boolean isNullOrEmpty(@Nullable Map<?, ?> map) {\n        return map == null || map.isEmpty();\n    }\n\n    /**\n     * Returns a string consisting of the given string repeated {@code times} times.\n     *\n     * @param string The string to repeat.\n     * @param times  The number of times to repeat the string.\n     * @return A string consisting of the given string repeated {@code times} times.\n     */\n    public static String repeat(String string, int times) {\n        StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < times; i++) {\n            sb.append(string);\n        }\n        return sb.toString();\n    }\n\n    /**\n     * Returns a random UUID.\n     *\n     * @return a UUID.\n     */\n    public static String randomUUID() {\n        return UUID.randomUUID().toString();\n    }\n\n    /**\n     * Internal method to get a SHA-256 instance of {@link MessageDigest}.\n     *\n     * @return a {@link MessageDigest}.\n     */\n    @JacocoIgnoreCoverageGenerated\n    private static MessageDigest getSha256Instance() {\n        try {\n            return MessageDigest.getInstance(\"SHA-256\");\n        } catch (NoSuchAlgorithmException e) {\n            throw new IllegalArgumentException(e);\n        }\n    }\n\n    /**\n     * Generates a UUID from a hash of the given input string.\n     *\n     * @param input The input string.\n     * @return A UUID.\n     */\n    public static String generateUUIDFrom(String input) {\n        byte[] hashBytes = getSha256Instance().digest(input.getBytes(UTF_8));\n        String hexFormat = HexFormat.of().formatHex(hashBytes);\n        return UUID.nameUUIDFromBytes(hexFormat.getBytes(UTF_8)).toString();\n    }\n\n    /**\n     * Appends a trailing '/' if the provided URL does not end with '/'\n     *\n     * @param url URL to check for trailing '/'\n     * @return Same URL if it already ends with '/' or a new URL with '/' appended\n     */\n    public static String ensureTrailingForwardSlash(String url) {\n        return url.endsWith(\"/\") ? url : url + \"/\";\n    }\n\n    /**\n     * Returns the given object's {@code toString()} surrounded by quotes.\n     *\n     * <p>If the given object is {@code null}, the string {@code \"null\"} is returned.\n     *\n     * @param object The object to quote.\n     * @return The given object surrounded by quotes.\n     */\n    public static String quoted(Object object) {\n        if (object == null) {\n            return \"null\";\n        }\n        return \"\\\"\" + object + \"\\\"\";\n    }\n\n    /**\n     * Returns the first {@code numberOfChars} characters of the given string.\n     * If the string is shorter than {@code numberOfChars}, the whole string is returned.\n     *\n     * @param string        The string to get the first characters from.\n     * @param numberOfChars The number of characters to return.\n     * @return The first {@code numberOfChars} characters of the given string.\n     */\n    public static String firstChars(String string, int numberOfChars) {\n        if (string == null) {\n            return null;\n        }\n        return string.length() > numberOfChars ? string.substring(0, numberOfChars) : string;\n    }",
    "func": "    public static byte[] readBytes(String url) {\n        try {\n            if (url.startsWith(\"http://\") || url.startsWith(\"https://\")) {\n                // Handle URLs\n                HttpURLConnection connection = (HttpURLConnection) new URL(url).openConnection();\n                connection.setRequestMethod(\"GET\");\n                // Add headers to appear as a legitimate browser request\n                connection.setRequestProperty(\n                        \"User-Agent\",\n                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\");\n                connection.setRequestProperty(\n                        \"Accept\", \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\");\n                connection.setRequestProperty(\"Accept-Language\", \"en-US,en;q=0.5\");\n                connection.setRequestProperty(\"Accept-Encoding\", \"gzip, deflate\");\n                connection.setRequestProperty(\"Connection\", \"keep-alive\");\n\n                int responseCode = connection.getResponseCode();\n\n                if (responseCode == HTTP_OK) {\n                    InputStream inputStream = connection.getInputStream();\n                    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n\n                    byte[] buffer = new byte[1024];\n                    int bytesRead;\n                    while ((bytesRead = inputStream.read(buffer)) != -1) {\n                        outputStream.write(buffer, 0, bytesRead);\n                    }\n\n                    return outputStream.toByteArray();\n                } else {\n                    throw new RuntimeException(\"Error while reading: \" + responseCode);\n                }\n            } else {\n                // Handle files\n                return Files.readAllBytes(Path.of(new URI(url)));\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }",
    "comment": "    /**\n     * Reads the content as bytes from the given URL as a GET request for HTTP/HTTPS resources,\n     * and from files stored on the local filesystem.\n     *\n     * @param url The URL to read from.\n     * @return The content as bytes.\n     * @throws RuntimeException if the request fails.\n     */",
    "test_funcs": "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_generate_tokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::userMessageWithTokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::textWithTokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::systemMessageWithTokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::aiMessageWithTokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_system_message_with_tokens langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_repeat_n_times langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_ai_message_with_tokens langchain4j-core/src/test/java/dev/langchain4j/internal/UtilsTest.java::test_readBytes langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_user_message_with_tokens",
    "test_class": "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::TestUtils langchain4j-core/src/test/java/dev/langchain4j/internal/UtilsTest.java::UtilsTest",
    "func_start": 311,
    "func_end": 350,
    "body_len": 39,
    "instruction": "Write a function `readBytes(String url)` that takes a URL as a string and returns the content as a byte array. The function should handle both HTTP/HTTPS URLs and local file paths. For HTTP/HTTPS URLs, it should make a GET request with headers mimicking a real browser (including User-Agent, Accept, Accept-Language, Accept-Encoding, and Connection headers) to increase the likelihood of a successful response. If the HTTP response code is not 200 (OK), the function should throw a RuntimeException with the response code. For local file paths, the function should read the file directly using NIO and return its contents as bytes. Any exceptions during the process should be wrapped and rethrown as a RuntimeException.",
    "func_name_with_file_path": "langchain4j-core/src/main/java/dev/langchain4j/internal/Utils.java::readBytes",
    "test_funcs_split": [
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_generate_tokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::userMessageWithTokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::textWithTokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::systemMessageWithTokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::aiMessageWithTokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_system_message_with_tokens",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_repeat_n_times",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_ai_message_with_tokens",
      "langchain4j-core/src/test/java/dev/langchain4j/internal/UtilsTest.java::test_readBytes",
      "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java::should_create_user_message_with_tokens"
    ],
    "test_start": 62,
    "test_end": 68,
    "test_file": "langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java",
    "test_instruction": "mvn test -pl langchain4j -Dtest=\"dev.langchain4j.internal.TestUtils#should_generate_tokens\"",
    "test_code": "    @ParameterizedTest\n    @ValueSource(ints = {1, 2, 5, 10, 25, 50, 100, 250, 500, 1000})\n    void should_generate_tokens(int numberOfTokens) {\n        String text = textWithTokens(numberOfTokens);\n\n        assertThat(TOKEN_COUNT_ESTIMATOR.estimateTokenCountInText(text)).isEqualTo(numberOfTokens);\n    }"
  },
  {
    "task-id": "mybatis-flex-mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java-methodToProperty",
    "project": "mybatis-flex",
    "file_path": "mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java",
    "func_name": "methodToProperty",
    "context": "/*\n *  Copyright (c) 2022-2025, Mybatis-Flex (fuhai999@gmail.com).\n *  <p>\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *  <p>\n *  http://www.apache.org/licenses/LICENSE-2.0\n *  <p>\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\npackage com.mybatisflex.core.util;\n\n\nimport com.mybatisflex.core.exception.FlexExceptions;\n\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.function.Function;\nimport java.util.regex.Pattern;\n\npublic class StringUtil {\n\n    private StringUtil() {\n    }",
    "func": "    public static String methodToProperty(String name) {\n        if (name.startsWith(\"is\")) {\n            name = name.substring(2);\n        } else if (name.startsWith(\"get\") || name.startsWith(\"set\")) {\n            name = name.substring(3);\n        } else {\n            throw FlexExceptions.wrap(\"Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.\", name);\n        }\n        if (!name.isEmpty()) {\n            name = name.substring(0, 1).toLowerCase(Locale.ENGLISH).concat(name.substring(1));\n        }\n        return name;\n    }",
    "comment": "    /**\n     * @see org.apache.ibatis.reflection.property.PropertyNamer#methodToProperty(String)\n     */",
    "test_funcs": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java::testMethod2Property",
    "test_class": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java::StringUtilTest",
    "func_start": 34,
    "func_end": 46,
    "body_len": 12,
    "instruction": "Write a function `methodToProperty(String name)` that converts a Java method name into its corresponding property name by removing the prefix 'is', 'get', or 'set' and converting the first letter of the remaining name to lowercase. If the method name does not start with one of these prefixes, throw an exception indicating the invalid format. The function should return the resulting property name as a string.",
    "func_name_with_file_path": "mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java::methodToProperty",
    "test_funcs_split": [
      "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java::testMethod2Property"
    ],
    "test_start": 38,
    "test_end": 46,
    "test_file": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java",
    "test_instruction": "mvn test -pl mybatis-flex-core -Dtest=\"com.mybatisflex.coretest.StringUtilTest#testMethod2Property\"",
    "test_code": "    @Test\n    public void testMethod2Property() {\n        Assert.assertEquals(\"u\", StringUtil.methodToProperty(\"isU\"));\n        Assert.assertEquals(\"u\", StringUtil.methodToProperty(\"getU\"));\n        Assert.assertEquals(\"name\", StringUtil.methodToProperty(\"getName\"));\n        Assert.assertEquals(\"uName\", StringUtil.methodToProperty(\"getUName\"));\n        Assert.assertEquals(\"uName\", StringUtil.methodToProperty(\"isUName\"));\n        Assert.assertThrows(MybatisFlexException.class, () -> StringUtil.methodToProperty(\"name\"));\n    }"
  },
  {
    "task-id": "mybatis-flex-mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java-camelToUnderline",
    "project": "mybatis-flex",
    "file_path": "mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java",
    "func_name": "camelToUnderline",
    "context": "/*\n *  Copyright (c) 2022-2025, Mybatis-Flex (fuhai999@gmail.com).\n *  <p>\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *  <p>\n *  http://www.apache.org/licenses/LICENSE-2.0\n *  <p>\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\npackage com.mybatisflex.core.util;\n\n\nimport com.mybatisflex.core.exception.FlexExceptions;\n\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.function.Function;\nimport java.util.regex.Pattern;\n\npublic class StringUtil {\n\n    private StringUtil() {\n    }\n\n    /**\n     * @see org.apache.ibatis.reflection.property.PropertyNamer#methodToProperty(String)\n     */\n    public static String methodToProperty(String name) {\n        if (name.startsWith(\"is\")) {\n            name = name.substring(2);\n        } else if (name.startsWith(\"get\") || name.startsWith(\"set\")) {\n            name = name.substring(3);\n        } else {\n            throw FlexExceptions.wrap(\"Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.\", name);\n        }\n        if (!name.isEmpty()) {\n            name = name.substring(0, 1).toLowerCase(Locale.ENGLISH).concat(name.substring(1));\n        }\n        return name;\n    }\n\n\n    /**\n     * \n     *\n     * @param string\n     */\n    public static String firstCharToLowerCase(String string) {\n        char firstChar = string.charAt(0);\n        if (firstChar >= 'A' && firstChar <= 'Z') {\n            char[] chars = string.toCharArray();\n            chars[0] += ('a' - 'A');\n            return new String(chars);\n        }\n        return string;\n    }\n\n\n    /**\n     * \n     *\n     * @param string\n     */\n    public static String firstCharToUpperCase(String string) {\n        char firstChar = string.charAt(0);\n        if (firstChar >= 'a' && firstChar <= 'z') {\n            char[] chars = string.toCharArray();\n            chars[0] -= ('a' - 'A');\n            return new String(chars);\n        }\n        return string;\n    }\n",
    "func": "    public static String camelToUnderline(String string) {\n        if (noText(string)) {\n            return \"\";\n        }\n        int strLen = string.length();\n        StringBuilder sb = new StringBuilder(strLen);\n        for (int i = 0; i < strLen; i++) {\n            char c = string.charAt(i);\n            if (Character.isUpperCase(c) && i > 0) {\n                char prev = string.charAt(i - 1);\n                if (!Character.isUpperCase(prev) && prev != '_') {\n                    sb.append('_');\n                }\n            }\n            sb.append(Character.toLowerCase(c));\n        }\n        return sb.toString();\n    }",
    "comment": "    /**\n     * \n     *\n     * @param string\n     */",
    "test_funcs": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java::underlineToCamel mybatis-flex-core/src/test/java/com/mybatisflex/core/util/StringUtilTest.java::testCamelToUnderline",
    "test_class": "mybatis-flex-core/src/test/java/com/mybatisflex/core/util/StringUtilTest.java::StringUtilTest mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java::StringUtilTest",
    "func_start": 86,
    "func_end": 103,
    "body_len": 17,
    "instruction": "Write a function `camelToUnderline(String string)` that converts a camelCase string into a lowercase underscore-separated string. The function should insert an underscore before each uppercase letter (except at the start of the string) only if the previous character is a lowercase letter or not an underscore, then convert the entire string to lowercase. If the input string is null or empty, return an empty string.",
    "func_name_with_file_path": "mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java::camelToUnderline",
    "test_funcs_split": [
      "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java::underlineToCamel",
      "mybatis-flex-core/src/test/java/com/mybatisflex/core/util/StringUtilTest.java::testCamelToUnderline"
    ],
    "test_start": 26,
    "test_end": 36,
    "test_file": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java",
    "test_instruction": "mvn test -pl mybatis-flex-core -Dtest=\"com.mybatisflex.coretest.StringUtilTest#underlineToCamel\"",
    "test_code": "    @Test\n    public void underlineToCamel() {\n        String testString1 = \"aa_bb_\";\n        String underlineToCamel = StringUtil.underlineToCamel(testString1);\n        System.out.println(underlineToCamel);\n        Assert.assertEquals(\"aaBb\", underlineToCamel);\n\n        String underline = StringUtil.camelToUnderline(underlineToCamel);\n        System.out.println(underline);\n        Assert.assertEquals(\"aa_bb\", underline);\n    }"
  },
  {
    "task-id": "mybatis-flex-mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java-underlineToCamel",
    "project": "mybatis-flex",
    "file_path": "mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java",
    "func_name": "underlineToCamel",
    "context": "/*\n *  Copyright (c) 2022-2025, Mybatis-Flex (fuhai999@gmail.com).\n *  <p>\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *  <p>\n *  http://www.apache.org/licenses/LICENSE-2.0\n *  <p>\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\npackage com.mybatisflex.core.util;\n\n\nimport com.mybatisflex.core.exception.FlexExceptions;\n\nimport java.util.Collection;\nimport java.util.Locale;\nimport java.util.function.Function;\nimport java.util.regex.Pattern;\n\npublic class StringUtil {\n\n    private StringUtil() {\n    }\n\n    /**\n     * @see org.apache.ibatis.reflection.property.PropertyNamer#methodToProperty(String)\n     */\n    public static String methodToProperty(String name) {\n        if (name.startsWith(\"is\")) {\n            name = name.substring(2);\n        } else if (name.startsWith(\"get\") || name.startsWith(\"set\")) {\n            name = name.substring(3);\n        } else {\n            throw FlexExceptions.wrap(\"Error parsing property name '%s'.  Didn't start with 'is', 'get' or 'set'.\", name);\n        }\n        if (!name.isEmpty()) {\n            name = name.substring(0, 1).toLowerCase(Locale.ENGLISH).concat(name.substring(1));\n        }\n        return name;\n    }\n\n\n    /**\n     * \n     *\n     * @param string\n     */\n    public static String firstCharToLowerCase(String string) {\n        char firstChar = string.charAt(0);\n        if (firstChar >= 'A' && firstChar <= 'Z') {\n            char[] chars = string.toCharArray();\n            chars[0] += ('a' - 'A');\n            return new String(chars);\n        }\n        return string;\n    }\n\n\n    /**\n     * \n     *\n     * @param string\n     */\n    public static String firstCharToUpperCase(String string) {\n        char firstChar = string.charAt(0);\n        if (firstChar >= 'a' && firstChar <= 'z') {\n            char[] chars = string.toCharArray();\n            chars[0] -= ('a' - 'A');\n            return new String(chars);\n        }\n        return string;\n    }\n\n\n    /**\n     * \n     *\n     * @param string\n     */\n    public static String camelToUnderline(String string) {\n        if (noText(string)) {\n            return \"\";\n        }\n        int strLen = string.length();\n        StringBuilder sb = new StringBuilder(strLen);\n        for (int i = 0; i < strLen; i++) {\n            char c = string.charAt(i);\n            if (Character.isUpperCase(c) && i > 0) {\n                char prev = string.charAt(i - 1);\n                if (!Character.isUpperCase(prev) && prev != '_') {\n                    sb.append('_');\n                }\n            }\n            sb.append(Character.toLowerCase(c));\n        }\n        return sb.toString();\n    }",
    "func": "    public static String underlineToCamel(String string) {\n        if (noText(string)) {\n            return \"\";\n        }\n        if (Character.isUpperCase(string.charAt(0))) {\n            string = string.toLowerCase();\n        }\n        int strLen = string.length();\n        StringBuilder sb = new StringBuilder(strLen);\n        for (int i = 0; i < strLen; i++) {\n            char c = string.charAt(i);\n            if (c == '_') {\n                if (++i < strLen) {\n                    sb.append(Character.toUpperCase(string.charAt(i)));\n                }\n            } else {\n                sb.append(c);\n            }\n        }\n        return sb.toString();\n    }",
    "comment": "    /**\n     * \n     *\n     * @param string\n     */",
    "test_funcs": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java::underlineToCamel",
    "test_class": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java::StringUtilTest",
    "func_start": 110,
    "func_end": 130,
    "body_len": 20,
    "instruction": "Write a function `underlineToCamel(String string)` that converts a string from underscore notation to camelCase. The function should handle empty or null input by returning an empty string. If the first character is uppercase, it should be converted to lowercase before processing. Each underscore in the string should be removed, and the character immediately following it should be converted to uppercase. The function should return the resulting camelCase string.",
    "func_name_with_file_path": "mybatis-flex-core/src/main/java/com/mybatisflex/core/util/StringUtil.java::underlineToCamel",
    "test_funcs_split": [
      "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java::underlineToCamel"
    ],
    "test_start": 26,
    "test_end": 36,
    "test_file": "mybatis-flex-core/src/test/java/com/mybatisflex/coretest/StringUtilTest.java",
    "test_instruction": "mvn test -pl mybatis-flex-core -Dtest=\"com.mybatisflex.coretest.StringUtilTest#underlineToCamel\"",
    "test_code": "    @Test\n    public void underlineToCamel() {\n        String testString1 = \"aa_bb_\";\n        String underlineToCamel = StringUtil.underlineToCamel(testString1);\n        System.out.println(underlineToCamel);\n        Assert.assertEquals(\"aaBb\", underlineToCamel);\n\n        String underline = StringUtil.camelToUnderline(underlineToCamel);\n        System.out.println(underline);\n        Assert.assertEquals(\"aa_bb\", underline);\n    }"
  },
  {
    "task-id": "spring-ai-models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiAudioSpeechModel.java-stream",
    "project": "spring-ai",
    "file_path": "models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiAudioSpeechModel.java",
    "func_name": "stream",
    "context": "/*\n * Copyright 2023-2025 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.openai;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport reactor.core.publisher.Flux;\n\nimport org.springframework.ai.chat.metadata.RateLimit;\nimport org.springframework.ai.openai.api.OpenAiAudioApi;\nimport org.springframework.ai.openai.api.OpenAiAudioApi.SpeechRequest.AudioResponseFormat;\nimport org.springframework.ai.openai.audio.speech.Speech;\nimport org.springframework.ai.openai.audio.speech.SpeechModel;\nimport org.springframework.ai.openai.audio.speech.SpeechPrompt;\nimport org.springframework.ai.openai.audio.speech.SpeechResponse;\nimport org.springframework.ai.openai.audio.speech.StreamingSpeechModel;\nimport org.springframework.ai.openai.metadata.audio.OpenAiAudioSpeechResponseMetadata;\nimport org.springframework.ai.openai.metadata.support.OpenAiResponseHeaderExtractor;\nimport org.springframework.ai.retry.RetryUtils;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.retry.support.RetryTemplate;\nimport org.springframework.util.Assert;\nimport org.springframework.util.StringUtils;\n\n/**\n * OpenAI audio speech client implementation for backed by {@link OpenAiAudioApi}.\n *\n * @author Ahmed Yousri\n * @author Hyunjoon Choi\n * @author Thomas Vitale\n * @author Jonghoon Park\n * @see OpenAiAudioApi\n * @since 1.0.0-M1\n */\npublic class OpenAiAudioSpeechModel implements SpeechModel, StreamingSpeechModel {\n\n\t/**\n\t * The speed of the default voice synthesis.\n\t * @see OpenAiAudioSpeechOptions\n\t */\n\tprivate static final Float SPEED = 1.0f;\n\n\tprivate final Logger logger = LoggerFactory.getLogger(getClass());\n\n\t/**\n\t * The default options used for the audio completion requests.\n\t */\n\tprivate final OpenAiAudioSpeechOptions defaultOptions;\n\n\t/**\n\t * The retry template used to retry the OpenAI Audio API calls.\n\t */\n\tprivate final RetryTemplate retryTemplate;\n\n\t/**\n\t * Low-level access to the OpenAI Audio API.\n\t */\n\tprivate final OpenAiAudioApi audioApi;\n\n\t/**\n\t * Initializes a new instance of the OpenAiAudioSpeechModel class with the provided\n\t * OpenAiAudioApi. It uses the model tts-1, response format mp3, voice alloy, and the\n\t * default speed of 1.0.\n\t * @param audioApi The OpenAiAudioApi to use for speech synthesis.\n\t */\n\tpublic OpenAiAudioSpeechModel(OpenAiAudioApi audioApi) {\n\t\tthis(audioApi,\n\t\t\t\tOpenAiAudioSpeechOptions.builder()\n\t\t\t\t\t.model(OpenAiAudioApi.TtsModel.TTS_1.getValue())\n\t\t\t\t\t.responseFormat(AudioResponseFormat.MP3)\n\t\t\t\t\t.voice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY.getValue())\n\t\t\t\t\t.speed(SPEED)\n\t\t\t\t\t.build());\n\t}\n\n\t/**\n\t * Initializes a new instance of the OpenAiAudioSpeechModel class with the provided\n\t * OpenAiAudioApi and options.\n\t * @param audioApi The OpenAiAudioApi to use for speech synthesis.\n\t * @param options The OpenAiAudioSpeechOptions containing the speech synthesis\n\t * options.\n\t */\n\tpublic OpenAiAudioSpeechModel(OpenAiAudioApi audioApi, OpenAiAudioSpeechOptions options) {\n\t\tthis(audioApi, options, RetryUtils.DEFAULT_RETRY_TEMPLATE);\n\t}\n\n\t/**\n\t * Initializes a new instance of the OpenAiAudioSpeechModel class with the provided\n\t * OpenAiAudioApi and options.\n\t * @param audioApi The OpenAiAudioApi to use for speech synthesis.\n\t * @param options The OpenAiAudioSpeechOptions containing the speech synthesis\n\t * options.\n\t * @param retryTemplate The retry template.\n\t */\n\tpublic OpenAiAudioSpeechModel(OpenAiAudioApi audioApi, OpenAiAudioSpeechOptions options,\n\t\t\tRetryTemplate retryTemplate) {\n\t\tAssert.notNull(audioApi, \"OpenAiAudioApi must not be null\");\n\t\tAssert.notNull(options, \"OpenAiSpeechOptions must not be null\");\n\t\tAssert.notNull(options, \"RetryTemplate must not be null\");\n\t\tthis.audioApi = audioApi;\n\t\tthis.defaultOptions = options;\n\t\tthis.retryTemplate = retryTemplate;\n\t}\n\n\t@Override\n\tpublic byte[] call(String text) {\n\t\tSpeechPrompt speechRequest = new SpeechPrompt(text);\n\t\treturn call(speechRequest).getResult().getOutput();\n\t}\n\n\t@Override\n\tpublic SpeechResponse call(SpeechPrompt speechPrompt) {\n\n\t\tOpenAiAudioApi.SpeechRequest speechRequest = createRequest(speechPrompt);\n\n\t\tResponseEntity<byte[]> speechEntity = this.retryTemplate\n\t\t\t.execute(ctx -> this.audioApi.createSpeech(speechRequest));\n\n\t\tvar speech = speechEntity.getBody();\n\n\t\tif (speech == null) {\n\t\t\tlogger.warn(\"No speech response returned for speechRequest: {}\", speechRequest);\n\t\t\treturn new SpeechResponse(new Speech(new byte[0]));\n\t\t}\n\n\t\tRateLimit rateLimits = OpenAiResponseHeaderExtractor.extractAiResponseHeaders(speechEntity);\n\n\t\treturn new SpeechResponse(new Speech(speech), new OpenAiAudioSpeechResponseMetadata(rateLimits));\n\t}",
    "func": "\t@Override\n\tpublic Flux<SpeechResponse> stream(SpeechPrompt speechPrompt) {\n\n\t\tOpenAiAudioApi.SpeechRequest speechRequest = createRequest(speechPrompt);\n\n\t\tFlux<ResponseEntity<byte[]>> speechEntity = this.retryTemplate\n\t\t\t.execute(ctx -> this.audioApi.stream(speechRequest));\n\n\t\treturn speechEntity.map(entity -> new SpeechResponse(new Speech(entity.getBody()),\n\t\t\t\tnew OpenAiAudioSpeechResponseMetadata(OpenAiResponseHeaderExtractor.extractAiResponseHeaders(entity))));\n\t}",
    "comment": "\t/**\n\t * Streams the audio response for the given speech prompt.\n\t * @param speechPrompt The speech prompt containing the text and options for speech\n\t * synthesis.\n\t * @return A Flux of SpeechResponse objects containing the streamed audio and\n\t * metadata.\n\t */",
    "test_funcs": "models/spring-ai-openai/src/test/java/org/springframework/ai/openai/ChatCompletionRequestTests.java::createRequestWithChatOptions spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/openai/OpenAiAutoConfigurationIT.java::generateStreaming models/spring-ai-openai/src/test/java/org/springframework/ai/openai/ChatCompletionRequestTests.java::promptOptionsTools models/spring-ai-openai/src/test/java/org/springframework/ai/openai/ChatCompletionRequestTests.java::defaultOptionsTools",
    "test_class": "models/spring-ai-openai/src/test/java/org/springframework/ai/openai/ChatCompletionRequestTests.java::ChatCompletionRequestTests spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/openai/OpenAiAutoConfigurationIT.java::OpenAiAutoConfigurationIT",
    "func_start": 152,
    "func_end": 162,
    "body_len": 10,
    "instruction": "Write a function `stream(SpeechPrompt speechPrompt)` that takes a speech prompt containing text and synthesis options, converts it into a speech request using a helper method, streams the audio response from an external API with retry logic, and returns a Flux of SpeechResponse objects where each response contains the audio data and extracted metadata from the API headers.",
    "func_name_with_file_path": "models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiAudioSpeechModel.java::stream",
    "test_funcs_split": [
      "models/spring-ai-openai/src/test/java/org/springframework/ai/openai/ChatCompletionRequestTests.java::createRequestWithChatOptions",
      "spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/openai/OpenAiAutoConfigurationIT.java::generateStreaming",
      "models/spring-ai-openai/src/test/java/org/springframework/ai/openai/ChatCompletionRequestTests.java::promptOptionsTools",
      "models/spring-ai-openai/src/test/java/org/springframework/ai/openai/ChatCompletionRequestTests.java::defaultOptionsTools"
    ],
    "test_start": 76,
    "test_end": 101,
    "test_file": "models/spring-ai-openai/src/test/java/org/springframework/ai/openai/ChatCompletionRequestTests.java",
    "test_instruction": "mvn test -pl models/spring-ai-openai -Dtest=\"org.springframework.ai.openai.ChatCompletionRequestTests#createRequestWithChatOptions\"",
    "test_code": "\t@Test\n\tvoid createRequestWithChatOptions() {\n\t\tvar client = OpenAiChatModel.builder()\n\t\t\t.openAiApi(OpenAiApi.builder().apiKey(\"TEST\").build())\n\t\t\t.defaultOptions(OpenAiChatOptions.builder().model(\"DEFAULT_MODEL\").temperature(66.6).build())\n\t\t\t.build();\n\n\t\tvar prompt = client.buildRequestPrompt(new Prompt(\"Test message content\"));\n\n\t\tvar request = client.createRequest(prompt, false);\n\n\t\tassertThat(request.messages()).hasSize(1);\n\t\tassertThat(request.stream()).isFalse();\n\n\t\tassertThat(request.model()).isEqualTo(\"DEFAULT_MODEL\");\n\t\tassertThat(request.temperature()).isEqualTo(66.6);\n\n\t\trequest = client.createRequest(new Prompt(\"Test message content\",\n\t\t\t\tOpenAiChatOptions.builder().model(\"PROMPT_MODEL\").temperature(99.9).build()), true);\n\n\t\tassertThat(request.messages()).hasSize(1);\n\t\tassertThat(request.stream()).isTrue();\n\n\t\tassertThat(request.model()).isEqualTo(\"PROMPT_MODEL\");\n\t\tassertThat(request.temperature()).isEqualTo(99.9);\n\t}"
  },
  {
    "task-id": "spring-ai-models/spring-ai-postgresml/src/main/java/org/springframework/ai/postgresml/PostgresMlEmbeddingModel.java-mergeOptions",
    "project": "spring-ai",
    "file_path": "models/spring-ai-postgresml/src/main/java/org/springframework/ai/postgresml/PostgresMlEmbeddingModel.java",
    "func_name": "mergeOptions",
    "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.postgresml;\n\nimport java.sql.Array;\nimport java.sql.PreparedStatement;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Map;\n\nimport org.springframework.ai.chat.metadata.EmptyUsage;\nimport org.springframework.ai.document.Document;\nimport org.springframework.ai.embedding.AbstractEmbeddingModel;\nimport org.springframework.ai.embedding.Embedding;\nimport org.springframework.ai.embedding.EmbeddingOptions;\nimport org.springframework.ai.embedding.EmbeddingRequest;\nimport org.springframework.ai.embedding.EmbeddingResponse;\nimport org.springframework.ai.embedding.EmbeddingResponseMetadata;\nimport org.springframework.ai.model.EmbeddingUtils;\nimport org.springframework.ai.model.ModelOptionsUtils;\nimport org.springframework.beans.factory.InitializingBean;\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport org.springframework.jdbc.core.RowMapper;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.StringUtils;\n\n/**\n * <a href=\"https://postgresml.org\">PostgresML</a> EmbeddingModel\n *\n * @author Toshiaki Maki\n * @author Christian Tzolov\n */\npublic class PostgresMlEmbeddingModel extends AbstractEmbeddingModel implements InitializingBean {\n\n\tpublic static final String DEFAULT_TRANSFORMER_MODEL = \"distilbert-base-uncased\";\n\n\tprivate final PostgresMlEmbeddingOptions defaultOptions;\n\n\tprivate final JdbcTemplate jdbcTemplate;\n\n\tprivate final boolean createExtension;\n\n\t/**\n\t * a constructor\n\t * @param jdbcTemplate JdbcTemplate\n\t */\n\tpublic PostgresMlEmbeddingModel(JdbcTemplate jdbcTemplate) {\n\t\tthis(jdbcTemplate, PostgresMlEmbeddingOptions.builder().build(), false);\n\t}\n\n\tpublic PostgresMlEmbeddingModel(JdbcTemplate jdbcTemplate, PostgresMlEmbeddingOptions options) {\n\t\tthis(jdbcTemplate, options, false);\n\t}\n\n\t/**\n\t * a PostgresMlEmbeddingModel constructor\n\t * @param jdbcTemplate JdbcTemplate to use to interact with the database.\n\t * @param options PostgresMlEmbeddingOptions to configure the client.\n\t */\n\tpublic PostgresMlEmbeddingModel(JdbcTemplate jdbcTemplate, PostgresMlEmbeddingOptions options,\n\t\t\tboolean createExtension) {\n\t\tAssert.notNull(jdbcTemplate, \"jdbc template must not be null.\");\n\t\tAssert.notNull(options, \"options must not be null.\");\n\t\tAssert.notNull(options.getTransformer(), \"transformer must not be null.\");\n\t\tAssert.notNull(options.getVectorType(), \"vectorType must not be null.\");\n\t\tAssert.notNull(options.getKwargs(), \"kwargs must not be null.\");\n\t\tAssert.notNull(options.getMetadataMode(), \"metadataMode must not be null.\");\n\n\t\tthis.jdbcTemplate = jdbcTemplate;\n\t\tthis.defaultOptions = options;\n\t\tthis.createExtension = createExtension;\n\t}\n\n\t@SuppressWarnings(\"null\")\n\t@Override\n\tpublic float[] embed(String text) {\n\t\treturn this.jdbcTemplate.queryForObject(\n\t\t\t\t\"SELECT pgml.embed(?, ?, ?::JSONB)\" + this.defaultOptions.getVectorType().cast + \" AS embedding\",\n\t\t\t\tthis.defaultOptions.getVectorType().rowMapper, this.defaultOptions.getTransformer(), text,\n\t\t\t\tModelOptionsUtils.toJsonString(this.defaultOptions.getKwargs()));\n\t}\n\n\t@Override\n\tpublic float[] embed(Document document) {\n\t\treturn this.embed(document.getFormattedContent(this.defaultOptions.getMetadataMode()));\n\t}\n\n\t@SuppressWarnings(\"null\")\n\t@Override\n\tpublic EmbeddingResponse call(EmbeddingRequest request) {\n\n\t\tfinal PostgresMlEmbeddingOptions optionsToUse = this.mergeOptions(request.getOptions());\n\n\t\tList<Embedding> data = new ArrayList<>();\n\t\tList<float[]> embed = List.of();\n\n\t\tList<String> texts = request.getInstructions();\n\t\tif (!CollectionUtils.isEmpty(texts)) {\n\t\t\tembed = this.jdbcTemplate.query(connection -> {\n\t\t\t\tPreparedStatement preparedStatement = connection.prepareStatement(\"SELECT pgml.embed(?, text, ?::JSONB)\"\n\t\t\t\t\t\t+ optionsToUse.getVectorType().cast + \" AS embedding FROM (SELECT unnest(?) AS text) AS texts\");\n\t\t\t\tpreparedStatement.setString(1, optionsToUse.getTransformer());\n\t\t\t\tpreparedStatement.setString(2, ModelOptionsUtils.toJsonString(optionsToUse.getKwargs()));\n\t\t\t\tpreparedStatement.setArray(3, connection.createArrayOf(\"TEXT\", texts.toArray(Object[]::new)));\n\t\t\t\treturn preparedStatement;\n\t\t\t}, rs -> {\n\t\t\t\tList<float[]> result = new ArrayList<>();\n\t\t\t\twhile (rs.next()) {\n\t\t\t\t\tresult.add(optionsToUse.getVectorType().rowMapper.mapRow(rs, -1));\n\t\t\t\t}\n\t\t\t\treturn result;\n\t\t\t});\n\t\t}\n\n\t\tif (!CollectionUtils.isEmpty(embed)) {\n\t\t\tfor (int i = 0; i < embed.size(); i++) {\n\t\t\t\tdata.add(new Embedding(embed.get(i), i));\n\t\t\t}\n\t\t}\n\n\t\tMap<String, Object> embeddingMetadata = Map.of(\"transformer\", optionsToUse.getTransformer(), \"vector-type\",\n\t\t\t\toptionsToUse.getVectorType().name(), \"kwargs\",\n\t\t\t\tModelOptionsUtils.toJsonString(optionsToUse.getKwargs()));\n\t\tvar embeddingResponseMetadata = new EmbeddingResponseMetadata(\"unknown\", new EmptyUsage(), embeddingMetadata);\n\t\treturn new EmbeddingResponse(data, embeddingResponseMetadata);\n\t}",
    "func": "\tPostgresMlEmbeddingOptions mergeOptions(EmbeddingOptions requestOptions) {\n\n\t\tPostgresMlEmbeddingOptions options = (this.defaultOptions != null) ? this.defaultOptions\n\t\t\t\t: PostgresMlEmbeddingOptions.builder().build();\n\n\t\tif (requestOptions != null) {\n\t\t\toptions = ModelOptionsUtils.merge(requestOptions, options, PostgresMlEmbeddingOptions.class);\n\t\t}\n\n\t\treturn options;\n\t}",
    "comment": "\t/**\n\t * Merge the default and request options.\n\t * @param requestOptions request options to merge.\n\t * @return the merged options.\n\t */",
    "test_funcs": "models/spring-ai-postgresml/src/test/java/org/springframework/ai/postgresml/PostgresMlEmbeddingOptionsTests.java::mergeOptions",
    "test_class": "models/spring-ai-postgresml/src/test/java/org/springframework/ai/postgresml/PostgresMlEmbeddingOptionsTests.java::PostgresMlEmbeddingOptionsTests",
    "func_start": 149,
    "func_end": 159,
    "body_len": 10,
    "instruction": "Write a function `mergeOptions(EmbeddingOptions requestOptions)` that takes request-specific embedding options and merges them with the default options. If no default options are set, use a newly created instance with default values. The function should return a `PostgresMlEmbeddingOptions` object containing the combined settings, where request options override the defaults. Use `ModelOptionsUtils.merge` to perform the actual merging, ensuring type safety with `PostgresMlEmbeddingOptions.class`.",
    "func_name_with_file_path": "models/spring-ai-postgresml/src/main/java/org/springframework/ai/postgresml/PostgresMlEmbeddingModel.java::mergeOptions",
    "test_funcs_split": [
      "models/spring-ai-postgresml/src/test/java/org/springframework/ai/postgresml/PostgresMlEmbeddingOptionsTests.java::mergeOptions"
    ],
    "test_start": 59,
    "test_end": 96,
    "test_file": "models/spring-ai-postgresml/src/test/java/org/springframework/ai/postgresml/PostgresMlEmbeddingOptionsTests.java",
    "test_instruction": "mvn test -pl models/spring-ai-postgresml -Dtest=\"org.springframework.ai.postgresml.PostgresMlEmbeddingOptionsTests#mergeOptions\"",
    "test_code": "\t@Test\n\tpublic void mergeOptions() {\n\n\t\tvar jdbcTemplate = Mockito.mock(JdbcTemplate.class);\n\t\tPostgresMlEmbeddingModel embeddingModel = new PostgresMlEmbeddingModel(jdbcTemplate);\n\n\t\tPostgresMlEmbeddingOptions options = embeddingModel.mergeOptions(EmbeddingOptionsBuilder.builder().build());\n\n\t\t// Default options\n\t\tassertThat(options.getTransformer()).isEqualTo(PostgresMlEmbeddingModel.DEFAULT_TRANSFORMER_MODEL);\n\t\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY);\n\t\tassertThat(options.getKwargs()).isEqualTo(Map.of());\n\t\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.EMBED);\n\n\t\t// Partial override\n\t\toptions = embeddingModel.mergeOptions(PostgresMlEmbeddingOptions.builder()\n\t\t\t.transformer(\"intfloat/e5-small\")\n\t\t\t.kwargs(Map.of(\"device\", \"cpu\"))\n\t\t\t.build());\n\n\t\tassertThat(options.getTransformer()).isEqualTo(\"intfloat/e5-small\");\n\t\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_ARRAY); // Default\n\t\tassertThat(options.getKwargs()).isEqualTo(Map.of(\"device\", \"cpu\"));\n\t\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.EMBED); // Default\n\n\t\t// Complete override\n\t\toptions = embeddingModel.mergeOptions(PostgresMlEmbeddingOptions.builder()\n\t\t\t.transformer(\"intfloat/e5-small\")\n\t\t\t.vectorType(PostgresMlEmbeddingModel.VectorType.PG_VECTOR)\n\t\t\t.metadataMode(org.springframework.ai.document.MetadataMode.ALL)\n\t\t\t.kwargs(Map.of(\"device\", \"cpu\"))\n\t\t\t.build());\n\n\t\tassertThat(options.getTransformer()).isEqualTo(\"intfloat/e5-small\");\n\t\tassertThat(options.getVectorType()).isEqualTo(PostgresMlEmbeddingModel.VectorType.PG_VECTOR);\n\t\tassertThat(options.getKwargs()).isEqualTo(Map.of(\"device\", \"cpu\"));\n\t\tassertThat(options.getMetadataMode()).isEqualTo(org.springframework.ai.document.MetadataMode.ALL);\n\t}"
  },
  {
    "task-id": "spring-ai-spring-ai-model/src/main/java/org/springframework/ai/embedding/AbstractEmbeddingModel.java-dimensions",
    "project": "spring-ai",
    "file_path": "spring-ai-model/src/main/java/org/springframework/ai/embedding/AbstractEmbeddingModel.java",
    "func_name": "dimensions",
    "context": "/*\n * Copyright 2023-2025 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.embedding;\n\nimport java.io.IOException;\nimport java.util.Map;\nimport java.util.Properties;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.stream.Collectors;\n\nimport org.springframework.aot.hint.RuntimeHints;\nimport org.springframework.aot.hint.RuntimeHintsRegistrar;\nimport org.springframework.context.annotation.ImportRuntimeHints;\nimport org.springframework.core.io.ClassPathResource;\nimport org.springframework.core.io.Resource;\nimport org.springframework.util.Assert;\n\n/**\n * Abstract implementation of the {@link EmbeddingModel} interface that provides\n * dimensions calculation caching.\n *\n * @author Christian Tzolov\n * @author Josh Long\n */\n@ImportRuntimeHints(AbstractEmbeddingModel.Hints.class)\npublic abstract class AbstractEmbeddingModel implements EmbeddingModel {\n\n\tprivate static final Resource EMBEDDING_MODEL_DIMENSIONS_PROPERTIES = new ClassPathResource(\n\t\t\t\"/embedding/embedding-model-dimensions.properties\");\n\n\tprivate static final Map<String, Integer> KNOWN_EMBEDDING_DIMENSIONS = loadKnownModelDimensions();\n\n\t/**\n\t * Cached embedding dimensions.\n\t */\n\tprotected final AtomicInteger embeddingDimensions = new AtomicInteger(-1);",
    "func": "\tpublic static int dimensions(EmbeddingModel embeddingModel, String modelName, String dummyContent) {\n\n\t\tif (KNOWN_EMBEDDING_DIMENSIONS.containsKey(modelName)) {\n\t\t\t// Retrieve the dimension from a pre-configured file.\n\t\t\treturn KNOWN_EMBEDDING_DIMENSIONS.get(modelName);\n\t\t}\n\t\telse {\n\t\t\t// Determine the dimensions empirically.\n\t\t\t// Generate an embedding and count the dimension size;\n\t\t\treturn embeddingModel.embed(dummyContent).length;\n\t\t}\n\t}",
    "comment": "\t/**\n\t * Return the dimension of the requested embedding generative name. If the generative\n\t * name is unknown uses the EmbeddingModel to perform a dummy EmbeddingModel#embed and\n\t * count the response dimensions.\n\t * @param embeddingModel Fall-back client to determine, empirically the dimensions.\n\t * @param modelName Embedding generative name to retrieve the dimensions for.\n\t * @param dummyContent Dummy content to use for the empirical dimension calculation.\n\t * @return Returns the embedding dimensions for the modelName.\n\t */",
    "test_funcs": "spring-ai-model/src/test/java/org/springframework/ai/embedding/AbstractEmbeddingModelTests.java::testDefaultMethodImplementation spring-ai-model/src/test/java/org/springframework/ai/embedding/AbstractEmbeddingModelTests.java::testUnknownModelDimension spring-ai-model/src/test/java/org/springframework/ai/embedding/AbstractEmbeddingModelTests.java::testKnownEmbeddingModelDimensions",
    "test_class": "spring-ai-model/src/test/java/org/springframework/ai/embedding/AbstractEmbeddingModelTests.java::AbstractEmbeddingModelTests",
    "func_start": 61,
    "func_end": 72,
    "body_len": 11,
    "instruction": "Write a function `dimensions` that takes an `EmbeddingModel`, a model name as a string, and dummy content as a string, and returns the dimension size of the embedding for that model. If the model name is found in a predefined map of known dimensions, return the associated dimension value. Otherwise, use the provided `EmbeddingModel` to generate an embedding from the dummy content and return the length of the resulting embedding array as the dimension count.",
    "func_name_with_file_path": "spring-ai-model/src/main/java/org/springframework/ai/embedding/AbstractEmbeddingModel.java::dimensions",
    "test_funcs_split": [
      "spring-ai-model/src/test/java/org/springframework/ai/embedding/AbstractEmbeddingModelTests.java::testDefaultMethodImplementation",
      "spring-ai-model/src/test/java/org/springframework/ai/embedding/AbstractEmbeddingModelTests.java::testUnknownModelDimension",
      "spring-ai-model/src/test/java/org/springframework/ai/embedding/AbstractEmbeddingModelTests.java::testKnownEmbeddingModelDimensions"
    ],
    "test_start": 46,
    "test_end": 78,
    "test_file": "spring-ai-model/src/test/java/org/springframework/ai/embedding/AbstractEmbeddingModelTests.java",
    "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.embedding.AbstractEmbeddingModelTests#testDefaultMethodImplementation\"",
    "test_code": "\t@Test\n\tpublic void testDefaultMethodImplementation() {\n\n\t\tEmbeddingModel dummy = new EmbeddingModel() {\n\n\t\t\t@Override\n\t\t\tpublic float[] embed(String text) {\n\t\t\t\treturn new float[] { 0.1f, 0.1f, 0.1f };\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic float[] embed(Document document) {\n\t\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'embed'\");\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic List<float[]> embed(List<String> texts) {\n\t\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'embed'\");\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic EmbeddingResponse embedForResponse(List<String> texts) {\n\t\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'embedForResponse'\");\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic EmbeddingResponse call(EmbeddingRequest request) {\n\t\t\t\tthrow new UnsupportedOperationException(\"Unimplemented method 'call'\");\n\t\t\t}\n\t\t};\n\n\t\tassertThat(dummy.dimensions()).isEqualTo(3);\n\t}"
  },
  {
    "task-id": "spring-ai-spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java-merge",
    "project": "spring-ai",
    "file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java",
    "func_name": "merge",
    "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.model;\n\nimport java.beans.PropertyDescriptor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Type;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.core.type.TypeReference;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport com.fasterxml.jackson.databind.cfg.CoercionAction;\nimport com.fasterxml.jackson.databind.cfg.CoercionInputShape;\nimport com.fasterxml.jackson.databind.json.JsonMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport com.github.victools.jsonschema.module.jackson.JacksonModule;\nimport com.github.victools.jsonschema.module.jackson.JacksonOption;\nimport com.github.victools.jsonschema.module.swagger2.Swagger2Module;\n\nimport org.springframework.ai.util.JacksonUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.core.KotlinDetector;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.ObjectUtils;\n\n/**\n * Utility class for manipulating {@link ModelOptions} objects.\n *\n * @author Christian Tzolov\n * @author Thomas Vitale\n * @since 0.8.0\n */\npublic abstract class ModelOptionsUtils {\n\n\tpublic static final ObjectMapper OBJECT_MAPPER = JsonMapper.builder()\n\t\t.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n\t\t.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS)\n\t\t.addModules(JacksonUtils.instantiateAvailableModules())\n\t\t.build()\n\t\t.configure(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT, true);\n\n\tstatic {\n\t\t// Configure coercion for empty strings to null for Enum types\n\t\t// This fixes the issue where empty string finish_reason values cause\n\t\t// deserialization failures\n\t\tOBJECT_MAPPER.coercionConfigFor(Enum.class).setCoercion(CoercionInputShape.EmptyString, CoercionAction.AsNull);\n\t}\n\n\tprivate static final List<String> BEAN_MERGE_FIELD_EXCISIONS = List.of(\"class\");\n\n\tprivate static final ConcurrentHashMap<Class<?>, List<String>> REQUEST_FIELD_NAMES_PER_CLASS = new ConcurrentHashMap<>();\n\n\tprivate static final AtomicReference<SchemaGenerator> SCHEMA_GENERATOR_CACHE = new AtomicReference<>();\n\n\tprivate static TypeReference<HashMap<String, Object>> MAP_TYPE_REF = new TypeReference<>() {\n\n\t};\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using the default\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json) {\n\t\treturn jsonToMap(json, OBJECT_MAPPER);\n\t}\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using a custom\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @param objectMapper the ObjectMapper to use for deserialization.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json, ObjectMapper objectMapper) {\n\t\ttry {\n\t\t\treturn objectMapper.readValue(json, MAP_TYPE_REF);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given JSON string to an Object of the given type.\n\t * @param <T> the type of the object to return.\n\t * @param json the JSON string to convert to an object.\n\t * @param type the type of the object to return.\n\t * @return Object instance of the given type.\n\t */\n\tpublic static <T> T jsonToObject(String json, Class<T> type) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.readValue(json, type);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Failed to json: \" + json, e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonString(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonStringPrettyPrinter(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writerWithDefaultPrettyPrinter().writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}",
    "func": "\tpublic static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n\n\t\tif (source == null) {\n\t\t\tsource = Map.of();\n\t\t}\n\n\t\tList<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n\t\t\t\t? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n\t\t\t\t: acceptedFieldNames;\n\n\t\tif (CollectionUtils.isEmpty(requestFieldNames)) {\n\t\t\tthrow new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n\t\t}\n\n\t\tMap<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n\t\tMap<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n\t\ttargetMap.putAll(sourceMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n\t\ttargetMap = targetMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> requestFieldNames.contains(e.getKey()))\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n\t\treturn ModelOptionsUtils.mapToClass(targetMap, clazz);\n\t}",
    "comment": "\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. If the acceptedFieldNames is not empty, only the\n\t * fields with the given names are merged and returned. If the acceptedFieldNames is\n\t * empty, use the {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @param acceptedFieldNames the list of field names accepted for the target object.\n\t * @return the merged object represented by the given class.\n\t */",
    "test_funcs": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::merge",
    "test_class": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::ModelOptionsUtilsTests",
    "func_start": 178,
    "func_end": 206,
    "body_len": 28,
    "instruction": "Write a generic static method `merge` that takes a source object, a target object, a class type, and an optional list of accepted field names. The method should merge the non-null properties from the source object into the target object using JSON property names (as defined by `@JsonProperty`) to match fields. If a list of accepted field names is provided, only those fields should be considered during the merge; otherwise, use the `@JsonProperty` annotations from the given class to determine valid fields. Null values in the source should be ignored and not override target values. The result should be returned as an instance of the specified class, constructed from the merged data. If no valid JSON property fields are found on the class, throw an `IllegalArgumentException`. Use internal utility methods to convert objects to maps, perform the merge, and map back to the target class.",
    "func_name_with_file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java::merge",
    "test_funcs_split": [
      "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::merge"
    ],
    "test_start": 37,
    "test_end": 59,
    "test_file": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java",
    "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.model.ModelOptionsUtilsTests#merge\"",
    "test_code": "\t@Test\n\tpublic void merge() {\n\t\tTestPortableOptionsImpl portableOptions = new TestPortableOptionsImpl();\n\t\tportableOptions.setName(\"John\");\n\t\tportableOptions.setAge(30);\n\t\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\t\tTestSpecificOptions specificOptions = new TestSpecificOptions();\n\t\tspecificOptions.setName(\"Mike\");\n\t\tspecificOptions.setSpecificField(\"SpecificField\");\n\n\t\tassertThatThrownBy(\n\t\t\t\t() -> ModelOptionsUtils.merge(portableOptions, specificOptions, TestPortableOptionsImpl.class))\n\t\t\t.isInstanceOf(IllegalArgumentException.class)\n\t\t\t.hasMessageContaining(\"No @JsonProperty fields found in the \");\n\n\t\tvar specificOptions2 = ModelOptionsUtils.merge(portableOptions, specificOptions, TestSpecificOptions.class);\n\n\t\tassertThat(specificOptions2.getAge()).isEqualTo(30);\n\t\tassertThat(specificOptions2.getName()).isEqualTo(\"John\"); // !!! Overridden by the\n\t\t// portableOptions\n\t\tassertThat(specificOptions2.getSpecificField()).isEqualTo(\"SpecificField\");\n\t}"
  },
  {
    "task-id": "spring-ai-spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java-objectToMap",
    "project": "spring-ai",
    "file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java",
    "func_name": "objectToMap",
    "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.model;\n\nimport java.beans.PropertyDescriptor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Type;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.core.type.TypeReference;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport com.fasterxml.jackson.databind.cfg.CoercionAction;\nimport com.fasterxml.jackson.databind.cfg.CoercionInputShape;\nimport com.fasterxml.jackson.databind.json.JsonMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport com.github.victools.jsonschema.module.jackson.JacksonModule;\nimport com.github.victools.jsonschema.module.jackson.JacksonOption;\nimport com.github.victools.jsonschema.module.swagger2.Swagger2Module;\n\nimport org.springframework.ai.util.JacksonUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.core.KotlinDetector;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.ObjectUtils;\n\n/**\n * Utility class for manipulating {@link ModelOptions} objects.\n *\n * @author Christian Tzolov\n * @author Thomas Vitale\n * @since 0.8.0\n */\npublic abstract class ModelOptionsUtils {\n\n\tpublic static final ObjectMapper OBJECT_MAPPER = JsonMapper.builder()\n\t\t.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n\t\t.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS)\n\t\t.addModules(JacksonUtils.instantiateAvailableModules())\n\t\t.build()\n\t\t.configure(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT, true);\n\n\tstatic {\n\t\t// Configure coercion for empty strings to null for Enum types\n\t\t// This fixes the issue where empty string finish_reason values cause\n\t\t// deserialization failures\n\t\tOBJECT_MAPPER.coercionConfigFor(Enum.class).setCoercion(CoercionInputShape.EmptyString, CoercionAction.AsNull);\n\t}\n\n\tprivate static final List<String> BEAN_MERGE_FIELD_EXCISIONS = List.of(\"class\");\n\n\tprivate static final ConcurrentHashMap<Class<?>, List<String>> REQUEST_FIELD_NAMES_PER_CLASS = new ConcurrentHashMap<>();\n\n\tprivate static final AtomicReference<SchemaGenerator> SCHEMA_GENERATOR_CACHE = new AtomicReference<>();\n\n\tprivate static TypeReference<HashMap<String, Object>> MAP_TYPE_REF = new TypeReference<>() {\n\n\t};\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using the default\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json) {\n\t\treturn jsonToMap(json, OBJECT_MAPPER);\n\t}\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using a custom\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @param objectMapper the ObjectMapper to use for deserialization.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json, ObjectMapper objectMapper) {\n\t\ttry {\n\t\t\treturn objectMapper.readValue(json, MAP_TYPE_REF);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given JSON string to an Object of the given type.\n\t * @param <T> the type of the object to return.\n\t * @param json the JSON string to convert to an object.\n\t * @param type the type of the object to return.\n\t * @return Object instance of the given type.\n\t */\n\tpublic static <T> T jsonToObject(String json, Class<T> type) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.readValue(json, type);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Failed to json: \" + json, e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonString(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonStringPrettyPrinter(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writerWithDefaultPrettyPrinter().writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. If the acceptedFieldNames is not empty, only the\n\t * fields with the given names are merged and returned. If the acceptedFieldNames is\n\t * empty, use the {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @param acceptedFieldNames the list of field names accepted for the target object.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n\n\t\tif (source == null) {\n\t\t\tsource = Map.of();\n\t\t}\n\n\t\tList<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n\t\t\t\t? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n\t\t\t\t: acceptedFieldNames;\n\n\t\tif (CollectionUtils.isEmpty(requestFieldNames)) {\n\t\t\tthrow new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n\t\t}\n\n\t\tMap<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n\t\tMap<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n\t\ttargetMap.putAll(sourceMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n\t\ttargetMap = targetMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> requestFieldNames.contains(e.getKey()))\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n\t\treturn ModelOptionsUtils.mapToClass(targetMap, clazz);\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. Returns the only field names that match the\n\t * {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz) {\n\t\treturn ModelOptionsUtils.merge(source, target, clazz, null);\n\t}",
    "func": "\tpublic static Map<String, Object> objectToMap(Object source) {\n\t\tif (source == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, new TypeReference<Map<String, Object>>() {\n\n\t\t\t})\n\t\t\t\t.entrySet()\n\t\t\t\t.stream()\n\t\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}",
    "comment": "\t/**\n\t * Converts the given object to a Map.\n\t * @param source the object to convert to a Map.\n\t * @return the converted Map.\n\t */",
    "test_funcs": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::objectToMap",
    "test_class": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::ModelOptionsUtilsTests",
    "func_start": 229,
    "func_end": 246,
    "body_len": 17,
    "instruction": "Write a function `objectToMap` that takes an arbitrary Java object as input and converts it into a `Map<String, Object>` by serializing the object to JSON and then deserializing it into a map, while filtering out any key-value pairs where the value is null. If the input object is null, return an empty HashMap. Handle any JSON processing exceptions by wrapping them in a RuntimeException.",
    "func_name_with_file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java::objectToMap",
    "test_funcs_split": [
      "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::objectToMap"
    ],
    "test_start": 61,
    "test_end": 73,
    "test_file": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java",
    "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.model.ModelOptionsUtilsTests#objectToMap\"",
    "test_code": "\t@Test\n\tpublic void objectToMap() {\n\t\tTestPortableOptionsImpl portableOptions = new TestPortableOptionsImpl();\n\t\tportableOptions.setName(\"John\");\n\t\tportableOptions.setAge(30);\n\t\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\t\tMap<String, Object> map = ModelOptionsUtils.objectToMap(portableOptions);\n\n\t\tassertThat(map).containsEntry(\"name\", \"John\");\n\t\tassertThat(map).containsEntry(\"age\", 30);\n\t\tassertThat(map).containsEntry(\"nonInterfaceField\", \"NonInterfaceField\");\n\t}"
  },
  {
    "task-id": "spring-ai-spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java-copyToTarget",
    "project": "spring-ai",
    "file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java",
    "func_name": "copyToTarget",
    "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.model;\n\nimport java.beans.PropertyDescriptor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Type;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.core.type.TypeReference;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport com.fasterxml.jackson.databind.cfg.CoercionAction;\nimport com.fasterxml.jackson.databind.cfg.CoercionInputShape;\nimport com.fasterxml.jackson.databind.json.JsonMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport com.github.victools.jsonschema.module.jackson.JacksonModule;\nimport com.github.victools.jsonschema.module.jackson.JacksonOption;\nimport com.github.victools.jsonschema.module.swagger2.Swagger2Module;\n\nimport org.springframework.ai.util.JacksonUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.core.KotlinDetector;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.ObjectUtils;\n\n/**\n * Utility class for manipulating {@link ModelOptions} objects.\n *\n * @author Christian Tzolov\n * @author Thomas Vitale\n * @since 0.8.0\n */\npublic abstract class ModelOptionsUtils {\n\n\tpublic static final ObjectMapper OBJECT_MAPPER = JsonMapper.builder()\n\t\t.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n\t\t.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS)\n\t\t.addModules(JacksonUtils.instantiateAvailableModules())\n\t\t.build()\n\t\t.configure(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT, true);\n\n\tstatic {\n\t\t// Configure coercion for empty strings to null for Enum types\n\t\t// This fixes the issue where empty string finish_reason values cause\n\t\t// deserialization failures\n\t\tOBJECT_MAPPER.coercionConfigFor(Enum.class).setCoercion(CoercionInputShape.EmptyString, CoercionAction.AsNull);\n\t}\n\n\tprivate static final List<String> BEAN_MERGE_FIELD_EXCISIONS = List.of(\"class\");\n\n\tprivate static final ConcurrentHashMap<Class<?>, List<String>> REQUEST_FIELD_NAMES_PER_CLASS = new ConcurrentHashMap<>();\n\n\tprivate static final AtomicReference<SchemaGenerator> SCHEMA_GENERATOR_CACHE = new AtomicReference<>();\n\n\tprivate static TypeReference<HashMap<String, Object>> MAP_TYPE_REF = new TypeReference<>() {\n\n\t};\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using the default\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json) {\n\t\treturn jsonToMap(json, OBJECT_MAPPER);\n\t}\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using a custom\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @param objectMapper the ObjectMapper to use for deserialization.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json, ObjectMapper objectMapper) {\n\t\ttry {\n\t\t\treturn objectMapper.readValue(json, MAP_TYPE_REF);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given JSON string to an Object of the given type.\n\t * @param <T> the type of the object to return.\n\t * @param json the JSON string to convert to an object.\n\t * @param type the type of the object to return.\n\t * @return Object instance of the given type.\n\t */\n\tpublic static <T> T jsonToObject(String json, Class<T> type) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.readValue(json, type);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Failed to json: \" + json, e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonString(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonStringPrettyPrinter(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writerWithDefaultPrettyPrinter().writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. If the acceptedFieldNames is not empty, only the\n\t * fields with the given names are merged and returned. If the acceptedFieldNames is\n\t * empty, use the {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @param acceptedFieldNames the list of field names accepted for the target object.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n\n\t\tif (source == null) {\n\t\t\tsource = Map.of();\n\t\t}\n\n\t\tList<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n\t\t\t\t? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n\t\t\t\t: acceptedFieldNames;\n\n\t\tif (CollectionUtils.isEmpty(requestFieldNames)) {\n\t\t\tthrow new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n\t\t}\n\n\t\tMap<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n\t\tMap<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n\t\ttargetMap.putAll(sourceMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n\t\ttargetMap = targetMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> requestFieldNames.contains(e.getKey()))\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n\t\treturn ModelOptionsUtils.mapToClass(targetMap, clazz);\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. Returns the only field names that match the\n\t * {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz) {\n\t\treturn ModelOptionsUtils.merge(source, target, clazz, null);\n\t}\n\n\t/**\n\t * Converts the given object to a Map.\n\t * @param source the object to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> objectToMap(Object source) {\n\t\tif (source == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, new TypeReference<Map<String, Object>>() {\n\n\t\t\t})\n\t\t\t\t.entrySet()\n\t\t\t\t.stream()\n\t\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given Map to the given class.\n\t * @param <T> the type of the class to return.\n\t * @param source the Map to convert to the given class.\n\t * @param clazz the class to convert the Map to.\n\t * @return the converted class.\n\t */\n\tpublic static <T> T mapToClass(Map<String, Object> source, Class<T> clazz) {\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, clazz);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Returns the list of name values of the {@link JsonProperty} annotations.\n\t * @param clazz the class that contains fields annotated with {@link JsonProperty}.\n\t * @return the list of values of the {@link JsonProperty} annotations.\n\t */\n\tpublic static List<String> getJsonPropertyValues(Class<?> clazz) {\n\t\tList<String> values = new ArrayList<>();\n\t\tField[] fields = clazz.getDeclaredFields();\n\t\tfor (Field field : fields) {\n\t\t\tJsonProperty jsonPropertyAnnotation = field.getAnnotation(JsonProperty.class);\n\t\t\tif (jsonPropertyAnnotation != null) {\n\t\t\t\tvalues.add(jsonPropertyAnnotation.value());\n\t\t\t}\n\t\t}\n\t\treturn values;\n\t}",
    "func": "\tpublic static <I, S extends I, T extends S> T copyToTarget(S sourceBean, Class<I> sourceInterfaceClazz,\n\t\t\tClass<T> targetBeanClazz) {\n\n\t\tAssert.notNull(sourceInterfaceClazz, \"SourceOptionsClazz must not be null\");\n\t\tAssert.notNull(targetBeanClazz, \"TargetOptionsClazz must not be null\");\n\n\t\tif (sourceBean == null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tif (sourceBean.getClass().isAssignableFrom(targetBeanClazz)) {\n\t\t\treturn (T) sourceBean;\n\t\t}\n\n\t\ttry {\n\t\t\tT targetOptions = targetBeanClazz.getConstructor().newInstance();\n\n\t\t\tModelOptionsUtils.mergeBeans(sourceBean, targetOptions, sourceInterfaceClazz, true);\n\n\t\t\treturn targetOptions;\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\"Failed to convert the \" + sourceInterfaceClazz.getName() + \" into \" + targetBeanClazz.getName(),\n\t\t\t\t\te);\n\t\t}\n\t}",
    "comment": "\t/**\n\t * Returns a new instance of the targetBeanClazz that copies the bean values from the\n\t * sourceBean instance.\n\t * @param sourceBean the source bean to copy the values from.\n\t * @param sourceInterfaceClazz the source interface class. Only the fields with the\n\t * same name as the interface methods are copied. This allow the source object to be a\n\t * subclass of the source interface with additional, non-interface fields.\n\t * @param targetBeanClazz the target class, a subclass of the ChatOptions, to convert\n\t * into.\n\t * @param <T> the target class type.\n\t * @return a new instance of the targetBeanClazz with the values from the sourceBean\n\t * instance.\n\t */",
    "test_funcs": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::copyToTarget",
    "test_class": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::ModelOptionsUtilsTests",
    "func_start": 295,
    "func_end": 321,
    "body_len": 26,
    "instruction": "Write a generic function `copyToTarget` that takes a source object, a source interface class, and a target class (which extends the source class), and returns a new instance of the target class with values copied from the source object. The copying should only include fields that are defined in the source interface (by matching getter method names), and the function should use reflection to instantiate the target object and copy the compatible fields. If the source object is already an instance of the target class, return it as-is. If the source is null, return null. Any instantiation or reflection errors should be wrapped in a RuntimeException with a descriptive message.",
    "func_name_with_file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java::copyToTarget",
    "test_funcs_split": [
      "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::copyToTarget"
    ],
    "test_start": 115,
    "test_end": 128,
    "test_file": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java",
    "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.model.ModelOptionsUtilsTests#copyToTarget\"",
    "test_code": "\t@Test\n\tpublic void copyToTarget() {\n\t\tvar portableOptions = new TestPortableOptionsImpl();\n\t\tportableOptions.setName(\"John\");\n\t\tportableOptions.setAge(30);\n\t\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\t\tTestSpecificOptions target = ModelOptionsUtils.copyToTarget(portableOptions, TestPortableOptions.class,\n\t\t\t\tTestSpecificOptions.class);\n\n\t\tassertThat(target.getAge()).isEqualTo(30);\n\t\tassertThat(target.getName()).isEqualTo(\"John\");\n\t\tassertThat(target.getSpecificField()).isNull();\n\t}"
  },
  {
    "task-id": "spring-ai-spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java-mergeBeans",
    "project": "spring-ai",
    "file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java",
    "func_name": "mergeBeans",
    "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.model;\n\nimport java.beans.PropertyDescriptor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Type;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.core.type.TypeReference;\nimport com.fasterxml.jackson.databind.DeserializationFeature;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport com.fasterxml.jackson.databind.cfg.CoercionAction;\nimport com.fasterxml.jackson.databind.cfg.CoercionInputShape;\nimport com.fasterxml.jackson.databind.json.JsonMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport com.github.victools.jsonschema.module.jackson.JacksonModule;\nimport com.github.victools.jsonschema.module.jackson.JacksonOption;\nimport com.github.victools.jsonschema.module.swagger2.Swagger2Module;\n\nimport org.springframework.ai.util.JacksonUtils;\nimport org.springframework.beans.BeanWrapper;\nimport org.springframework.beans.BeanWrapperImpl;\nimport org.springframework.core.KotlinDetector;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.ObjectUtils;\n\n/**\n * Utility class for manipulating {@link ModelOptions} objects.\n *\n * @author Christian Tzolov\n * @author Thomas Vitale\n * @since 0.8.0\n */\npublic abstract class ModelOptionsUtils {\n\n\tpublic static final ObjectMapper OBJECT_MAPPER = JsonMapper.builder()\n\t\t.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\n\t\t.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS)\n\t\t.addModules(JacksonUtils.instantiateAvailableModules())\n\t\t.build()\n\t\t.configure(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT, true);\n\n\tstatic {\n\t\t// Configure coercion for empty strings to null for Enum types\n\t\t// This fixes the issue where empty string finish_reason values cause\n\t\t// deserialization failures\n\t\tOBJECT_MAPPER.coercionConfigFor(Enum.class).setCoercion(CoercionInputShape.EmptyString, CoercionAction.AsNull);\n\t}\n\n\tprivate static final List<String> BEAN_MERGE_FIELD_EXCISIONS = List.of(\"class\");\n\n\tprivate static final ConcurrentHashMap<Class<?>, List<String>> REQUEST_FIELD_NAMES_PER_CLASS = new ConcurrentHashMap<>();\n\n\tprivate static final AtomicReference<SchemaGenerator> SCHEMA_GENERATOR_CACHE = new AtomicReference<>();\n\n\tprivate static TypeReference<HashMap<String, Object>> MAP_TYPE_REF = new TypeReference<>() {\n\n\t};\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using the default\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json) {\n\t\treturn jsonToMap(json, OBJECT_MAPPER);\n\t}\n\n\t/**\n\t * Converts the given JSON string to a Map of String and Object using a custom\n\t * ObjectMapper.\n\t * @param json the JSON string to convert to a Map.\n\t * @param objectMapper the ObjectMapper to use for deserialization.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> jsonToMap(String json, ObjectMapper objectMapper) {\n\t\ttry {\n\t\t\treturn objectMapper.readValue(json, MAP_TYPE_REF);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given JSON string to an Object of the given type.\n\t * @param <T> the type of the object to return.\n\t * @param json the JSON string to convert to an object.\n\t * @param type the type of the object to return.\n\t * @return Object instance of the given type.\n\t */\n\tpublic static <T> T jsonToObject(String json, Class<T> type) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.readValue(json, type);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Failed to json: \" + json, e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonString(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given object to a JSON string.\n\t * @param object the object to convert to a JSON string.\n\t * @return the JSON string.\n\t */\n\tpublic static String toJsonStringPrettyPrinter(Object object) {\n\t\ttry {\n\t\t\treturn OBJECT_MAPPER.writerWithDefaultPrettyPrinter().writeValueAsString(object);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. If the acceptedFieldNames is not empty, only the\n\t * fields with the given names are merged and returned. If the acceptedFieldNames is\n\t * empty, use the {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @param acceptedFieldNames the list of field names accepted for the target object.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz, List<String> acceptedFieldNames) {\n\n\t\tif (source == null) {\n\t\t\tsource = Map.of();\n\t\t}\n\n\t\tList<String> requestFieldNames = CollectionUtils.isEmpty(acceptedFieldNames)\n\t\t\t\t? REQUEST_FIELD_NAMES_PER_CLASS.computeIfAbsent(clazz, ModelOptionsUtils::getJsonPropertyValues)\n\t\t\t\t: acceptedFieldNames;\n\n\t\tif (CollectionUtils.isEmpty(requestFieldNames)) {\n\t\t\tthrow new IllegalArgumentException(\"No @JsonProperty fields found in the \" + clazz.getName());\n\t\t}\n\n\t\tMap<String, Object> sourceMap = ModelOptionsUtils.objectToMap(source);\n\t\tMap<String, Object> targetMap = ModelOptionsUtils.objectToMap(target);\n\n\t\ttargetMap.putAll(sourceMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n\n\t\ttargetMap = targetMap.entrySet()\n\t\t\t.stream()\n\t\t\t.filter(e -> requestFieldNames.contains(e.getKey()))\n\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n\t\treturn ModelOptionsUtils.mapToClass(targetMap, clazz);\n\t}\n\n\t/**\n\t * Merges the source object into the target object and returns an object represented\n\t * by the given class. The JSON property names are used to match the fields to merge.\n\t * The source non-null values override the target values with the same field name. The\n\t * source null values are ignored. Returns the only field names that match the\n\t * {@code @JsonProperty} names, inferred from the provided clazz.\n\t * @param <T> they type of the class to return.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param clazz the class to return.\n\t * @return the merged object represented by the given class.\n\t */\n\tpublic static <T> T merge(Object source, Object target, Class<T> clazz) {\n\t\treturn ModelOptionsUtils.merge(source, target, clazz, null);\n\t}\n\n\t/**\n\t * Converts the given object to a Map.\n\t * @param source the object to convert to a Map.\n\t * @return the converted Map.\n\t */\n\tpublic static Map<String, Object> objectToMap(Object source) {\n\t\tif (source == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, new TypeReference<Map<String, Object>>() {\n\n\t\t\t})\n\t\t\t\t.entrySet()\n\t\t\t\t.stream()\n\t\t\t\t.filter(e -> e.getValue() != null)\n\t\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Converts the given Map to the given class.\n\t * @param <T> the type of the class to return.\n\t * @param source the Map to convert to the given class.\n\t * @param clazz the class to convert the Map to.\n\t * @return the converted class.\n\t */\n\tpublic static <T> T mapToClass(Map<String, Object> source, Class<T> clazz) {\n\t\ttry {\n\t\t\tString json = OBJECT_MAPPER.writeValueAsString(source);\n\t\t\treturn OBJECT_MAPPER.readValue(json, clazz);\n\t\t}\n\t\tcatch (JsonProcessingException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}\n\n\t/**\n\t * Returns the list of name values of the {@link JsonProperty} annotations.\n\t * @param clazz the class that contains fields annotated with {@link JsonProperty}.\n\t * @return the list of values of the {@link JsonProperty} annotations.\n\t */\n\tpublic static List<String> getJsonPropertyValues(Class<?> clazz) {\n\t\tList<String> values = new ArrayList<>();\n\t\tField[] fields = clazz.getDeclaredFields();\n\t\tfor (Field field : fields) {\n\t\t\tJsonProperty jsonPropertyAnnotation = field.getAnnotation(JsonProperty.class);\n\t\t\tif (jsonPropertyAnnotation != null) {\n\t\t\t\tvalues.add(jsonPropertyAnnotation.value());\n\t\t\t}\n\t\t}\n\t\treturn values;\n\t}\n\n\t/**\n\t * Returns a new instance of the targetBeanClazz that copies the bean values from the\n\t * sourceBean instance.\n\t * @param sourceBean the source bean to copy the values from.\n\t * @param sourceInterfaceClazz the source interface class. Only the fields with the\n\t * same name as the interface methods are copied. This allow the source object to be a\n\t * subclass of the source interface with additional, non-interface fields.\n\t * @param targetBeanClazz the target class, a subclass of the ChatOptions, to convert\n\t * into.\n\t * @param <T> the target class type.\n\t * @return a new instance of the targetBeanClazz with the values from the sourceBean\n\t * instance.\n\t */\n\tpublic static <I, S extends I, T extends S> T copyToTarget(S sourceBean, Class<I> sourceInterfaceClazz,\n\t\t\tClass<T> targetBeanClazz) {\n\n\t\tAssert.notNull(sourceInterfaceClazz, \"SourceOptionsClazz must not be null\");\n\t\tAssert.notNull(targetBeanClazz, \"TargetOptionsClazz must not be null\");\n\n\t\tif (sourceBean == null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tif (sourceBean.getClass().isAssignableFrom(targetBeanClazz)) {\n\t\t\treturn (T) sourceBean;\n\t\t}\n\n\t\ttry {\n\t\t\tT targetOptions = targetBeanClazz.getConstructor().newInstance();\n\n\t\t\tModelOptionsUtils.mergeBeans(sourceBean, targetOptions, sourceInterfaceClazz, true);\n\n\t\t\treturn targetOptions;\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\"Failed to convert the \" + sourceInterfaceClazz.getName() + \" into \" + targetBeanClazz.getName(),\n\t\t\t\t\te);\n\t\t}\n\t}",
    "func": "\tpublic static <I, S extends I, T extends S> T mergeBeans(S source, T target, Class<I> sourceInterfaceClazz,\n\t\t\tboolean overrideNonNullTargetValues) {\n\t\tAssert.notNull(source, \"Source object must not be null\");\n\t\tAssert.notNull(target, \"Target object must not be null\");\n\n\t\tBeanWrapper sourceBeanWrap = new BeanWrapperImpl(source);\n\t\tBeanWrapper targetBeanWrap = new BeanWrapperImpl(target);\n\n\t\tList<String> interfaceNames = Arrays.stream(sourceInterfaceClazz.getMethods()).map(m -> m.getName()).toList();\n\n\t\tfor (PropertyDescriptor descriptor : sourceBeanWrap.getPropertyDescriptors()) {\n\n\t\t\tif (!BEAN_MERGE_FIELD_EXCISIONS.contains(descriptor.getName())\n\t\t\t\t\t&& interfaceNames.contains(toGetName(descriptor.getName()))) {\n\n\t\t\t\tString propertyName = descriptor.getName();\n\t\t\t\tObject value = sourceBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t\t// Copy value to the target object\n\t\t\t\tif (value != null) {\n\t\t\t\t\tvar targetValue = targetBeanWrap.getPropertyValue(propertyName);\n\n\t\t\t\t\tif (targetValue == null || overrideNonNullTargetValues) {\n\t\t\t\t\t\ttargetBeanWrap.setPropertyValue(propertyName, value);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn target;\n\t}",
    "comment": "\t/**\n\t * Merges the source object into the target object. The source null values are\n\t * ignored. Only objects with Getter and Setter methods are supported.\n\t * @param <T> the type of the source and target object.\n\t * @param source the source object to merge.\n\t * @param target the target object to merge into.\n\t * @param sourceInterfaceClazz the source interface class. Only the fields with the\n\t * same name as the interface methods are merged. This allow the source object to be a\n\t * subclass of the source interface with additional, non-interface fields.\n\t * @param overrideNonNullTargetValues if true, the source non-null values override the\n\t * target values with the same field name. If false, the source non-null values are\n\t * ignored.\n\t * @return the merged target object.\n\t */",
    "test_funcs": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::mergeBeans",
    "test_class": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::ModelOptionsUtilsTests",
    "func_start": 337,
    "func_end": 367,
    "body_len": 30,
    "instruction": "Write a generic function `mergeBeans` that takes a source object, a target object, a source interface class, and a boolean flag `overrideNonNullTargetValues`. The function should merge properties from the source to the target object only if they are defined in the given interface (by matching getter method names), ignoring any null values in the source. If `overrideNonNullTargetValues` is true, non-null source values should overwrite existing non-null target values; otherwise, they should be skipped if the target property is already set. The function must use Spring's `BeanWrapperImpl` to access and modify properties via getter and setter methods, and it should exclude certain fields listed in `BEAN_MERGE_FIELD_EXCISIONS`. The function should return the modified target object after merging, and throw an exception if either source or target is null.",
    "func_name_with_file_path": "spring-ai-model/src/main/java/org/springframework/ai/model/ModelOptionsUtils.java::mergeBeans",
    "test_funcs_split": [
      "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java::mergeBeans"
    ],
    "test_start": 86,
    "test_end": 113,
    "test_file": "spring-ai-model/src/test/java/org/springframework/ai/model/ModelOptionsUtilsTests.java",
    "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.model.ModelOptionsUtilsTests#mergeBeans\"",
    "test_code": "\t@Test\n\tpublic void mergeBeans() {\n\n\t\tvar portableOptions = new TestPortableOptionsImpl();\n\t\tportableOptions.setName(\"John\");\n\t\tportableOptions.setAge(30);\n\t\tportableOptions.setNonInterfaceField(\"NonInterfaceField\");\n\n\t\tvar specificOptions = new TestSpecificOptions();\n\n\t\tspecificOptions.setName(\"Mike\");\n\t\tspecificOptions.setAge(60);\n\t\tspecificOptions.setSpecificField(\"SpecificField\");\n\n\t\tTestSpecificOptions specificOptions2 = ModelOptionsUtils.mergeBeans(portableOptions, specificOptions,\n\t\t\t\tTestPortableOptions.class, false);\n\n\t\tassertThat(specificOptions2.getAge()).isEqualTo(60);\n\t\tassertThat(specificOptions2.getName()).isEqualTo(\"Mike\");\n\t\tassertThat(specificOptions2.getSpecificField()).isEqualTo(\"SpecificField\");\n\n\t\tTestSpecificOptions specificOptionsWithOverride = ModelOptionsUtils.mergeBeans(portableOptions, specificOptions,\n\t\t\t\tTestPortableOptions.class, true);\n\n\t\tassertThat(specificOptionsWithOverride.getAge()).isEqualTo(30);\n\t\tassertThat(specificOptionsWithOverride.getName()).isEqualTo(\"John\");\n\t\tassertThat(specificOptionsWithOverride.getSpecificField()).isEqualTo(\"SpecificField\");\n\t}"
  },
  {
    "task-id": "spring-ai-spring-ai-model/src/main/java/org/springframework/ai/aot/AiRuntimeHints.java-findJsonAnnotatedClassesInPackage",
    "project": "spring-ai",
    "file_path": "spring-ai-model/src/main/java/org/springframework/ai/aot/AiRuntimeHints.java",
    "func_name": "findJsonAnnotatedClassesInPackage",
    "context": "/*\n * Copyright 2023-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.aot;\n\nimport java.lang.reflect.Executable;\nimport java.util.Arrays;\nimport java.util.HashSet;\nimport java.util.Objects;\nimport java.util.Set;\nimport java.util.stream.Collectors;\n\nimport com.fasterxml.jackson.annotation.JsonInclude;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.springframework.aot.hint.TypeReference;\nimport org.springframework.context.annotation.ClassPathScanningCandidateComponentProvider;\nimport org.springframework.core.type.filter.AnnotationTypeFilter;\nimport org.springframework.core.type.filter.TypeFilter;\n\n/**\n * Utility methods for creating native runtime hints. See other modules for their\n * respective native runtime hints.\n *\n * @author Josh Long\n * @author Christian Tzolov\n * @author Mark Pollack\n */\npublic abstract class AiRuntimeHints {\n\n\tprivate static final Logger log = LoggerFactory.getLogger(AiRuntimeHints.class);",
    "func": "\tpublic static Set<TypeReference> findJsonAnnotatedClassesInPackage(String packageName) {\n\t\tvar annotationTypeFilter = new AnnotationTypeFilter(JsonInclude.class);\n\t\tTypeFilter typeFilter = (metadataReader, metadataReaderFactory) -> {\n\t\t\ttry {\n\t\t\t\tvar clazz = Class.forName(metadataReader.getClassMetadata().getClassName());\n\t\t\t\treturn annotationTypeFilter.match(metadataReader, metadataReaderFactory)\n\t\t\t\t\t\t|| !discoverJacksonAnnotatedTypesFromRootType(clazz).isEmpty();\n\t\t\t}\n\t\t\tcatch (ClassNotFoundException e) {\n\t\t\t\tthrow new RuntimeException(e);\n\t\t\t}\n\t\t};\n\n\t\treturn findClassesInPackage(packageName, typeFilter);\n\t}",
    "comment": "\t/**\n\t * Finds classes in a package that are annotated with JsonInclude or have Jackson\n\t * annotations.\n\t * @param packageName The name of the package to search for annotated classes.\n\t * @return A set of TypeReference objects representing the annotated classes found.\n\t */",
    "test_funcs": "spring-ai-model/src/test/java/org/springframework/ai/aot/AiRuntimeHintsTests.java::discoverRelevantClasses",
    "test_class": "spring-ai-model/src/test/java/org/springframework/ai/aot/AiRuntimeHintsTests.java::AiRuntimeHintsTests",
    "func_start": 54,
    "func_end": 68,
    "body_len": 14,
    "instruction": "Write a function `findJsonAnnotatedClassesInPackage(String packageName)` that takes a package name as input and returns a set of `TypeReference` objects representing classes within that package which are either annotated with `@JsonInclude` or have any Jackson-related annotations present on their fields, methods, or class level. The function should use Spring's `MetadataReader` and `ClassPathScanningCandidateComponentProvider` under the hood to scan for classes, apply a custom `TypeFilter` to check for the presence of `@JsonInclude` or other Jackson annotations by dynamically loading the class and inspecting it, and then convert the matching classes into `TypeReference` instances for return.",
    "func_name_with_file_path": "spring-ai-model/src/main/java/org/springframework/ai/aot/AiRuntimeHints.java::findJsonAnnotatedClassesInPackage",
    "test_funcs_split": [
      "spring-ai-model/src/test/java/org/springframework/ai/aot/AiRuntimeHintsTests.java::discoverRelevantClasses"
    ],
    "test_start": 34,
    "test_end": 43,
    "test_file": "spring-ai-model/src/test/java/org/springframework/ai/aot/AiRuntimeHintsTests.java",
    "test_instruction": "mvn test -pl spring-ai-model -Dtest=\"org.springframework.ai.aot.AiRuntimeHintsTests#discoverRelevantClasses\"",
    "test_code": "\t@Test\n\tvoid discoverRelevantClasses() {\n\t\tvar classes = AiRuntimeHints.findJsonAnnotatedClassesInPackage(TestApi.class);\n\t\tvar included = Set.of(TestApi.Bar.class, TestApi.Foo.class)\n\t\t\t.stream()\n\t\t\t.map(t -> TypeReference.of(t.getName()))\n\t\t\t.collect(Collectors.toSet());\n\t\tLogFactory.getLog(getClass()).info(classes);\n\t\tAssert.state(classes.containsAll(included), \"there should be all of the enumerated classes. \");\n\t}"
  },
  {
    "task-id": "spring-ai-document-readers/pdf-reader/src/main/java/org/springframework/ai/reader/pdf/ParagraphPdfDocumentReader.java-get",
    "project": "spring-ai",
    "file_path": "document-readers/pdf-reader/src/main/java/org/springframework/ai/reader/pdf/ParagraphPdfDocumentReader.java",
    "func_name": "get",
    "context": "/*\n * Copyright 2023-2025 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.ai.reader.pdf;\n\nimport java.awt.Rectangle;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.pdfbox.pdfparser.PDFParser;\nimport org.apache.pdfbox.pdmodel.PDDocument;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.springframework.ai.document.Document;\nimport org.springframework.ai.document.DocumentReader;\nimport org.springframework.ai.reader.pdf.config.ParagraphManager;\nimport org.springframework.ai.reader.pdf.config.ParagraphManager.Paragraph;\nimport org.springframework.ai.reader.pdf.config.PdfDocumentReaderConfig;\nimport org.springframework.ai.reader.pdf.layout.PDFLayoutTextStripperByArea;\nimport org.springframework.core.io.DefaultResourceLoader;\nimport org.springframework.core.io.Resource;\nimport org.springframework.util.CollectionUtils;\nimport org.springframework.util.StringUtils;\n\n/**\n * Uses the PDF catalog (e.g. TOC) information to split the input PDF into text paragraphs\n * and output a single {@link Document} per paragraph.\n *\n * This class provides methods for reading and processing PDF documents. It uses the\n * Apache PDFBox library for parsing PDF content and converting it into text paragraphs.\n * The paragraphs are grouped into {@link Document} objects.\n *\n * @author Christian Tzolov\n * @author Heonwoo Kim\n */\npublic class ParagraphPdfDocumentReader implements DocumentReader {\n\n\t// Constants for metadata keys\n\tprivate static final String METADATA_START_PAGE = \"page_number\";\n\n\tprivate static final String METADATA_END_PAGE = \"end_page_number\";\n\n\tprivate static final String METADATA_TITLE = \"title\";\n\n\tprivate static final String METADATA_LEVEL = \"level\";\n\n\tprivate static final String METADATA_FILE_NAME = \"file_name\";\n\n\tprotected final PDDocument document;\n\n\tprivate final Logger logger = LoggerFactory.getLogger(getClass());\n\n\tprivate final ParagraphManager paragraphTextExtractor;\n\n\tprotected String resourceFileName;\n\n\tprivate PdfDocumentReaderConfig config;\n\n\t/**\n\t * Constructs a ParagraphPdfDocumentReader using a resource URL.\n\t * @param resourceUrl The URL of the PDF resource.\n\t */\n\tpublic ParagraphPdfDocumentReader(String resourceUrl) {\n\t\tthis(new DefaultResourceLoader().getResource(resourceUrl));\n\t}\n\n\t/**\n\t * Constructs a ParagraphPdfDocumentReader using a resource.\n\t * @param pdfResource The PDF resource.\n\t */\n\tpublic ParagraphPdfDocumentReader(Resource pdfResource) {\n\t\tthis(pdfResource, PdfDocumentReaderConfig.defaultConfig());\n\t}\n\n\t/**\n\t * Constructs a ParagraphPdfDocumentReader using a resource URL and a configuration.\n\t * @param resourceUrl The URL of the PDF resource.\n\t * @param config The configuration for PDF document processing.\n\t */\n\tpublic ParagraphPdfDocumentReader(String resourceUrl, PdfDocumentReaderConfig config) {\n\t\tthis(new DefaultResourceLoader().getResource(resourceUrl), config);\n\t}\n\n\t/**\n\t * Constructs a ParagraphPdfDocumentReader using a resource and a configuration.\n\t * @param pdfResource The PDF resource.\n\t * @param config The configuration for PDF document processing.\n\t */\n\tpublic ParagraphPdfDocumentReader(Resource pdfResource, PdfDocumentReaderConfig config) {\n\n\t\ttry {\n\t\t\tPDFParser pdfParser = new PDFParser(\n\t\t\t\t\tnew org.apache.pdfbox.io.RandomAccessReadBuffer(pdfResource.getInputStream()));\n\t\t\tthis.document = pdfParser.parse();\n\n\t\t\tthis.config = config;\n\n\t\t\tthis.paragraphTextExtractor = new ParagraphManager(this.document);\n\n\t\t\tthis.resourceFileName = pdfResource.getFilename();\n\t\t}\n\t\tcatch (IllegalArgumentException iae) {\n\t\t\tthrow iae;\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}",
    "func": "\t@Override\n\tpublic List<Document> get() {\n\t\tvar paragraphs = this.paragraphTextExtractor.flatten();\n\t\tList<Document> documents = new ArrayList<>();\n\t\tif (CollectionUtils.isEmpty(paragraphs)) {\n\t\t\treturn documents;\n\t\t}\n\t\tlogger.info(\"Start processing paragraphs from PDF\");\n\t\tfor (int i = 0; i < paragraphs.size(); i++) {\n\t\t\tParagraph from = paragraphs.get(i);\n\t\t\tParagraph to = (i + 1 < paragraphs.size()) ? paragraphs.get(i + 1) : from;\n\t\t\tDocument document = toDocument(from, to);\n\t\t\tif (document != null && StringUtils.hasText(document.getText())) {\n\t\t\t\tdocuments.add(document);\n\t\t\t}\n\t\t}\n\t\tlogger.info(\"End processing paragraphs from PDF\");\n\t\treturn documents;\n\t}",
    "comment": "\t/**\n\t * Reads and processes the PDF document to extract paragraphs.\n\t * @return A list of {@link Document} objects representing paragraphs.\n\t */",
    "test_funcs": "document-readers/pdf-reader/src/test/java/org/springframework/ai/reader/pdf/PagePdfDocumentReaderTests.java::classpathRead",
    "test_class": "document-readers/pdf-reader/src/test/java/org/springframework/ai/reader/pdf/PagePdfDocumentReaderTests.java::PagePdfDocumentReaderTests",
    "func_start": 128,
    "func_end": 146,
    "body_len": 18,
    "instruction": "Write a function that processes a list of paragraphs extracted from a PDF document, converts each paragraph into a Document object by pairing it with the next paragraph (or itself if it is the last one), and returns a list of non-empty Document objects containing the processed text. The function should log the start and end of the processing phase and skip any documents with null or empty text.",
    "func_name_with_file_path": "document-readers/pdf-reader/src/main/java/org/springframework/ai/reader/pdf/ParagraphPdfDocumentReader.java::get",
    "test_funcs_split": [
      "document-readers/pdf-reader/src/test/java/org/springframework/ai/reader/pdf/PagePdfDocumentReaderTests.java::classpathRead"
    ],
    "test_start": 36,
    "test_end": 60,
    "test_file": "document-readers/pdf-reader/src/test/java/org/springframework/ai/reader/pdf/PagePdfDocumentReaderTests.java",
    "test_instruction": "mvn test -pl document-readers/pdf-reader -Dtest=\"org.springframework.ai.reader.pdf.PagePdfDocumentReaderTests#classpathRead\"",
    "test_code": "\t@Test\n\tvoid classpathRead() {\n\n\t\tPagePdfDocumentReader pdfReader = new PagePdfDocumentReader(\"classpath:/sample1.pdf\",\n\t\t\t\tPdfDocumentReaderConfig.builder()\n\t\t\t\t\t.withPageTopMargin(0)\n\t\t\t\t\t.withPageBottomMargin(0)\n\t\t\t\t\t.withPageExtractedTextFormatter(ExtractedTextFormatter.builder()\n\t\t\t\t\t\t.withNumberOfTopTextLinesToDelete(0)\n\t\t\t\t\t\t.withNumberOfBottomTextLinesToDelete(3)\n\t\t\t\t\t\t.withNumberOfTopPagesToSkipBeforeDelete(0)\n\t\t\t\t\t\t.overrideLineSeparator(\"\\n\")\n\t\t\t\t\t\t.build())\n\t\t\t\t\t.withPagesPerDocument(1)\n\t\t\t\t\t.build());\n\n\t\tList<Document> docs = pdfReader.get();\n\n\t\tassertThat(docs).hasSize(4);\n\n\t\tString allText = docs.stream().map(Document::getText).collect(Collectors.joining(System.lineSeparator()));\n\n\t\tassertThat(allText).doesNotContain(\n\t\t\t\tList.of(\"Page  1 of 4\", \"Page  2 of 4\", \"Page  3 of 4\", \"Page  4 of 4\", \"PDF  Bookmark   Sample\"));\n\t}"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/util/FixedBitSet.java-ensureCapacity",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/util/FixedBitSet.java",
    "func_name": "ensureCapacity",
    "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.util;\n\nimport java.util.Arrays;\n\n/**\n * BitSet of fixed length (numBits), backed by accessible ({@link #getBits}) long[], accessed with\n * an int index, implementing {@link Bits}.\n */\npublic final class FixedBitSet extends BitSet {\n    private static final long BASE_RAM_BYTES_USED =\n            RamUsageEstimator.shallowSizeOfInstance(FixedBitSet.class);\n\n    private final long[] bits; // Array of longs holding the bits\n    private final int numBits; // The number of bits in use\n    private final int numWords; // The exact number of longs needed to hold numBits (<= bits.length)",
    "func": "    public static FixedBitSet ensureCapacity(FixedBitSet bits, int numBits) {\n        if (numBits < bits.numBits) {\n            return bits;\n        } else {\n            // Depends on the ghost bits being clear!\n            // (Otherwise, they may become visible in the new instance)\n            int numWords = bits2words(numBits);\n            long[] arr = bits.getBits();\n            if (numWords >= arr.length) {\n                arr = ArrayUtil.grow(arr, numWords + 1);\n            }\n            return new FixedBitSet(arr, arr.length << 6);\n        }\n    }",
    "comment": "    /**\n     * If the given {@link FixedBitSet} is large enough to hold {@code numBits+1}, returns the given\n     * bits, otherwise returns a new {@link FixedBitSet} which can hold the requested number of bits.\n     *\n     * <p><b>NOTE:</b> the returned bitset reuses the underlying {@code long[]} of the given {@code\n     * bits} if possible. Also, calling {@link #length()} on the returned bits may return a value\n     * greater than {@code numBits}.\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testPrevSetBit jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testIntersectionCount jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testSmall jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testEnsureCapacity jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::doRandomSets jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::doNextSetBit jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::checkPrevSetBitArray jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testNextBitSet jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::checkNextSetBitArray jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testEquals jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testSmallBitSets jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::makeIntArray jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testApproximateCardinality jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestAtomicFixedBitSet.java::testApproximateCardinality jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::makeFixedBitSet jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::doGet jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testUnionCount jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testAndNotCount jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testBits2Words jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testHashCodeEquals jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::doPrevSetBit jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::makeBitSet jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::copyOf jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestAtomicFixedBitSet.java::copyOf",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::TestFixedBitSet jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestAtomicFixedBitSet.java::TestAtomicFixedBitSet",
    "func_start": 49,
    "func_end": 62,
    "body_len": 13,
    "instruction": "Write a function `ensureCapacity` that takes a `FixedBitSet` and an integer `numBits`, and returns a `FixedBitSet` capable of holding at least `numBits + 1` bits. If the given bitset already has sufficient capacity, return it directly. Otherwise, return a new `FixedBitSet` with extended capacity, ensuring that the underlying `long[]` array is reused when possible for efficiency. The new bitset may have a length greater than `numBits`, and you must ensure that any previously unused 'ghost' bits are properly cleared to avoid exposing garbage bits in the new instance.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/util/FixedBitSet.java::ensureCapacity",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testPrevSetBit",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testIntersectionCount",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testSmall",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testEnsureCapacity",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::doRandomSets",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::doNextSetBit",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::checkPrevSetBitArray",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testNextBitSet",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::checkNextSetBitArray",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testEquals",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testSmallBitSets",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::makeIntArray",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testApproximateCardinality",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestAtomicFixedBitSet.java::testApproximateCardinality",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::makeFixedBitSet",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::doGet",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testUnionCount",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testAndNotCount",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testBits2Words",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::testHashCodeEquals",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::doPrevSetBit",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::makeBitSet",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java::copyOf",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestAtomicFixedBitSet.java::copyOf"
    ],
    "test_start": 329,
    "test_end": 335,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestFixedBitSet.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.util.TestFixedBitSet#testPrevSetBit\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "  @Override\n  @Test\n  public void testPrevSetBit() {\n    checkPrevSetBitArray(new int[] {}, 0);\n    checkPrevSetBitArray(new int[] {0}, 1);\n    checkPrevSetBitArray(new int[] {0, 2}, 3);\n  }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/util/AbstractLongHeap.java-pop",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/util/AbstractLongHeap.java",
    "func_name": "pop",
    "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.util;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport java.util.PrimitiveIterator;\n\n/**\n * A min heap that stores longs; a primitive priority queue that like all priority queues maintains\n * a partial ordering of its elements such that the leastbo element can always be found in constant\n * time. Push()'s and pop()'s require log(size). {@link #push(long)} may either grow the heap or\n * replace the worst element, depending on the subclass implementation.\n * <p>\n * The heap is a min heap, meaning that the top element is the lowest value.\n */\npublic abstract class AbstractLongHeap {\n\n    protected long[] heap;\n    protected int size = 0;\n\n    /**\n     * Create an empty heap with the configured initial size.\n     *\n     * @param initialSize the initial size of the heap\n     */\n    public AbstractLongHeap(int initialSize) {\n        final int heapSize;\n        if (initialSize < 1 || initialSize >= ArrayUtil.MAX_ARRAY_LENGTH) {\n            // Throw exception to prevent confusing OOME:\n            throw new IllegalArgumentException(\n                    \"initialSize must be > 0 and < \" + (ArrayUtil.MAX_ARRAY_LENGTH - 1) + \"; got: \" + initialSize);\n        }\n        // NOTE: we add +1 because all access to heap is 1-based not 0-based.  heap[0] is unused.\n        heapSize = initialSize + 1;\n        this.heap = new long[heapSize];\n    }\n\n    /**\n     * Adds a value to an LongHeap in log(size) time.\n     *\n     * @return true if the new value was added. (A fixed-size heap will not add the new value\n     * if it is full, and the new value is worse than the existing ones.)\n     */\n    public abstract boolean push(long element);\n\n    /**\n     * Adds elements from the given iterator to this heap until elementsSize elements have been added or the iterator\n     * is exhausted. Then re-heapifies in O(n) time (Floyd's build-heap).\n     *\n     * @param elements the iterator to pull elements from\n     * @param elementsSize the maximum number of elements to pull from the elements iterator\n     */\n    public abstract void pushMany(PrimitiveIterator.OfLong elements, int elementsSize);\n\n    protected long add(long element) {\n        size++;\n        if (size == heap.length) {\n            heap = ArrayUtil.grow(heap, (size * 3 + 1) / 2);\n        }\n        heap[size] = element;\n        upHeap(size);\n        return heap[1];\n    }\n\n    /**\n     * Bulk-adds the minimum between elementsSize and the number of elements in the iterator from the given iterator\n     * to this heap, then re-heapifies in O(n) time (Floyd's build-heap). For a proof explaining the linear time\n     * complexity, see <a href=\"https://stackoverflow.com/a/18742428\">this stackoverflow answer</a>.\n     *\n     * @param elements the iterator to pull elements from\n     * @param elementsSize the maximum number of elements to pull from the elements iterator\n     */\n    protected void addMany(PrimitiveIterator.OfLong elements, int elementsSize) {\n        if (!elements.hasNext()) {\n            return; // nothing to do\n        }\n\n        // 1) Ensure we have enough capacity\n        // NOTE: we add +1 to size because all access to heap is 1-based not 0-based.  heap[0] is unused.\n        int newSize = (size + 1) + elementsSize;\n        if (newSize > heap.length) {\n            heap = ArrayUtil.grow(heap, newSize);\n        }\n\n        // 2) Copy the new elements directly into the array\n        int added = 0;\n        while (elements.hasNext() && added++ < elementsSize) {\n            heap[++size] = elements.nextLong();\n        }\n\n        // 3) \"Bottom-up\" re-heapify:\n        //    Start from the last non-leaf node (size >>> 1) down to the root (1).\n        //    This is Floyd's build-heap algorithm.\n        // The loop goes down to 1 as heap is 1-based not 0-based.\n        for (int i = size >>> 1; i >= 1; i--) {\n            downHeap(i);\n        }\n    }\n\n    /**\n     * Returns the least element of the LongHeap in constant time. It is up to the caller to verify\n     * that the heap is not empty; no checking is done, and if no elements have been added, 0 is\n     * returned.\n     */\n    public final long top() {\n        return heap[1];\n    }",
    "func": "    public final long pop() {\n        if (size > 0) {\n            long result = heap[1]; // save first value\n            heap[1] = heap[size]; // move last to first\n            size--;\n            downHeap(1); // adjust heap\n            return result;\n        } else {\n            throw new IllegalStateException(\"The heap is empty\");\n        }\n    }",
    "comment": "    /**\n     * Removes and returns the least element of the PriorityQueue in log(size) time.\n     *\n     * @throws IllegalStateException if the LongHeap is empty.\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestLongHeap.java::testPQ",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestLongHeap.java::TestLongHeap",
    "func_start": 136,
    "func_end": 146,
    "body_len": 10,
    "instruction": "Write a function `pop()` that removes and returns the smallest element from a priority queue implemented as a binary heap, maintaining the heap property in O(log n) time. The function should throw an IllegalStateException if the heap is empty. Assume the heap is stored in a 1-indexed array, and after removing the root, the last element is moved to the root and then the heap is restored by performing a down-heap operation starting from the root.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/util/AbstractLongHeap.java::pop",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestLongHeap.java::testPQ"
    ],
    "test_start": 46,
    "test_end": 49,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/util/TestLongHeap.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.util.TestLongHeap#testPQ\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "    @Test\n    public void testPQ() {\n        testPQ(atLeast(1000), random());\n    }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/vector/Matrix.java-invert",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/vector/Matrix.java",
    "func_name": "invert",
    "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.vector;\n\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\nimport static java.lang.Math.abs;\n\n/**\n * Matrix object where each row is a VectorFloat; this makes multiplication of a matrix by a vector\n * a series of efficient dot products.\n */\npublic class Matrix {\n    private static final VectorTypeSupport vts = VectorizationProvider.getInstance().getVectorTypeSupport();\n\n    VectorFloat<?>[] data;\n\n    public Matrix(int m, int n) {\n        this(m, n, true);\n    }\n\n    public Matrix(int m, int n, boolean allocateZeroed) {\n        data = new VectorFloat[m];\n        if (allocateZeroed) {\n            for (int i = 0; i < m; i++) {\n                data[i] = vts.createFloatVector(n);\n            }\n        }\n    }\n\n    public float get(int i, int j) {\n        return data[i].get(j);\n    }\n\n    public void set(int i, int j, float value) {\n        data[i].set(j, value);\n    }\n\n    public boolean isIsomorphicWith(Matrix other) {\n        return data.length == other.data.length && data[0].length() == other.data[0].length();\n    }\n\n    public String toString() {\n        StringBuilder sb = new StringBuilder();\n        for (VectorFloat<?> row : data) {\n            sb.append(row.toString());\n            sb.append(\"\\n\");\n        }\n        return sb.toString();\n    }",
    "func": "    public Matrix invert() {\n        if (data.length == 0 || data.length != data[0].length()) {\n            throw new IllegalArgumentException(\"matrix must be square\");\n        }\n\n        int N = data.length;\n\n        // Initialize augmented matrix (original matrix on the left, identity matrix on the right)\n        var augmented = new Matrix(N, 2 * N);\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                augmented.set(i, j, get(i, j));\n                augmented.set(i, j + N, (i == j) ? 1 : 0);\n            }\n        }\n\n        // Perform Gaussian elimination with pivoting\n        for (int i = 0; i < N; i++) {\n            // Pivot: Find the row with the largest absolute value in column i to promote numerical stability\n            int maxRow = i;\n            for (int k = i + 1; k < N; k++) {\n                if (abs(augmented.get(k, i)) > abs(augmented.get(maxRow, i))) {\n                    maxRow = k;\n                }\n            }\n\n            // Swap the current row with the maxRow\n            var temp = augmented.data[i];\n            augmented.data[i] = augmented.data[maxRow];\n            augmented.data[maxRow] = temp;\n\n            // Scale pivot row\n            VectorUtil.scale(augmented.data[i], 1 / augmented.get(i, i));\n\n            // Eliminate below and above pivot\n            for (int k = 0; k < N; k++) {\n                if (k != i) {\n                    float factor = augmented.get(k, i);\n                    for (int j = 0; j < 2 * N; j++) {\n                        augmented.addTo(k, j, -factor * augmented.get(i, j));\n                    }\n                }\n            }\n        }\n\n        // Extract inverse matrix\n        var inverse = new Matrix(N, N);\n        for (int i = 0; i < N; i++) {\n            inverse.data[i].copyFrom(augmented.data[i], N, 0, N);\n        }\n\n        return inverse;\n    }",
    "comment": "    /**\n     * Inverts a square matrix using gaussian elimination.\n     * @return The inverse of the matrix.\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/vector/TestMatrixUtil.java::testInvert jvector-tests/src/test/java/io/github/jbellis/jvector/vector/TestMatrixUtil.java::testInvertNonSquareMatrix",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/vector/TestMatrixUtil.java::TestMatrixUtil",
    "func_start": 71,
    "func_end": 123,
    "body_len": 52,
    "instruction": "Write a method `invert()` in the `Matrix` class that computes and returns the inverse of a square matrix using Gaussian elimination with partial pivoting. The method should first check if the matrix is square, and if not, throw an `IllegalArgumentException`. To perform the inversion, augment the original matrix with the identity matrix of the same size, then apply row operations to transform the left half into the identity matrix, which will result in the inverse appearing on the right half. Use partial pivoting for numerical stability by selecting the row with the largest absolute value in the current column as the pivot. After elimination, extract and return the right half of the augmented matrix as the inverse.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/vector/Matrix.java::invert",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/vector/TestMatrixUtil.java::testInvert",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/vector/TestMatrixUtil.java::testInvertNonSquareMatrix"
    ],
    "test_start": 33,
    "test_end": 38,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/vector/TestMatrixUtil.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.vector.TestMatrixUtil#testInvert\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "    @Test\n    public void testInvert() {\n        var matrix = Matrix.from(new float[][] {{4, 7}, {2, 6}});\n        var expected = Matrix.from(new float[][] {{0.6f, -0.7f}, {-0.2f, 0.4f}});\n        assertEquals(expected, matrix.invert());\n    }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeQueue.java-rerank",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeQueue.java",
    "func_name": "rerank",
    "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.graph.similarity.ScoreFunction;\nimport io.github.jbellis.jvector.util.AbstractLongHeap;\nimport io.github.jbellis.jvector.util.BoundedLongHeap;\nimport io.github.jbellis.jvector.util.NumericUtils;\nimport java.util.PrimitiveIterator;\nimport org.agrona.collections.Int2ObjectHashMap;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeQueue uses a {@link io.github.jbellis.jvector.util.AbstractLongHeap} to store lists of nodes in a graph,\n * represented as a node id with an associated score packed together as a sortable long, which is sorted\n * primarily by score. The queue {@link #push(int, float)} operation provides either fixed-size\n * or unbounded operations, depending on the implementation subclasses, and either maxheap or minheap behavior.\n */\npublic class NodeQueue {\n    public enum Order {\n        /** Smallest values at the top of the heap */\n        MIN_HEAP {\n            @Override\n            long apply(long v) {\n                return v;\n            }\n        },\n        /** Largest values at the top of the heap */\n        MAX_HEAP {\n            @Override\n            long apply(long v) {\n                // This cannot be just `-v` since Long.MIN_VALUE doesn't have a positive counterpart. It\n                // needs a function that returns MAX_VALUE for MIN_VALUE and vice-versa.\n                return -1 - v;\n            }\n        };\n\n        abstract long apply(long v);\n    }\n\n    private final AbstractLongHeap heap;\n    private final Order order;\n\n    public NodeQueue(AbstractLongHeap heap, Order order) {\n        this.heap = heap;\n        this.order = order;\n    }\n\n    /**\n     * @return the number of elements in the heap\n     */\n    public int size() {\n        return heap.size();\n    }\n\n    /**\n     * Adds a new graph node to the heap.  Will extend storage or replace the worst element\n     * depending on the type of heap it is.\n     *\n     * @param newNode  the node id\n     * @param newScore the relative similarity score to the node of the owner\n     *\n     * @return true if the new value was added.\n     */\n    public boolean push(int newNode, float newScore) {\n        return heap.push(encode(newNode, newScore));\n    }\n\n    /**\n     * Encodes then adds elements from the given iterator to this heap until elementsSize elements have been added or\n     * the iterator is exhausted. The heap then re-heapifies in O(n) time (Floyd's build-heap).\n     *\n     * @param nodeScoreIterator the node/score pairs to add\n     * @param count             the maximum number of elements to pull from the nodeScoreIterator\n     */\n    public void pushMany(NodeScoreIterator nodeScoreIterator, int count) {\n        heap.pushMany(new NodeScoreIteratorConverter(nodeScoreIterator, this), count);\n    }\n\n    /**\n     * Encodes the node ID and its similarity score as long.  If two scores are equals,\n     * the smaller node ID wins.\n     *\n     * <p>The most significant 32 bits represent the float score, encoded as a sortable int.\n     *\n     * <p>The less significant 32 bits represent the node ID.\n     *\n     * <p>The bits representing the node ID are complemented to guarantee the win for the smaller node\n     * ID.\n     *\n     * <p>The AND with 0xFFFFFFFFL (a long with first 32 bit as 1) is necessary to obtain a long that\n     * has\n     *\n     * <p>The most significant 32 bits to 0\n     *\n     * <p>The less significant 32 bits represent the node ID.\n     *\n     * @param node  the node ID\n     * @param score the node score\n     * @return the encoded score, node ID\n     */\n    private long encode(int node, float score) {\n        assert node >= 0 : node;\n        return order.apply(\n                (((long) NumericUtils.floatToSortableInt(score)) << 32) | (0xFFFFFFFFL & ~node));\n    }\n\n    private float decodeScore(long heapValue) {\n        return NumericUtils.sortableIntToFloat((int) (order.apply(heapValue) >> 32));\n    }\n\n    private int decodeNodeId(long heapValue) {\n        return (int) ~(order.apply(heapValue));\n    }\n\n    /** Removes the top element and returns its node id. */\n    public int pop() {\n        return decodeNodeId(heap.pop());\n    }\n\n    /** Returns a copy of the internal nodes array. Not sorted by score! */\n    public int[] nodesCopy() {\n        int size = size();\n        int[] nodes = new int[size];\n        for (int i = 0; i < size; i++) {\n            nodes[i] = decodeNodeId(heap.get(i + 1));\n        }\n        return nodes;\n    }",
    "func": "    public float rerank(int topK, ScoreFunction.ExactScoreFunction reranker, float rerankFloor, NodeQueue reranked, NodesUnsorted unused) {\n        // Rescore the nodes whose approximate score meets the floor.  Nodes that do not will be marked as -1\n        int[] ids = new int[size()];\n        float[] exactScores = new float[size()];\n        var approximateScoresById = new Int2ObjectHashMap<Float>();\n        float bestScore = Float.NEGATIVE_INFINITY;\n        int bestIndex = -1;\n        int scoresAboveFloor = 0;\n        for (int i = 0; i < size(); i++) {\n            long heapValue = heap.get(i + 1);\n            float score = decodeScore(heapValue);\n            var nodeId = decodeNodeId(heapValue);\n            // track the best score found so far in case nothing is above the floor\n            if (score > bestScore) {\n                bestScore = score;\n                bestIndex = i;\n            }\n\n            if (score >= rerankFloor) {\n                // rerank this one\n                ids[i] = nodeId;\n                exactScores[i] = reranker.similarityTo(ids[i]);\n                approximateScoresById.put(ids[i], Float.valueOf(score));\n                scoresAboveFloor++;\n            } else {\n                // mark it unranked\n                ids[i] = -1;\n            }\n        }\n\n        if (scoresAboveFloor == 0 && bestIndex >= 0) {\n            // if nothing was above the floor, then rerank the best one found\n            ids[bestIndex] = decodeNodeId(heap.get(bestIndex + 1));\n            exactScores[bestIndex] = reranker.similarityTo(ids[bestIndex]);\n            approximateScoresById.put(ids[bestIndex], Float.valueOf(bestScore));\n        }\n\n        // go through the entries and add to the appropriate collection\n        for (int i = 0; i < ids.length; i++) {\n            if (ids[i] == -1) {\n                unused.add(decodeNodeId(heap.get(i + 1)), decodeScore(heap.get(i + 1)));\n                continue;\n            }\n\n            // if the reranked queue is full, then either this node, or the one it replaces on the heap, will be added\n            // to the unused pile, but push() can't tell us what node was evicted when the queue was already full, so\n            // we examine that manually\n            if (reranked.size() < topK) {\n                reranked.push(ids[i], exactScores[i]);\n            } else if (exactScores[i] > reranked.topScore()) {\n                int evictedNode = reranked.topNode();\n                unused.add(evictedNode, approximateScoresById.get(evictedNode));\n                reranked.push(ids[i], exactScores[i]);\n            } else {\n                unused.add(ids[i], decodeScore(heap.get(i + 1)));\n            }\n        }\n\n        // final pass to find the worst approximate score in the topK\n        // (we can't do this as part of the earlier loops because we don't know which nodes will be in the final topK)\n        float worstApproximateInTopK = Float.POSITIVE_INFINITY;\n        if (reranked.size() < topK) {\n            return worstApproximateInTopK;\n        }\n        for (int i = 0; i < reranked.size(); i++) {\n            int nodeId = decodeNodeId(reranked.heap.get(i + 1));\n            worstApproximateInTopK = min(worstApproximateInTopK, approximateScoresById.get(nodeId));\n        }\n\n        return worstApproximateInTopK;\n    }",
    "comment": "    /**\n     * Rerank results and return the worst approximate score that made it into the topK.\n     * The topK results will be placed into `reranked`, and the remainder into `unused`.\n     * <p>\n     * Only the best result or results whose approximate score is at least `rerankFloor` will be reranked.\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testTopMinHeap jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testUnboundedQueue jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testNeighborsProduct jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testMaxSizeQueue jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testClear jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testToString jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testNeighborsMaxHeap jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testTopMaxHeap",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::TestNodeQueue",
    "func_start": 160,
    "func_end": 230,
    "body_len": 70,
    "instruction": "Write a function `rerank` that takes a number of top results to keep (topK), an exact scoring function for reranking, a score threshold (rerankFloor), and two output containers: one for the reranked top results and one for the remaining unused results. The function should first identify all items whose approximate scores meet or exceed the rerankFloor (or the single best item if none meet the floor), then compute their exact scores using the provided scoring function. It should maintain a priority queue of the topK results based on exact scores, evicting lower-scoring items as needed, and place evicted or unqualified items into the unused container with their approximate scores. Finally, the function should return the worst (lowest) approximate score among the final topK reranked results, or positive infinity if fewer than topK results were reranked.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeQueue.java::rerank",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testTopMinHeap",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testUnboundedQueue",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testNeighborsProduct",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testMaxSizeQueue",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testClear",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testToString",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testNeighborsMaxHeap",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java::testTopMaxHeap"
    ],
    "test_start": 69,
    "test_end": 77,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeQueue.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeQueue#testTopMinHeap\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "  @Test\n  public void testTopMinHeap() {\n    NodeQueue nn = new NodeQueue(new GrowableLongHeap(2), NodeQueue.Order.MIN_HEAP);\n    nn.push(1, 0.5f);\n    nn.push(2, -0.5f);\n    // higher scores are better; lowest score on top\n    assertEquals(-0.5f, nn.topScore(), 0);\n    assertEquals(2, nn.topNode());\n  }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java-merge",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java",
    "func_name": "merge",
    "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.util.ArrayUtil;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.RamUsageEstimator;\nimport org.agrona.collections.IntHashSet;\n\nimport java.util.Arrays;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeArray encodes nodeids and their scores relative to some other element \n * (a query vector, or another graph node) as a pair of growable arrays. \n * Nodes are arranged in the sorted order of their scores in descending order,\n * i.e. the most-similar nodes are first.\n */\npublic class NodeArray {\n    public static final NodeArray EMPTY = new NodeArray(0);\n\n    private int size;\n    private float[] scores;\n    private int[] nodes;\n\n    public NodeArray(int initialSize) {\n        nodes = new int[initialSize];\n        scores = new float[initialSize];\n    }\n\n    // this idiosyncratic constructor exists for the benefit of subclass ConcurrentNeighborMap\n    protected NodeArray(NodeArray nodeArray) {\n        this.size = nodeArray.size();\n        this.nodes = nodeArray.nodes;\n        this.scores = nodeArray.scores;\n    }",
    "func": "    static NodeArray merge(NodeArray a1, NodeArray a2) {\n        NodeArray merged = new NodeArray(a1.size() + a2.size());\n        int i = 0, j = 0;\n\n        // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n        // we need to remember all the nodes with the current score to avoid adding duplicates\n        var nodesWithLastScore = new IntHashSet();\n        float lastAddedScore = Float.NaN;\n\n        // loop through both source arrays, adding the highest score element to the merged array,\n        // until we reach the end of one of the sources\n        while (i < a1.size() && j < a2.size()) {\n            if (a1.scores[i] < a2.scores[j]) {\n                // add from a2\n                if (a2.scores[j] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a2.scores[j];\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            } else if (a1.scores[i] > a2.scores[j]) {\n                // add from a1\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            } else {\n                // same score -- add both\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                i++;\n                j++;\n            }\n        }\n\n        // If elements remain in a1, add them\n        if (i < a1.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (i < a1.size && a1.scores[i] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size, a1.size - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size, a1.size - i);\n            merged.size += a1.size - i;\n        }\n\n        // If elements remain in a2, add them\n        if (j < a2.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (j < a2.size && a2.scores[j] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size, a2.size - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size, a2.size - j);\n            merged.size += a2.size - j;\n        }\n\n        return merged;\n    }",
    "comment": "    /** always creates a new NodeArray to return, even when a1 or a2 is empty */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesSimple jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainNoneSelected jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedNotFront jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testScoresDescOrder jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesDescOrder jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::validateSortedByScore jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainAllSelected jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesOnce jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesRandom jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedAtFront jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertNodesEqual jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertScoresEqual jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseConcurrent jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesSameScores",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::TestNeighbors jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::TestNodeArray",
    "func_start": 63,
    "func_end": 143,
    "body_len": 80,
    "instruction": "Write a static method `merge` that takes two `NodeArray` objects, `a1` and `a2`, and returns a new `NodeArray` containing all nodes from both arrays, merged in descending order by their scores. The merge should eliminate duplicate nodes (based on node identity) when they share the same score, ensuring that only one instance of each node is included per distinct score level. The method must handle ties in score by grouping nodes with the same score together and deduplicating within that group. Even if one of the input arrays is empty, a new `NodeArray` must always be returned. After merging the overlapping portion of both arrays, any remaining elements from either array should be added efficiently, with special care to avoid duplicates for nodes with the last processed score, and bulk-copying the rest for performance.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java::merge",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesSimple",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainNoneSelected",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedNotFront",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testScoresDescOrder",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesDescOrder",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::validateSortedByScore",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainAllSelected",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesOnce",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesRandom",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedAtFront",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertNodesEqual",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertScoresEqual",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseConcurrent",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesSameScores"
    ],
    "test_start": 189,
    "test_end": 229,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeArray#testMergeCandidatesSimple\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "  @Test\n  public void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n  }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java-addInOrder",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java",
    "func_name": "addInOrder",
    "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.util.ArrayUtil;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.RamUsageEstimator;\nimport org.agrona.collections.IntHashSet;\n\nimport java.util.Arrays;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeArray encodes nodeids and their scores relative to some other element \n * (a query vector, or another graph node) as a pair of growable arrays. \n * Nodes are arranged in the sorted order of their scores in descending order,\n * i.e. the most-similar nodes are first.\n */\npublic class NodeArray {\n    public static final NodeArray EMPTY = new NodeArray(0);\n\n    private int size;\n    private float[] scores;\n    private int[] nodes;\n\n    public NodeArray(int initialSize) {\n        nodes = new int[initialSize];\n        scores = new float[initialSize];\n    }\n\n    // this idiosyncratic constructor exists for the benefit of subclass ConcurrentNeighborMap\n    protected NodeArray(NodeArray nodeArray) {\n        this.size = nodeArray.size();\n        this.nodes = nodeArray.nodes;\n        this.scores = nodeArray.scores;\n    }\n\n    /** always creates a new NodeArray to return, even when a1 or a2 is empty */\n    static NodeArray merge(NodeArray a1, NodeArray a2) {\n        NodeArray merged = new NodeArray(a1.size() + a2.size());\n        int i = 0, j = 0;\n\n        // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n        // we need to remember all the nodes with the current score to avoid adding duplicates\n        var nodesWithLastScore = new IntHashSet();\n        float lastAddedScore = Float.NaN;\n\n        // loop through both source arrays, adding the highest score element to the merged array,\n        // until we reach the end of one of the sources\n        while (i < a1.size() && j < a2.size()) {\n            if (a1.scores[i] < a2.scores[j]) {\n                // add from a2\n                if (a2.scores[j] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a2.scores[j];\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            } else if (a1.scores[i] > a2.scores[j]) {\n                // add from a1\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            } else {\n                // same score -- add both\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                i++;\n                j++;\n            }\n        }\n\n        // If elements remain in a1, add them\n        if (i < a1.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (i < a1.size && a1.scores[i] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size, a1.size - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size, a1.size - i);\n            merged.size += a1.size - i;\n        }\n\n        // If elements remain in a2, add them\n        if (j < a2.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (j < a2.size && a2.scores[j] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size, a2.size - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size, a2.size - j);\n            merged.size += a2.size - j;\n        }\n\n        return merged;\n    }",
    "func": "    public void addInOrder(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        if (size > 0) {\n            float previousScore = scores[size - 1];\n            assert ((previousScore >= newScore))\n                    : \"Nodes are added in the incorrect order! Comparing \"\n                    + newScore\n                    + \" to \"\n                    + Arrays.toString(ArrayUtil.copyOfSubArray(scores, 0, size));\n        }\n        nodes[size] = newNode;\n        scores[size] = newScore;\n        ++size;\n    }",
    "comment": "    /**\n     * Add a new node to the NodeArray. The new node must be worse than all previously stored\n     * nodes.\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesSimple jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainNoneSelected jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedNotFront jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testScoresDescOrder jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesDescOrder jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::validateSortedByScore jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainAllSelected jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesOnce jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesRandom jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedAtFront jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertNodesEqual jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseRetainsNatural jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertScoresEqual jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesSameScores",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::TestNeighbors jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::TestNodeArray",
    "func_start": 149,
    "func_end": 164,
    "body_len": 15,
    "instruction": "Write a method `addInOrder(int newNode, float newScore)` that adds a new node and its corresponding score to the end of internal arrays, ensuring that each new node has a score that is less than or equal to the score of the previously added node. The method should first check if there is enough capacity in the arrays, resizing them if necessary, and then perform an assertion to verify the ordering constraint before inserting the new elements. The size of the collection should be incremented after the insertion.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java::addInOrder",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesSimple",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainNoneSelected",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedNotFront",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testScoresDescOrder",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesDescOrder",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::validateSortedByScore",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainAllSelected",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesOnce",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesRandom",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedAtFront",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertNodesEqual",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseRetainsNatural",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertScoresEqual",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesSameScores"
    ],
    "test_start": 189,
    "test_end": 229,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeArray#testMergeCandidatesSimple\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "  @Test\n  public void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n  }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java-insertSorted",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java",
    "func_name": "insertSorted",
    "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.util.ArrayUtil;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.RamUsageEstimator;\nimport org.agrona.collections.IntHashSet;\n\nimport java.util.Arrays;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeArray encodes nodeids and their scores relative to some other element \n * (a query vector, or another graph node) as a pair of growable arrays. \n * Nodes are arranged in the sorted order of their scores in descending order,\n * i.e. the most-similar nodes are first.\n */\npublic class NodeArray {\n    public static final NodeArray EMPTY = new NodeArray(0);\n\n    private int size;\n    private float[] scores;\n    private int[] nodes;\n\n    public NodeArray(int initialSize) {\n        nodes = new int[initialSize];\n        scores = new float[initialSize];\n    }\n\n    // this idiosyncratic constructor exists for the benefit of subclass ConcurrentNeighborMap\n    protected NodeArray(NodeArray nodeArray) {\n        this.size = nodeArray.size();\n        this.nodes = nodeArray.nodes;\n        this.scores = nodeArray.scores;\n    }\n\n    /** always creates a new NodeArray to return, even when a1 or a2 is empty */\n    static NodeArray merge(NodeArray a1, NodeArray a2) {\n        NodeArray merged = new NodeArray(a1.size() + a2.size());\n        int i = 0, j = 0;\n\n        // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n        // we need to remember all the nodes with the current score to avoid adding duplicates\n        var nodesWithLastScore = new IntHashSet();\n        float lastAddedScore = Float.NaN;\n\n        // loop through both source arrays, adding the highest score element to the merged array,\n        // until we reach the end of one of the sources\n        while (i < a1.size() && j < a2.size()) {\n            if (a1.scores[i] < a2.scores[j]) {\n                // add from a2\n                if (a2.scores[j] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a2.scores[j];\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            } else if (a1.scores[i] > a2.scores[j]) {\n                // add from a1\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            } else {\n                // same score -- add both\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                i++;\n                j++;\n            }\n        }\n\n        // If elements remain in a1, add them\n        if (i < a1.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (i < a1.size && a1.scores[i] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size, a1.size - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size, a1.size - i);\n            merged.size += a1.size - i;\n        }\n\n        // If elements remain in a2, add them\n        if (j < a2.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (j < a2.size && a2.scores[j] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size, a2.size - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size, a2.size - j);\n            merged.size += a2.size - j;\n        }\n\n        return merged;\n    }\n\n    /**\n     * Add a new node to the NodeArray. The new node must be worse than all previously stored\n     * nodes.\n     */\n    public void addInOrder(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        if (size > 0) {\n            float previousScore = scores[size - 1];\n            assert ((previousScore >= newScore))\n                    : \"Nodes are added in the incorrect order! Comparing \"\n                    + newScore\n                    + \" to \"\n                    + Arrays.toString(ArrayUtil.copyOfSubArray(scores, 0, size));\n        }\n        nodes[size] = newNode;\n        scores[size] = newScore;\n        ++size;\n    }\n\n    /**\n     * Returns the index at which the given node should be inserted to maintain sorted order,\n     * or -1 if the node already exists in the array (with the same score).\n     */\n    int insertionPoint(int newNode, float newScore) {\n        int insertionPoint = descSortFindRightMostInsertionPoint(newScore);\n        return duplicateExistsNear(insertionPoint, newNode, newScore) ? -1 : insertionPoint;\n    }",
    "func": "    public int insertSorted(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        int insertionPoint = insertionPoint(newNode, newScore);\n        if (insertionPoint == -1) {\n            return -1;\n        }\n\n        return insertInternal(insertionPoint, newNode, newScore);\n    }",
    "comment": "    /**\n     * Add a new node to the NodeArray into a correct sort position according to its score.\n     * Duplicate node + score pairs are ignored.\n     *\n     * @return the insertion point of the new node, or -1 if it already existed\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesSimple jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverse jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainNoneSelected jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedNotFront jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testScoresDescOrder jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesDescOrder jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::validateSortedByScore jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainAllSelected jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesOnce jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesRandom jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedAtFront jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertNodesEqual jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertScoresEqual jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseConcurrent jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesSameScores",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::TestNeighbors jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::TestNodeArray",
    "func_start": 181,
    "func_end": 191,
    "body_len": 10,
    "instruction": "Write a method `insertSorted` that takes an integer `newNode` and a float `newScore`, then inserts the node into a sorted array structure based on the score in ascending order while maintaining sorted order. If a node with the same score already exists, the method should ignore the duplicate and return -1. If the array is full, it should first resize the internal arrays to accommodate more elements. The method should return the index where the new node was inserted, or -1 if the node-score pair was already present, using helper methods to determine the correct insertion point and perform the actual insertion.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java::insertSorted",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesSimple",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverse",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainNoneSelected",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedNotFront",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testScoresDescOrder",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesDescOrder",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::validateSortedByScore",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainAllSelected",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesOnce",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesRandom",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedAtFront",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertNodesEqual",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertScoresEqual",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseConcurrent",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesSameScores"
    ],
    "test_start": 189,
    "test_end": 229,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeArray#testMergeCandidatesSimple\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "  @Test\n  public void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n  }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java-retain",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java",
    "func_name": "retain",
    "context": "/*\n * All changes to the original code are Copyright DataStax, Inc.\n *\n * Please see the included license file for details.\n */\n\n/*\n * Original license:\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.util.ArrayUtil;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.RamUsageEstimator;\nimport org.agrona.collections.IntHashSet;\n\nimport java.util.Arrays;\n\nimport static java.lang.Math.min;\n\n/**\n * NodeArray encodes nodeids and their scores relative to some other element \n * (a query vector, or another graph node) as a pair of growable arrays. \n * Nodes are arranged in the sorted order of their scores in descending order,\n * i.e. the most-similar nodes are first.\n */\npublic class NodeArray {\n    public static final NodeArray EMPTY = new NodeArray(0);\n\n    private int size;\n    private float[] scores;\n    private int[] nodes;\n\n    public NodeArray(int initialSize) {\n        nodes = new int[initialSize];\n        scores = new float[initialSize];\n    }\n\n    // this idiosyncratic constructor exists for the benefit of subclass ConcurrentNeighborMap\n    protected NodeArray(NodeArray nodeArray) {\n        this.size = nodeArray.size();\n        this.nodes = nodeArray.nodes;\n        this.scores = nodeArray.scores;\n    }\n\n    /** always creates a new NodeArray to return, even when a1 or a2 is empty */\n    static NodeArray merge(NodeArray a1, NodeArray a2) {\n        NodeArray merged = new NodeArray(a1.size() + a2.size());\n        int i = 0, j = 0;\n\n        // since nodes are only guaranteed to be sorted by score -- ties can appear in any node order --\n        // we need to remember all the nodes with the current score to avoid adding duplicates\n        var nodesWithLastScore = new IntHashSet();\n        float lastAddedScore = Float.NaN;\n\n        // loop through both source arrays, adding the highest score element to the merged array,\n        // until we reach the end of one of the sources\n        while (i < a1.size() && j < a2.size()) {\n            if (a1.scores[i] < a2.scores[j]) {\n                // add from a2\n                if (a2.scores[j] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a2.scores[j];\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            } else if (a1.scores[i] > a2.scores[j]) {\n                // add from a1\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            } else {\n                // same score -- add both\n                if (a1.scores[i] != lastAddedScore) {\n                    nodesWithLastScore.clear();\n                    lastAddedScore = a1.scores[i];\n                }\n                if (nodesWithLastScore.add(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                if (nodesWithLastScore.add(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                i++;\n                j++;\n            }\n        }\n\n        // If elements remain in a1, add them\n        if (i < a1.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (i < a1.size && a1.scores[i] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a1.nodes[i])) {\n                    merged.addInOrder(a1.nodes[i], a1.scores[i]);\n                }\n                i++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a1.nodes, i, merged.nodes, merged.size, a1.size - i);\n            System.arraycopy(a1.scores, i, merged.scores, merged.size, a1.size - i);\n            merged.size += a1.size - i;\n        }\n\n        // If elements remain in a2, add them\n        if (j < a2.size()) {\n            // avoid duplicates while adding nodes with the same score\n            while (j < a2.size && a2.scores[j] == lastAddedScore) {\n                if (!nodesWithLastScore.contains(a2.nodes[j])) {\n                    merged.addInOrder(a2.nodes[j], a2.scores[j]);\n                }\n                j++;\n            }\n            // the remaining nodes have a different score, so we can bulk-add them\n            System.arraycopy(a2.nodes, j, merged.nodes, merged.size, a2.size - j);\n            System.arraycopy(a2.scores, j, merged.scores, merged.size, a2.size - j);\n            merged.size += a2.size - j;\n        }\n\n        return merged;\n    }\n\n    /**\n     * Add a new node to the NodeArray. The new node must be worse than all previously stored\n     * nodes.\n     */\n    public void addInOrder(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        if (size > 0) {\n            float previousScore = scores[size - 1];\n            assert ((previousScore >= newScore))\n                    : \"Nodes are added in the incorrect order! Comparing \"\n                    + newScore\n                    + \" to \"\n                    + Arrays.toString(ArrayUtil.copyOfSubArray(scores, 0, size));\n        }\n        nodes[size] = newNode;\n        scores[size] = newScore;\n        ++size;\n    }\n\n    /**\n     * Returns the index at which the given node should be inserted to maintain sorted order,\n     * or -1 if the node already exists in the array (with the same score).\n     */\n    int insertionPoint(int newNode, float newScore) {\n        int insertionPoint = descSortFindRightMostInsertionPoint(newScore);\n        return duplicateExistsNear(insertionPoint, newNode, newScore) ? -1 : insertionPoint;\n    }\n\n    /**\n     * Add a new node to the NodeArray into a correct sort position according to its score.\n     * Duplicate node + score pairs are ignored.\n     *\n     * @return the insertion point of the new node, or -1 if it already existed\n     */\n    public int insertSorted(int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        int insertionPoint = insertionPoint(newNode, newScore);\n        if (insertionPoint == -1) {\n            return -1;\n        }\n\n        return insertInternal(insertionPoint, newNode, newScore);\n    }\n\n    /**\n     * Add a new node to the NodeArray into the specified insertion point.\n     */\n    void insertAt(int insertionPoint, int newNode, float newScore) {\n        if (size == nodes.length) {\n            growArrays();\n        }\n        insertInternal(insertionPoint, newNode, newScore);\n    }\n\n    private int insertInternal(int insertionPoint, int newNode, float newScore) {\n        System.arraycopy(nodes, insertionPoint, nodes, insertionPoint + 1, size - insertionPoint);\n        System.arraycopy(scores, insertionPoint, scores, insertionPoint + 1, size - insertionPoint);\n        nodes[insertionPoint] = newNode;\n        scores[insertionPoint] = newScore;\n        ++size;\n        return insertionPoint;\n    }\n\n    private boolean duplicateExistsNear(int insertionPoint, int newNode, float newScore) {\n        // Check to the left\n        for (int i = insertionPoint - 1; i >= 0 && scores[i] == newScore; i--) {\n            if (nodes[i] == newNode) {\n                return true;\n            }\n        }\n\n        // Check to the right\n        for (int i = insertionPoint; i < size && scores[i] == newScore; i++) {\n            if (nodes[i] == newNode) {\n                return true;\n            }\n        }\n\n        return false;\n    }",
    "func": "    public void retain(Bits selected) {\n        int writeIdx = 0; // index for where to write the next retained element\n\n        for (int readIdx = 0; readIdx < size; readIdx++) {\n            if (selected.get(readIdx)) {\n                if (writeIdx != readIdx) {\n                    // Move the selected entries to the front while maintaining their relative order\n                    nodes[writeIdx] = nodes[readIdx];\n                    scores[writeIdx] = scores[readIdx];\n                }\n                // else { we haven't created any gaps in the backing arrays yet, so we don't need to move anything }\n                writeIdx++;\n            }\n        }\n\n        size = writeIdx;\n    }",
    "comment": "    /**\n     * Retains only the elements in the current NodeArray whose corresponding index\n     * is set in the given BitSet.\n     * <p>\n     * This modifies the array in place, preserving the relative order of the elements retained.\n     * <p>\n     *\n     * @param selected A BitSet where the bit at index i is set if the i-th element should be retained.\n     *                 (Thus, the elements of selected represent positions in the NodeArray, NOT node ids.)\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesSimple jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainNoneSelected jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedNotFront jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testScoresDescOrder jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesDescOrder jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::validateSortedByScore jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainAllSelected jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesOnce jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesRandom jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedAtFront jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertNodesEqual jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertScoresEqual jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesSameScores",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::TestNodeArray",
    "func_start": 240,
    "func_end": 256,
    "body_len": 16,
    "instruction": "Write a method `retain(Bits selected)` that modifies the current NodeArray in place by keeping only the elements whose indices are marked in the provided BitSet. The BitSet indicates which positions (indices) in the array should be retained, and the method should preserve the original order of the retained elements. After processing, the size of the array should be updated to reflect the number of retained elements, and any elements beyond that point should be discarded. The operation should efficiently shift retained elements forward as needed without using extra space for another array.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/NodeArray.java::retain",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesSimple",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainNoneSelected",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedNotFront",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testScoresDescOrder",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesDescOrder",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::validateSortedByScore",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainAllSelected",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesOnce",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testMergeCandidatesRandom",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testRetainSomeSelectedAtFront",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertNodesEqual",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::assertScoresEqual",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java::testNoDuplicatesSameScores"
    ],
    "test_start": 189,
    "test_end": 229,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNodeArray.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNodeArray#testMergeCandidatesSimple\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "  @Test\n  public void testMergeCandidatesSimple() {\n    var arr1 = new NodeArray(1);\n    arr1.addInOrder(1, 1.0f);\n\n    var arr2 = new NodeArray(1);\n    arr2.addInOrder(0, 2.0f);\n\n    var merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [0, 1]\n    assertArrayEquals(new int[] {0, 1}, merged.copyDenseNodes());\n\n    arr1 = new NodeArray(3);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n    arr1.addInOrder(1, 1.0f);\n\n    arr2 = new NodeArray(3);\n    arr2.addInOrder(4, 4.0f);\n    arr2.addInOrder(2, 2.0f);\n    arr2.addInOrder(1, 1.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [4, 3, 2, 1]\n    assertArrayEquals(new int[] {4, 3, 2, 1}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {4.0f, 3.0f, 2.0f, 1.0f}, merged.copyDenseScores(), 0.0f);\n\n    // Testing boundary conditions\n    arr1 = new NodeArray(2);\n    arr1.addInOrder(3, 3.0f);\n    arr1.addInOrder(2, 2.0f);\n\n    arr2 = new NodeArray(1);\n    arr2.addInOrder(2, 2.0f);\n\n    merged = NodeArray.merge(arr1, arr2);\n    // Expected result: [3, 2]\n    assertArrayEquals(new int[] {3, 2}, merged.copyDenseNodes());\n    assertArrayEquals(new float[] {3.0f, 2.0f}, merged.copyDenseScores(), 0.0f);\n    validateSortedByScore(merged);\n  }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java-compute",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java",
    "func_name": "compute",
    "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.quantization;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.disk.RandomAccessReader;\nimport io.github.jbellis.jvector.graph.RandomAccessVectorValues;\nimport io.github.jbellis.jvector.graph.disk.OnDiskGraphIndex;\nimport io.github.jbellis.jvector.util.Accountable;\nimport io.github.jbellis.jvector.util.PhysicalCoreExecutor;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.VectorUtil;\nimport io.github.jbellis.jvector.vector.VectorizationProvider;\nimport io.github.jbellis.jvector.vector.types.ByteSequence;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\nimport java.io.DataOutput;\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Objects;\nimport java.util.concurrent.ForkJoinPool;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Supplier;\nimport java.util.logging.Logger;\nimport java.util.stream.Collectors;\nimport java.util.stream.IntStream;\n\nimport static io.github.jbellis.jvector.quantization.KMeansPlusPlusClusterer.UNWEIGHTED;\nimport static io.github.jbellis.jvector.util.MathUtil.square;\nimport static io.github.jbellis.jvector.vector.VectorUtil.dotProduct;\nimport static io.github.jbellis.jvector.vector.VectorUtil.sub;\nimport static java.lang.Math.min;\nimport static java.lang.Math.sqrt;\n\n/**\n * Product Quantization for float vectors.  Supports arbitrary source and target dimensionality;\n * in particular, the source does not need to be evenly divisible by the target.\n */\npublic class ProductQuantization implements VectorCompressor<ByteSequence<?>>, Accountable {\n    private static final int MAGIC = 0x75EC4012; // JVECTOR, with some imagination\n\n    protected static final Logger LOG = Logger.getLogger(ProductQuantization.class.getName());\n\n    private static final VectorTypeSupport vectorTypeSupport = VectorizationProvider.getInstance().getVectorTypeSupport();\n    static final int DEFAULT_CLUSTERS = 256; // number of clusters per subspace = one byte's worth\n    static final int K_MEANS_ITERATIONS = 6;\n    public static final int MAX_PQ_TRAINING_SET_SIZE = 128000;\n\n    final VectorFloat<?>[] codebooks; // array of codebooks, where each codebook is a VectorFloat consisting of k contiguous subvectors each of length M\n    final int M; // codebooks.length, redundantly reproduced for convenience\n    private final int clusterCount; // codebooks[0].length, redundantly reproduced for convenience\n    final int originalDimension;\n    final VectorFloat<?> globalCentroid;\n    final int[][] subvectorSizesAndOffsets;\n    final float anisotropicThreshold; // parallel cost multiplier\n    private final float[][] centroidNormsSquared; // precomputed norms of the centroids, for encoding\n    private final ThreadLocal<VectorFloat<?>> partialSums; // for dot product, euclidean, and cosine partials\n    private final ThreadLocal<VectorFloat<?>> partialBestDistances; // for partial best distances during fused ADC\n    private final ThreadLocal<ByteSequence<?>> partialQuantizedSums; // for quantized sums during fused ADC\n    private final AtomicReference<VectorFloat<?>> partialSquaredMagnitudes; // for cosine partials\n    private final AtomicReference<ByteSequence<?>> partialQuantizedSquaredMagnitudes; // for quantized squared magnitude partials during cosine fused ADC\n    protected volatile float squaredMagnitudeDelta = 0; // for cosine fused ADC squared magnitude quantization delta (since this is invariant for a given PQ)\n    protected volatile float minSquaredMagnitude = 0; // for cosine fused ADC minimum squared magnitude (invariant for a given PQ)\n\n    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     */\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter) {\n        return compute(ravv, M, clusterCount, globallyCenter, UNWEIGHTED, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter, float anisotropicThreshold) {\n        return compute(ravv, M, clusterCount, globallyCenter, anisotropicThreshold, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }",
    "func": "    public static ProductQuantization compute(RandomAccessVectorValues ravv,\n                                              int M,\n                                              int clusterCount,\n                                              boolean globallyCenter,\n                                              float anisotropicThreshold,\n                                              ForkJoinPool simdExecutor,\n                                              ForkJoinPool parallelExecutor)\n    {\n        checkClusterCount(clusterCount);\n\n        var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n        var vectors = extractTrainingVectors(ravv, parallelExecutor);\n\n        // subtract the centroid from each training vector\n        VectorFloat<?> globalCentroid;\n        if (globallyCenter) {\n            globalCentroid = KMeansPlusPlusClusterer.centroidOf(vectors);\n            // subtract the centroid from each vector\n            List<VectorFloat<?>> finalVectors = vectors;\n            vectors = simdExecutor.submit(() -> finalVectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n        } else {\n            globalCentroid = null;\n        }\n\n        // derive the codebooks\n        var codebooks = createCodebooks(vectors, subvectorSizesAndOffsets, clusterCount, anisotropicThreshold, simdExecutor);\n        return new ProductQuantization(codebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n    }",
    "comment": "    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param clusterCount number of clusters per subspace\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     * @param anisotropicThreshold the threshold of relevance for anisotropic angular distance shaping, giving\n     *        higher priority to parallel error.  Anisotropic shaping requires that your dataset be normalized\n     *        to unit length.  Use a threshold of UNWEIGHTED for isotropic distance\n     *        (i.e. normal, unweighted L2 distance).\n     * @param simdExecutor     ForkJoinPool instance for SIMD operations, best is to use a pool with the size of\n     *                         the number of physical cores.\n     * @param parallelExecutor ForkJoinPool instance for parallel stream operations\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::assertPerfectQuantization jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testPerfectReconstruction jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::loss jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovement jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testConvergenceAnisotropic jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testLoadVersion0 jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testRefine jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveVersion0 jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovementOnce jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::generate jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveLoad",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::TestProductQuantization",
    "func_start": 114,
    "func_end": 141,
    "body_len": 27,
    "instruction": "Write a function `compute` that takes a set of vectors to quantize, divides them into M subspaces, and applies Product Quantization to generate codebooks by clustering each subspace using K-means. The function should support optional global centering of vectors by subtracting the global centroid, and allow anisotropic angular distance shaping via a threshold parameter. It must use two ForkJoinPoolsone optimized for SIMD operations and another for parallel stream processingto improve performance during vector processing and clustering. The function should validate the cluster count, extract training vectors in parallel, compute centroids when global centering is enabled, and return a fully initialized ProductQuantization object containing the derived codebooks, subvector structure, and normalization parameters.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java::compute",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::assertPerfectQuantization",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testPerfectReconstruction",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::loss",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovement",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testConvergenceAnisotropic",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testLoadVersion0",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testRefine",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveVersion0",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovementOnce",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::generate",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveLoad"
    ],
    "test_start": 198,
    "test_end": 214,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.quantization.TestProductQuantization#testSaveLoad\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "    @Test\n    public void testSaveLoad() throws Exception {\n        // Generate a PQ for random 2D vectors\n        var vectors = createRandomVectors(512, 2);\n        var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n\n        // Write\n        var file = File.createTempFile(\"pqtest\", \".pq\");\n        try (var out = new DataOutputStream(new FileOutputStream(file))) {\n            pq.write(out);\n        }\n        // Read\n        try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n            var pq2 = ProductQuantization.load(readerSupplier.get());\n            Assertions.assertEquals(pq, pq2);\n        }\n    }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java-refine",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java",
    "func_name": "refine",
    "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.quantization;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.disk.RandomAccessReader;\nimport io.github.jbellis.jvector.graph.RandomAccessVectorValues;\nimport io.github.jbellis.jvector.graph.disk.OnDiskGraphIndex;\nimport io.github.jbellis.jvector.util.Accountable;\nimport io.github.jbellis.jvector.util.PhysicalCoreExecutor;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.VectorUtil;\nimport io.github.jbellis.jvector.vector.VectorizationProvider;\nimport io.github.jbellis.jvector.vector.types.ByteSequence;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\nimport java.io.DataOutput;\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Objects;\nimport java.util.concurrent.ForkJoinPool;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Supplier;\nimport java.util.logging.Logger;\nimport java.util.stream.Collectors;\nimport java.util.stream.IntStream;\n\nimport static io.github.jbellis.jvector.quantization.KMeansPlusPlusClusterer.UNWEIGHTED;\nimport static io.github.jbellis.jvector.util.MathUtil.square;\nimport static io.github.jbellis.jvector.vector.VectorUtil.dotProduct;\nimport static io.github.jbellis.jvector.vector.VectorUtil.sub;\nimport static java.lang.Math.min;\nimport static java.lang.Math.sqrt;\n\n/**\n * Product Quantization for float vectors.  Supports arbitrary source and target dimensionality;\n * in particular, the source does not need to be evenly divisible by the target.\n */\npublic class ProductQuantization implements VectorCompressor<ByteSequence<?>>, Accountable {\n    private static final int MAGIC = 0x75EC4012; // JVECTOR, with some imagination\n\n    protected static final Logger LOG = Logger.getLogger(ProductQuantization.class.getName());\n\n    private static final VectorTypeSupport vectorTypeSupport = VectorizationProvider.getInstance().getVectorTypeSupport();\n    static final int DEFAULT_CLUSTERS = 256; // number of clusters per subspace = one byte's worth\n    static final int K_MEANS_ITERATIONS = 6;\n    public static final int MAX_PQ_TRAINING_SET_SIZE = 128000;\n\n    final VectorFloat<?>[] codebooks; // array of codebooks, where each codebook is a VectorFloat consisting of k contiguous subvectors each of length M\n    final int M; // codebooks.length, redundantly reproduced for convenience\n    private final int clusterCount; // codebooks[0].length, redundantly reproduced for convenience\n    final int originalDimension;\n    final VectorFloat<?> globalCentroid;\n    final int[][] subvectorSizesAndOffsets;\n    final float anisotropicThreshold; // parallel cost multiplier\n    private final float[][] centroidNormsSquared; // precomputed norms of the centroids, for encoding\n    private final ThreadLocal<VectorFloat<?>> partialSums; // for dot product, euclidean, and cosine partials\n    private final ThreadLocal<VectorFloat<?>> partialBestDistances; // for partial best distances during fused ADC\n    private final ThreadLocal<ByteSequence<?>> partialQuantizedSums; // for quantized sums during fused ADC\n    private final AtomicReference<VectorFloat<?>> partialSquaredMagnitudes; // for cosine partials\n    private final AtomicReference<ByteSequence<?>> partialQuantizedSquaredMagnitudes; // for quantized squared magnitude partials during cosine fused ADC\n    protected volatile float squaredMagnitudeDelta = 0; // for cosine fused ADC squared magnitude quantization delta (since this is invariant for a given PQ)\n    protected volatile float minSquaredMagnitude = 0; // for cosine fused ADC minimum squared magnitude (invariant for a given PQ)\n\n    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     */\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter) {\n        return compute(ravv, M, clusterCount, globallyCenter, UNWEIGHTED, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter, float anisotropicThreshold) {\n        return compute(ravv, M, clusterCount, globallyCenter, anisotropicThreshold, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param clusterCount number of clusters per subspace\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     * @param anisotropicThreshold the threshold of relevance for anisotropic angular distance shaping, giving\n     *        higher priority to parallel error.  Anisotropic shaping requires that your dataset be normalized\n     *        to unit length.  Use a threshold of UNWEIGHTED for isotropic distance\n     *        (i.e. normal, unweighted L2 distance).\n     * @param simdExecutor     ForkJoinPool instance for SIMD operations, best is to use a pool with the size of\n     *                         the number of physical cores.\n     * @param parallelExecutor ForkJoinPool instance for parallel stream operations\n     */\n    public static ProductQuantization compute(RandomAccessVectorValues ravv,\n                                              int M,\n                                              int clusterCount,\n                                              boolean globallyCenter,\n                                              float anisotropicThreshold,\n                                              ForkJoinPool simdExecutor,\n                                              ForkJoinPool parallelExecutor)\n    {\n        checkClusterCount(clusterCount);\n\n        var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n        var vectors = extractTrainingVectors(ravv, parallelExecutor);\n\n        // subtract the centroid from each training vector\n        VectorFloat<?> globalCentroid;\n        if (globallyCenter) {\n            globalCentroid = KMeansPlusPlusClusterer.centroidOf(vectors);\n            // subtract the centroid from each vector\n            List<VectorFloat<?>> finalVectors = vectors;\n            vectors = simdExecutor.submit(() -> finalVectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n        } else {\n            globalCentroid = null;\n        }\n\n        // derive the codebooks\n        var codebooks = createCodebooks(vectors, subvectorSizesAndOffsets, clusterCount, anisotropicThreshold, simdExecutor);\n        return new ProductQuantization(codebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n    }\n\n    static List<VectorFloat<?>> extractTrainingVectors(RandomAccessVectorValues ravv, ForkJoinPool parallelExecutor) {\n        // limit the number of vectors we train on\n        var P = min(1.0f, MAX_PQ_TRAINING_SET_SIZE / (float) ravv.size());\n        var ravvCopy = ravv.threadLocalSupplier();\n        return parallelExecutor.submit(() -> IntStream.range(0, ravv.size()).parallel()\n                        .filter(i -> ThreadLocalRandom.current().nextFloat() < P)\n                        .mapToObj(targetOrd -> {\n                            var localRavv = ravvCopy.get();\n                            VectorFloat<?> v = localRavv.getVector(targetOrd);\n                            return localRavv.isValueShared() ? v.copy() : v;\n                        })\n                        .collect(Collectors.toList()))\n                .join();\n    }\n\n    /**\n     * Create a new PQ by fine-tuning this one with the data in `ravv`\n     */\n    public ProductQuantization refine(RandomAccessVectorValues ravv) {\n        return refine(ravv, 1, UNWEIGHTED, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }",
    "func": "    public ProductQuantization refine(RandomAccessVectorValues ravv,\n                                      int lloydsRounds,\n                                      float anisotropicThreshold,\n                                      ForkJoinPool simdExecutor,\n                                      ForkJoinPool parallelExecutor)\n    {\n        if (lloydsRounds < 0) {\n            throw new IllegalArgumentException(\"lloydsRounds must be non-negative\");\n        }\n\n        var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n        var vectorsMutable = extractTrainingVectors(ravv, parallelExecutor);\n        if (globalCentroid != null) {\n            var vectors = vectorsMutable;\n            vectorsMutable = simdExecutor.submit(() -> vectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n        }\n        var vectors = vectorsMutable; // \"effectively final\" to make the closure happy\n\n        var refinedCodebooks = simdExecutor.submit(() -> IntStream.range(0, M).parallel().mapToObj(m -> {\n            VectorFloat<?>[] subvectors = extractSubvectors(vectors, m, subvectorSizesAndOffsets);\n            var clusterer = new KMeansPlusPlusClusterer(subvectors, codebooks[m], anisotropicThreshold);\n            return clusterer.cluster(anisotropicThreshold == UNWEIGHTED ? lloydsRounds : 0,\n                                     anisotropicThreshold == UNWEIGHTED ? 0 : lloydsRounds);\n        }).toArray(VectorFloat<?>[]::new)).join();\n\n        return new ProductQuantization(refinedCodebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n    }",
    "comment": "    /**\n     * Create a new PQ by fine-tuning this one with the data in `ravv`\n     *\n     * @param lloydsRounds number of Lloyd's iterations to run against\n     *                     the new data.  Suggested values are 1 or 2.\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::assertPerfectQuantization jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testPerfectReconstruction jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::loss jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovement jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testConvergenceAnisotropic jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testLoadVersion0 jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testRefine jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveVersion0 jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovementOnce jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::generate jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveLoad",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::TestProductQuantization",
    "func_start": 171,
    "func_end": 197,
    "body_len": 26,
    "instruction": "Write a method `refine` in the `ProductQuantization` class that takes a `RandomAccessVectorValues` object containing new training data, the number of Lloyd's algorithm iterations to perform (`lloydsRounds`), an anisotropic threshold, and two `ForkJoinPool` executors for SIMD and general parallel operations. The method should validate that `lloydsRounds` is non-negative, compute subvector sizes and offsets based on the input dimension and number of subquantizers `M`, extract and optionally center the training vectors using the global centroid if present, then refine each subquantizer's codebook by applying K-means clustering (with Lloyd's iterations conditionally applied depending on whether anisotropic weighting is used), and finally return a new `ProductQuantization` instance with the updated codebooks and parameters while preserving the original configuration such as `clusterCount` and `globalCentroid`.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java::refine",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::assertPerfectQuantization",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testPerfectReconstruction",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::loss",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovement",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testConvergenceAnisotropic",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testLoadVersion0",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testRefine",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveVersion0",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovementOnce",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::generate",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveLoad"
    ],
    "test_start": 198,
    "test_end": 214,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.quantization.TestProductQuantization#testSaveLoad\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "    @Test\n    public void testSaveLoad() throws Exception {\n        // Generate a PQ for random 2D vectors\n        var vectors = createRandomVectors(512, 2);\n        var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n\n        // Write\n        var file = File.createTempFile(\"pqtest\", \".pq\");\n        try (var out = new DataOutputStream(new FileOutputStream(file))) {\n            pq.write(out);\n        }\n        // Read\n        try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n            var pq2 = ProductQuantization.load(readerSupplier.get());\n            Assertions.assertEquals(pq, pq2);\n        }\n    }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java-encodeAnisotropic",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java",
    "func_name": "encodeAnisotropic",
    "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.quantization;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.disk.RandomAccessReader;\nimport io.github.jbellis.jvector.graph.RandomAccessVectorValues;\nimport io.github.jbellis.jvector.graph.disk.OnDiskGraphIndex;\nimport io.github.jbellis.jvector.util.Accountable;\nimport io.github.jbellis.jvector.util.PhysicalCoreExecutor;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.VectorUtil;\nimport io.github.jbellis.jvector.vector.VectorizationProvider;\nimport io.github.jbellis.jvector.vector.types.ByteSequence;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\nimport java.io.DataOutput;\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Objects;\nimport java.util.concurrent.ForkJoinPool;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Supplier;\nimport java.util.logging.Logger;\nimport java.util.stream.Collectors;\nimport java.util.stream.IntStream;\n\nimport static io.github.jbellis.jvector.quantization.KMeansPlusPlusClusterer.UNWEIGHTED;\nimport static io.github.jbellis.jvector.util.MathUtil.square;\nimport static io.github.jbellis.jvector.vector.VectorUtil.dotProduct;\nimport static io.github.jbellis.jvector.vector.VectorUtil.sub;\nimport static java.lang.Math.min;\nimport static java.lang.Math.sqrt;\n\n/**\n * Product Quantization for float vectors.  Supports arbitrary source and target dimensionality;\n * in particular, the source does not need to be evenly divisible by the target.\n */\npublic class ProductQuantization implements VectorCompressor<ByteSequence<?>>, Accountable {\n    private static final int MAGIC = 0x75EC4012; // JVECTOR, with some imagination\n\n    protected static final Logger LOG = Logger.getLogger(ProductQuantization.class.getName());\n\n    private static final VectorTypeSupport vectorTypeSupport = VectorizationProvider.getInstance().getVectorTypeSupport();\n    static final int DEFAULT_CLUSTERS = 256; // number of clusters per subspace = one byte's worth\n    static final int K_MEANS_ITERATIONS = 6;\n    public static final int MAX_PQ_TRAINING_SET_SIZE = 128000;\n\n    final VectorFloat<?>[] codebooks; // array of codebooks, where each codebook is a VectorFloat consisting of k contiguous subvectors each of length M\n    final int M; // codebooks.length, redundantly reproduced for convenience\n    private final int clusterCount; // codebooks[0].length, redundantly reproduced for convenience\n    final int originalDimension;\n    final VectorFloat<?> globalCentroid;\n    final int[][] subvectorSizesAndOffsets;\n    final float anisotropicThreshold; // parallel cost multiplier\n    private final float[][] centroidNormsSquared; // precomputed norms of the centroids, for encoding\n    private final ThreadLocal<VectorFloat<?>> partialSums; // for dot product, euclidean, and cosine partials\n    private final ThreadLocal<VectorFloat<?>> partialBestDistances; // for partial best distances during fused ADC\n    private final ThreadLocal<ByteSequence<?>> partialQuantizedSums; // for quantized sums during fused ADC\n    private final AtomicReference<VectorFloat<?>> partialSquaredMagnitudes; // for cosine partials\n    private final AtomicReference<ByteSequence<?>> partialQuantizedSquaredMagnitudes; // for quantized squared magnitude partials during cosine fused ADC\n    protected volatile float squaredMagnitudeDelta = 0; // for cosine fused ADC squared magnitude quantization delta (since this is invariant for a given PQ)\n    protected volatile float minSquaredMagnitude = 0; // for cosine fused ADC minimum squared magnitude (invariant for a given PQ)\n\n    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     */\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter) {\n        return compute(ravv, M, clusterCount, globallyCenter, UNWEIGHTED, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    public static ProductQuantization compute(RandomAccessVectorValues ravv, int M, int clusterCount, boolean globallyCenter, float anisotropicThreshold) {\n        return compute(ravv, M, clusterCount, globallyCenter, anisotropicThreshold, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Initializes the codebooks by clustering the input data using Product Quantization.\n     *\n     * @param ravv the vectors to quantize\n     * @param M number of subspaces\n     * @param clusterCount number of clusters per subspace\n     * @param globallyCenter whether to center the vectors globally before quantization\n     *                       (not recommended when using the quantization for dot product)\n     * @param anisotropicThreshold the threshold of relevance for anisotropic angular distance shaping, giving\n     *        higher priority to parallel error.  Anisotropic shaping requires that your dataset be normalized\n     *        to unit length.  Use a threshold of UNWEIGHTED for isotropic distance\n     *        (i.e. normal, unweighted L2 distance).\n     * @param simdExecutor     ForkJoinPool instance for SIMD operations, best is to use a pool with the size of\n     *                         the number of physical cores.\n     * @param parallelExecutor ForkJoinPool instance for parallel stream operations\n     */\n    public static ProductQuantization compute(RandomAccessVectorValues ravv,\n                                              int M,\n                                              int clusterCount,\n                                              boolean globallyCenter,\n                                              float anisotropicThreshold,\n                                              ForkJoinPool simdExecutor,\n                                              ForkJoinPool parallelExecutor)\n    {\n        checkClusterCount(clusterCount);\n\n        var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n        var vectors = extractTrainingVectors(ravv, parallelExecutor);\n\n        // subtract the centroid from each training vector\n        VectorFloat<?> globalCentroid;\n        if (globallyCenter) {\n            globalCentroid = KMeansPlusPlusClusterer.centroidOf(vectors);\n            // subtract the centroid from each vector\n            List<VectorFloat<?>> finalVectors = vectors;\n            vectors = simdExecutor.submit(() -> finalVectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n        } else {\n            globalCentroid = null;\n        }\n\n        // derive the codebooks\n        var codebooks = createCodebooks(vectors, subvectorSizesAndOffsets, clusterCount, anisotropicThreshold, simdExecutor);\n        return new ProductQuantization(codebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n    }\n\n    static List<VectorFloat<?>> extractTrainingVectors(RandomAccessVectorValues ravv, ForkJoinPool parallelExecutor) {\n        // limit the number of vectors we train on\n        var P = min(1.0f, MAX_PQ_TRAINING_SET_SIZE / (float) ravv.size());\n        var ravvCopy = ravv.threadLocalSupplier();\n        return parallelExecutor.submit(() -> IntStream.range(0, ravv.size()).parallel()\n                        .filter(i -> ThreadLocalRandom.current().nextFloat() < P)\n                        .mapToObj(targetOrd -> {\n                            var localRavv = ravvCopy.get();\n                            VectorFloat<?> v = localRavv.getVector(targetOrd);\n                            return localRavv.isValueShared() ? v.copy() : v;\n                        })\n                        .collect(Collectors.toList()))\n                .join();\n    }\n\n    /**\n     * Create a new PQ by fine-tuning this one with the data in `ravv`\n     */\n    public ProductQuantization refine(RandomAccessVectorValues ravv) {\n        return refine(ravv, 1, UNWEIGHTED, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Create a new PQ by fine-tuning this one with the data in `ravv`\n     *\n     * @param lloydsRounds number of Lloyd's iterations to run against\n     *                     the new data.  Suggested values are 1 or 2.\n     */\n    public ProductQuantization refine(RandomAccessVectorValues ravv,\n                                      int lloydsRounds,\n                                      float anisotropicThreshold,\n                                      ForkJoinPool simdExecutor,\n                                      ForkJoinPool parallelExecutor)\n    {\n        if (lloydsRounds < 0) {\n            throw new IllegalArgumentException(\"lloydsRounds must be non-negative\");\n        }\n\n        var subvectorSizesAndOffsets = getSubvectorSizesAndOffsets(ravv.dimension(), M);\n        var vectorsMutable = extractTrainingVectors(ravv, parallelExecutor);\n        if (globalCentroid != null) {\n            var vectors = vectorsMutable;\n            vectorsMutable = simdExecutor.submit(() -> vectors.stream().parallel().map(v -> VectorUtil.sub(v, globalCentroid)).collect(Collectors.<VectorFloat<?>>toList())).join();\n        }\n        var vectors = vectorsMutable; // \"effectively final\" to make the closure happy\n\n        var refinedCodebooks = simdExecutor.submit(() -> IntStream.range(0, M).parallel().mapToObj(m -> {\n            VectorFloat<?>[] subvectors = extractSubvectors(vectors, m, subvectorSizesAndOffsets);\n            var clusterer = new KMeansPlusPlusClusterer(subvectors, codebooks[m], anisotropicThreshold);\n            return clusterer.cluster(anisotropicThreshold == UNWEIGHTED ? lloydsRounds : 0,\n                                     anisotropicThreshold == UNWEIGHTED ? 0 : lloydsRounds);\n        }).toArray(VectorFloat<?>[]::new)).join();\n\n        return new ProductQuantization(refinedCodebooks, clusterCount, subvectorSizesAndOffsets, globalCentroid, anisotropicThreshold);\n    }\n\n    ProductQuantization(VectorFloat<?>[] codebooks, int clusterCount, int[][] subvectorSizesAndOffsets, VectorFloat<?> globalCentroid, float anisotropicThreshold) {\n        checkClusterCount(clusterCount);\n\n        this.codebooks = codebooks;\n        this.globalCentroid = globalCentroid;\n        this.M = codebooks.length;\n        this.clusterCount = clusterCount;\n        this.subvectorSizesAndOffsets = subvectorSizesAndOffsets;\n        this.originalDimension = Arrays.stream(subvectorSizesAndOffsets).mapToInt(m -> m[0]).sum();\n        if (globalCentroid != null && globalCentroid.length() != originalDimension) {\n            var msg = String.format(\"Global centroid length %d does not match vector dimensionality %d\", globalCentroid.length(), originalDimension);\n            throw new IllegalArgumentException(msg);\n        }\n        this.anisotropicThreshold = anisotropicThreshold;\n        this.partialSums = ThreadLocal.withInitial(() -> vectorTypeSupport.createFloatVector(getSubspaceCount() * getClusterCount()));\n        this.partialBestDistances = ThreadLocal.withInitial(() -> vectorTypeSupport.createFloatVector(getSubspaceCount()));\n        this.partialQuantizedSums = ThreadLocal.withInitial(() -> vectorTypeSupport.createByteSequence(getSubspaceCount() * getClusterCount() * 2));\n        this.partialSquaredMagnitudes = new AtomicReference<>(null);\n        this.partialQuantizedSquaredMagnitudes= new AtomicReference<>(null);\n\n\n        centroidNormsSquared = new float[M][clusterCount];\n        for (int i = 0; i < M; i++) {\n            for (int j = 0; j < clusterCount; j++) {\n                centroidNormsSquared[i][j] = dotProduct(codebooks[i], j * subvectorSizesAndOffsets[i][0],\n                                                        codebooks[i], j * subvectorSizesAndOffsets[i][0],\n                                                        subvectorSizesAndOffsets[i][0]);\n            }\n        }\n    }\n\n    @Override\n    public ImmutablePQVectors createCompressedVectors(Object[] compressedVectors) {\n        return new ImmutablePQVectors(this, (ByteSequence<?>[]) compressedVectors, compressedVectors.length, 1);\n    }\n\n    /**\n     * Encodes the given vectors in parallel using the PQ codebooks. If a vector is missing (null), it will be encoded\n     * as a zero vector.\n     */\n    @Override\n    public PQVectors encodeAll(RandomAccessVectorValues ravv, ForkJoinPool simdExecutor) {\n        return PQVectors.encodeAndBuild(this, ravv.size(), ravv, simdExecutor);\n    }",
    "func": "    private void encodeAnisotropic(VectorFloat<?> vector, ByteSequence<?> result) {\n        // compute the residuals from each subvector to each corresponding codebook centroid\n        Residual[][] residuals = computeResiduals(vector);\n        assert residuals.length == M : \"Residuals length mismatch \" + residuals.length + \" != \" + M;\n        // start with centroids that minimize the residual norms\n        initializeToMinResidualNorms(residuals, result);\n        // sum the initial parallel residual component\n        float parallelResidualComponentSum = 0;\n        for (int i = 0; i < result.length(); i++) {\n            int centroidIdx = Byte.toUnsignedInt(result.get(i));\n            parallelResidualComponentSum += residuals[i][centroidIdx].parallelResidualComponent;\n        }\n\n        // SCANN sorts the subspaces by residual norm here (and adds a sorted->original subspace index map),\n        // presumably with the intent to help this converge faster, but profiling shows that almost 90% of the\n        // cost of this method is computeResiduals + initializeToMinResidualNorms, so we're not going to bother.\n\n        // Optimize until convergence\n        int MAX_ITERATIONS = 10; // borrowed from SCANN code without experimenting w/ other values\n        for (int iter = 0; iter < MAX_ITERATIONS; iter++) {\n            // loop over each subspace\n            boolean changed = false;\n            for (int i = 0; i < residuals.length; i++) {\n                int oldIdx = Byte.toUnsignedInt(result.get(i));\n                CoordinateDescentResult cdr = optimizeSingleSubspace(residuals[i], oldIdx, parallelResidualComponentSum);\n                if (cdr.newCenterIdx != oldIdx) {\n                    parallelResidualComponentSum = cdr.newParallelResidualComponent;\n                    result.set(i, (byte) cdr.newCenterIdx);\n                    changed = true;\n                }\n            }\n            // Done if nothing changed this iteration\n            if (!changed) {\n                break;\n            }\n        }\n    }",
    "comment": "    /**\n     * Encodes the input vector using the PQ codebooks, weighing parallel loss more than orthogonal loss, into\n     * the given ByteSequence.\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::assertPerfectQuantization jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testPerfectReconstruction jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::loss jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovement jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testConvergenceAnisotropic jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testLoadVersion0 jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testRefine jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveVersion0 jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovementOnce jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::generate jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveLoad",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::TestProductQuantization",
    "func_start": 248,
    "func_end": 284,
    "body_len": 36,
    "instruction": "Write a private method `encodeAnisotropic` that takes a floating-point vector and a byte sequence as input, encodes the vector using product quantization codebooks with an emphasis on minimizing parallel loss more than orthogonal loss, and stores the resulting encoding in the provided byte sequence. The method should first compute residuals between subvectors and their corresponding codebook centroids, initialize the encoding by selecting centroids that minimize residual norms, then iteratively refine the encoding using coordinate descent to optimize each subspace by considering the cumulative parallel residual component, updating the total parallel residual as centroids are changed, and terminating early if no changes occur in an iteration. The optimization should run for a maximum of 10 iterations, and the final encoded indices should be written into the byte sequence.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/quantization/ProductQuantization.java::encodeAnisotropic",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::assertPerfectQuantization",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testPerfectReconstruction",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::loss",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovement",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testConvergenceAnisotropic",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testLoadVersion0",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testRefine",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveVersion0",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testIterativeImprovementOnce",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::generate",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java::testSaveLoad"
    ],
    "test_start": 198,
    "test_end": 214,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/quantization/TestProductQuantization.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.quantization.TestProductQuantization#testSaveLoad\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "    @Test\n    public void testSaveLoad() throws Exception {\n        // Generate a PQ for random 2D vectors\n        var vectors = createRandomVectors(512, 2);\n        var pq = ProductQuantization.compute(new ListRandomAccessVectorValues(vectors, 2), 1, 256, false, 0.2f);\n\n        // Write\n        var file = File.createTempFile(\"pqtest\", \".pq\");\n        try (var out = new DataOutputStream(new FileOutputStream(file))) {\n            pq.write(out);\n        }\n        // Read\n        try (var readerSupplier = new SimpleMappedReader.Supplier(file.toPath())) {\n            var pq2 = ProductQuantization.load(readerSupplier.get());\n            Assertions.assertEquals(pq, pq2);\n        }\n    }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/graph/GraphIndexBuilder.java-removeDeletedNodes",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/GraphIndexBuilder.java",
    "func_name": "removeDeletedNodes",
    "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.disk.RandomAccessReader;\nimport io.github.jbellis.jvector.graph.GraphIndex.NodeAtLevel;\nimport io.github.jbellis.jvector.graph.SearchResult.NodeScore;\nimport io.github.jbellis.jvector.graph.diversity.VamanaDiversityProvider;\nimport io.github.jbellis.jvector.graph.similarity.BuildScoreProvider;\nimport io.github.jbellis.jvector.graph.similarity.ScoreFunction;\nimport io.github.jbellis.jvector.graph.similarity.SearchScoreProvider;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.ExceptionUtils;\nimport io.github.jbellis.jvector.util.ExplicitThreadLocal;\nimport io.github.jbellis.jvector.util.PhysicalCoreExecutor;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport org.agrona.collections.IntArrayList;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.Closeable;\nimport java.io.IOException;\nimport java.io.UncheckedIOException;\nimport java.util.*;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentSkipListSet;\nimport java.util.concurrent.ForkJoinPool;\nimport java.util.concurrent.ThreadLocalRandom;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.stream.IntStream;\n\nimport static io.github.jbellis.jvector.util.DocIdSetIterator.NO_MORE_DOCS;\nimport static java.lang.Math.*;\n\n/**\n * Builder for Concurrent GraphIndex. See {@link GraphIndex} for a high level overview, and the\n * comments to `addGraphNode` for details on the concurrent building approach.\n * <p>\n * GIB allocates scratch space and copies of the RandomAccessVectorValues for each thread\n * that calls `addGraphNode`.  These allocations are retained until the GIB itself is no longer referenced.\n * Under most conditions this is not something you need to worry about, but it does mean\n * that spawning a new Thread per call is not advisable.  This includes virtual threads.\n */\npublic class GraphIndexBuilder implements Closeable {\n    private static final int BUILD_BATCH_SIZE = 50;\n\n    private static final Logger logger = LoggerFactory.getLogger(GraphIndexBuilder.class);\n\n    private final int beamWidth;\n    private final ExplicitThreadLocal<NodeArray> naturalScratch;\n    private final ExplicitThreadLocal<NodeArray> concurrentScratch;\n\n    private final int dimension;\n    private final float neighborOverflow;\n    private final float alpha;\n    private final boolean addHierarchy;\n    private final boolean refineFinalGraph;\n\n    @VisibleForTesting\n    final OnHeapGraphIndex graph;\n\n    private final ConcurrentSkipListSet<NodeAtLevel> insertionsInProgress = new ConcurrentSkipListSet<>();\n\n    private final BuildScoreProvider scoreProvider;\n\n    private final ForkJoinPool simdExecutor;\n    private final ForkJoinPool parallelExecutor;\n\n    private final ExplicitThreadLocal<GraphSearcher> searchers;\n\n    private final Random rng;\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     * By default, refineFinalGraph = true.\n     *\n     * @param vectorValues     the vectors whose relations are represented by the graph - must provide a\n     *                         different view over those vectors than the one used to add via addGraphNode.\n     * @param M                 the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     */\n    public GraphIndexBuilder(RandomAccessVectorValues vectorValues,\n                             VectorSimilarityFunction similarityFunction,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy)\n    {\n        this(BuildScoreProvider.randomAccessScoreProvider(vectorValues, similarityFunction),\n                vectorValues.dimension(),\n                M,\n                beamWidth,\n                neighborOverflow,\n                alpha,\n                addHierarchy,\n                true);\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     *\n     * @param vectorValues     the vectors whose relations are represented by the graph - must provide a\n     *                         different view over those vectors than the one used to add via addGraphNode.\n     * @param M                 the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     */\n    public GraphIndexBuilder(RandomAccessVectorValues vectorValues,\n                             VectorSimilarityFunction similarityFunction,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph)\n    {\n        this(BuildScoreProvider.randomAccessScoreProvider(vectorValues, similarityFunction),\n                vectorValues.dimension(),\n                M,\n                beamWidth,\n                neighborOverflow,\n                alpha,\n                addHierarchy,\n                refineFinalGraph);\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     * Default executor pools are used.\n     * By default, refineFinalGraph = true.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param M                the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy)\n    {\n        this(scoreProvider, dimension, M, beamWidth, neighborOverflow, alpha, addHierarchy, true, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     * Default executor pools are used.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param M                the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph)\n    {\n        this(scoreProvider, dimension, M, beamWidth, neighborOverflow, alpha, addHierarchy, refineFinalGraph, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param M                the maximum number of connections a node can have\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     * @param simdExecutor     ForkJoinPool instance for SIMD operations, best is to use a pool with the size of\n     *                         the number of physical cores.\n     * @param parallelExecutor ForkJoinPool instance for parallel stream operations\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             int M,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph,\n                             ForkJoinPool simdExecutor,\n                             ForkJoinPool parallelExecutor)\n    {\n        this(scoreProvider, dimension, List.of(M), beamWidth, neighborOverflow, alpha, addHierarchy, refineFinalGraph, simdExecutor, parallelExecutor);\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     * Default executor pools are used.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param maxDegrees       the maximum number of connections a node can have in each layer; if fewer entries\n     *      *                  are specified than the number of layers, the last entry is used for all remaining layers.\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             List<Integer> maxDegrees,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph)\n    {\n        this(scoreProvider, dimension, maxDegrees, beamWidth, neighborOverflow, alpha, addHierarchy, refineFinalGraph, PhysicalCoreExecutor.pool(), ForkJoinPool.commonPool());\n    }\n\n    /**\n     * Reads all the vectors from vector values, builds a graph connecting them by their dense\n     * ordinals, using the given hyperparameter settings, and returns the resulting graph.\n     *\n     * @param scoreProvider    describes how to determine the similarities between vectors\n     * @param maxDegrees       the maximum number of connections a node can have in each layer; if fewer entries\n     *                         are specified than the number of layers, the last entry is used for all remaining layers.\n     * @param beamWidth        the size of the beam search to use when finding nearest neighbors.\n     * @param neighborOverflow the ratio of extra neighbors to allow temporarily when inserting a\n     *                         node. larger values will build more efficiently, but use more memory.\n     * @param alpha            how aggressive pruning diverse neighbors should be.  Set alpha &gt; 1.0 to\n     *                         allow longer edges.  If alpha = 1.0 then the equivalent of the lowest level of\n     *                         an HNSW graph will be created, which is usually not what you want.\n     * @param addHierarchy     whether we want to add an HNSW-style hierarchy on top of the Vamana index.\n     * @param refineFinalGraph whether we do a second pass over each node in the graph to refine its connections\n     * @param simdExecutor     ForkJoinPool instance for SIMD operations, best is to use a pool with the size of\n     *                         the number of physical cores.\n     * @param parallelExecutor ForkJoinPool instance for parallel stream operations\n     */\n    public GraphIndexBuilder(BuildScoreProvider scoreProvider,\n                             int dimension,\n                             List<Integer> maxDegrees,\n                             int beamWidth,\n                             float neighborOverflow,\n                             float alpha,\n                             boolean addHierarchy,\n                             boolean refineFinalGraph,\n                             ForkJoinPool simdExecutor,\n                             ForkJoinPool parallelExecutor)\n    {\n        if (maxDegrees.stream().anyMatch(i -> i <= 0)) {\n            throw new IllegalArgumentException(\"layer degrees must be positive\");\n        }\n        if (maxDegrees.size() > 1 && !addHierarchy) {\n            throw new IllegalArgumentException(\"Cannot specify multiple max degrees with addHierarchy=False\");\n        }\n        if (beamWidth <= 0) {\n            throw new IllegalArgumentException(\"beamWidth must be positive\");\n        }\n        if (neighborOverflow < 1.0f) {\n            throw new IllegalArgumentException(\"neighborOverflow must be >= 1.0\");\n        }\n        if (alpha <= 0) {\n            throw new IllegalArgumentException(\"alpha must be positive\");\n        }\n\n        this.scoreProvider = scoreProvider;\n        this.dimension = dimension;\n        this.neighborOverflow = neighborOverflow;\n        this.alpha = alpha;\n        this.addHierarchy = addHierarchy;\n        this.refineFinalGraph = refineFinalGraph;\n        this.beamWidth = beamWidth;\n        this.simdExecutor = simdExecutor;\n        this.parallelExecutor = parallelExecutor;\n\n        this.graph = new OnHeapGraphIndex(maxDegrees, neighborOverflow, new VamanaDiversityProvider(scoreProvider, alpha), BUILD_BATCH_SIZE);\n        this.searchers = ExplicitThreadLocal.withInitial(() -> {\n            var gs = new GraphSearcher(graph);\n            gs.usePruning(false);\n            return gs;\n        });\n\n        // in scratch we store candidates in reverse order: worse candidates are first\n        this.naturalScratch = ExplicitThreadLocal.withInitial(() -> new NodeArray(max(beamWidth, graph.maxDegree() + 1)));\n        this.concurrentScratch = ExplicitThreadLocal.withInitial(() -> new NodeArray(max(beamWidth, graph.maxDegree() + 1)));\n\n        this.rng = new Random(0);\n    }\n\n    // used by Cassandra when it fine-tunes the PQ codebook\n    public static GraphIndexBuilder rescore(GraphIndexBuilder other, BuildScoreProvider newProvider) {\n        var newBuilder = new GraphIndexBuilder(newProvider,\n                other.dimension,\n                other.graph.maxDegrees,\n                other.beamWidth,\n                other.neighborOverflow,\n                other.alpha,\n                other.addHierarchy,\n                other.refineFinalGraph,\n                other.simdExecutor,\n                other.parallelExecutor);\n\n        // Copy each node and its neighbors from the old graph to the new one\n        other.parallelExecutor.submit(() -> {\n            IntStream.range(0, other.graph.getIdUpperBound()).parallel().forEach(i -> {\n                // Find the highest layer this node exists in\n                int maxLayer = -1;\n                for (int lvl = 0; lvl < other.graph.layers.size(); lvl++) {\n                    if (other.graph.getNeighbors(lvl, i) == null) {\n                        break;\n                    }\n                    maxLayer = lvl;\n                }\n                if (maxLayer < 0) {\n                    return;\n                }\n\n                // Loop over 0..maxLayer, re-score neighbors for each layer\n                var sf = newProvider.searchProviderFor(i).scoreFunction();\n                for (int lvl = 0; lvl <= maxLayer; lvl++) {\n                    var oldNeighborsIt = other.graph.getNeighborsIterator(lvl, i);\n                    // Copy edges, compute new scores\n                    var newNeighbors = new NodeArray(oldNeighborsIt.size());\n                    while (oldNeighborsIt.hasNext()) {\n                        int neighbor = oldNeighborsIt.nextInt();\n                        // since we're using a different score provider, use insertSorted instead of addInOrder\n                        newNeighbors.insertSorted(neighbor, sf.similarityTo(neighbor));\n                    }\n                    newBuilder.graph.addNode(lvl, i, newNeighbors);\n                }\n            });\n        }).join();\n\n        // Set the entry node\n        newBuilder.graph.updateEntryNode(other.graph.entry());\n\n        return newBuilder;\n    }\n\n    public OnHeapGraphIndex build(RandomAccessVectorValues ravv) {\n        var vv = ravv.threadLocalSupplier();\n        int size = ravv.size();\n\n        simdExecutor.submit(() -> {\n            IntStream.range(0, size).parallel().forEach(node -> {\n                addGraphNode(node, vv.get().getVector(node));\n            });\n        }).join();\n\n        cleanup();\n        return graph;\n    }\n\n    /**\n     * Cleanup the graph by completing removal of marked-for-delete nodes, trimming\n     * neighbor sets to the advertised degree, and updating the entry node.\n     * <p>\n     * Uses default threadpool to process nodes in parallel.  There is currently no way to restrict this to a single thread.\n     * <p>\n     * Must be called before writing to disk.\n     * <p>\n     * May be called multiple times, but should not be called during concurrent modifications to the graph.\n     */\n    public void cleanup() {\n        if (graph.size(0) == 0) {\n            return;\n        }\n        graph.validateEntryNode(); // sanity check before we start\n\n        // purge deleted nodes.\n        // backlinks can cause neighbors to soft-overflow, so do this before neighbors cleanup\n        removeDeletedNodes();\n\n        if (graph.size(0) == 0) {\n            // After removing all the deleted nodes, we might end up with an empty graph.\n            // The calls below expect a valid entry node, but we do not have one right now.\n            return;\n        }\n\n        if (refineFinalGraph && graph.getMaxLevel() > 0) {\n            // improve connections on everything in L1 & L0.\n            // It may be helpful for 2D use cases, but empirically it seems unnecessary for high-dimensional vectors.\n            // It may bring a slight improvement in recall for small maximum degrees,\n            // but it can be easily be compensated by using a slightly larger neighborOverflow.\n            parallelExecutor.submit(() -> {\n                graph.nodeStream(1).parallel().forEach(this::improveConnections);\n            }).join();\n        }\n\n        // clean up overflowed neighbor lists\n        parallelExecutor.submit(() -> {\n            IntStream.range(0, graph.getIdUpperBound()).parallel().forEach(id -> {\n                for (int layer = 0; layer < graph.layers.size(); layer++) {\n                    graph.layers.get(layer).enforceDegree(id);\n                }\n            });\n        }).join();\n    }\n\n    private void improveConnections(int node) {\n        var ssp = scoreProvider.searchProviderFor(node);\n        var bits = new ExcludingBits(node);\n        try (var gs = searchers.get()) {\n            gs.initializeInternal(ssp, graph.entry(), bits);\n            var acceptedBits = Bits.intersectionOf(bits, gs.getView().liveNodes());\n\n            // Move downward from entry.level to 0\n            for (int lvl = graph.entry().level; lvl >= 0; lvl--) {\n                // This additional call seems redundant given that we have already initialized an ssp above.\n                // However, there is a subtle interplay between the ssp of the search and the ssp used in insertDiverse.\n                // Do not remove this line.\n                ssp = scoreProvider.searchProviderFor(node);\n\n                if (graph.layers.get(lvl).get(node) != null) {\n                    gs.searchOneLayer(ssp, beamWidth, 0.0f, lvl, acceptedBits);\n\n                    var candidates = new NodeArray(gs.approximateResults.size());\n                    gs.approximateResults.foreach(candidates::insertSorted);\n                    var newNeighbors = graph.layers.get(lvl).insertDiverse(node, candidates);\n                    graph.layers.get(lvl).backlink(newNeighbors, node, neighborOverflow);\n                } else {\n                    gs.searchOneLayer(ssp, 1, 0.0f, lvl, acceptedBits);\n                }\n                gs.setEntryPointsFromPreviousLayer();\n            }\n        } catch (IOException e) {\n            throw new UncheckedIOException(e);\n        }\n    }\n\n    public OnHeapGraphIndex getGraph() {\n        return graph;\n    }\n\n    /**\n     * Number of inserts in progress, across all threads.  Useful as a sanity check\n     * when calling non-threadsafe methods like cleanup().  (Do not use it to try to\n     * _prevent_ races, only to detect them.)\n     */\n    public int insertsInProgress() {\n        return insertionsInProgress.size();\n    }\n\n    @Deprecated\n    public long addGraphNode(int node, RandomAccessVectorValues ravv) {\n        return addGraphNode(node, ravv.getVector(node));\n    }\n\n    /**\n     * Assigns a hierarchy level to a node at random. It follows the HNSW sampling strategy.\n     * @return The assigned level\n     */\n    private int getRandomGraphLevel() {\n        double ml;\n        double randDouble;\n        if (addHierarchy) {\n            ml = graph.getDegree(0) == 1 ? 1 : 1 / log(1.0 * graph.getDegree(0));\n            do {\n                randDouble = this.rng.nextDouble();  // avoid 0 value, as log(0) is undefined\n            } while (randDouble == 0.0);\n        } else {\n            ml = 0;\n            randDouble = 0;\n        }\n        return ((int) (-log(randDouble) * ml));\n    }\n\n    /**\n     * Inserts a node with the given vector value to the graph.\n     *\n     * <p>To allow correctness under concurrency, we track in-progress updates in a\n     * ConcurrentSkipListSet. After adding ourselves, we take a snapshot of this set, and consider all\n     * other in-progress updates as neighbor candidates.\n     *\n     * @param node the node ID to add\n     * @param vector the vector to add\n     * @return an estimate of the number of extra bytes used by the graph after adding the given node\n     */\n    public long addGraphNode(int node, VectorFloat<?> vector) {\n        var ssp = scoreProvider.searchProviderFor(vector);\n        return addGraphNode(node, ssp);\n    }\n\n    /**\n     * Inserts a node with the given vector value to the graph.\n     *\n     * <p>To allow correctness under concurrency, we track in-progress updates in a\n     * ConcurrentSkipListSet. After adding ourselves, we take a snapshot of this set, and consider all\n     * other in-progress updates as neighbor candidates.\n     *\n     * @param node the node ID to add\n     * @param searchScoreProvider a SearchScoreProvider corresponding to the vector to add.\n     *                            It needs to be compatible with the BuildScoreProvider provided to the constructor\n     * @return an estimate of the number of extra bytes used by the graph after adding the given node\n     */\n    public long addGraphNode(int node, SearchScoreProvider searchScoreProvider) {\n        var nodeLevel = new NodeAtLevel(getRandomGraphLevel(), node);\n        // do this before adding to in-progress, so a concurrent writer checking\n        // the in-progress set doesn't have to worry about uninitialized neighbor sets\n        graph.addNode(nodeLevel);\n\n        insertionsInProgress.add(nodeLevel);\n        var inProgressBefore = insertionsInProgress.clone();\n        try (var gs = searchers.get()) {\n            gs.setView(graph.getView()); // new snapshot\n            var naturalScratchPooled = naturalScratch.get();\n            var concurrentScratchPooled = concurrentScratch.get();\n\n            var bits = new ExcludingBits(nodeLevel.node);\n            var entry = graph.entry();\n            SearchResult result;\n            if (entry == null) {\n                result = new SearchResult(new NodeScore[] {}, 0, 0, 0, 0, 0);\n            } else {\n                gs.initializeInternal(searchScoreProvider, entry, bits);\n\n                // Move downward from entry.level to 1\n                for (int lvl = entry.level; lvl > 0; lvl--) {\n                    if (lvl > nodeLevel.level) {\n                        gs.searchOneLayer(searchScoreProvider, 1, 0.0f, lvl, gs.getView().liveNodes());\n                    } else {\n                        gs.searchOneLayer(searchScoreProvider, beamWidth, 0.0f, lvl, gs.getView().liveNodes());\n                        NodeScore[] neighbors = new NodeScore[gs.approximateResults.size()];\n                        AtomicInteger index = new AtomicInteger();\n                        // TODO extract an interface that lets us avoid the copy here and in toScratchCandidates\n                        gs.approximateResults.foreach((neighbor, score) -> {\n                            neighbors[index.getAndIncrement()] = new NodeScore(neighbor, score);\n                        });\n                        Arrays.sort(neighbors);\n                        updateNeighborsOneLayer(lvl, nodeLevel.node, neighbors, naturalScratchPooled, inProgressBefore, concurrentScratchPooled, searchScoreProvider);\n                    }\n                    gs.setEntryPointsFromPreviousLayer();\n                }\n\n                // Now do the main search at layer 0\n                result = gs.resume(beamWidth, beamWidth, 0.0f, 0.0f);\n            }\n\n            updateNeighborsOneLayer(0, nodeLevel.node, result.getNodes(), naturalScratchPooled, inProgressBefore, concurrentScratchPooled, searchScoreProvider);\n\n            graph.markComplete(nodeLevel);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        } finally {\n            insertionsInProgress.remove(nodeLevel);\n        }\n\n        return IntStream.range(0, nodeLevel.level).mapToLong(graph::ramBytesUsedOneNode).sum();\n    }\n\n    private void updateNeighborsOneLayer(int layer, int node, NodeScore[] neighbors, NodeArray naturalScratchPooled, ConcurrentSkipListSet<NodeAtLevel> inProgressBefore, NodeArray concurrentScratchPooled, SearchScoreProvider ssp) {\n        // Update neighbors with these candidates.\n        // The DiskANN paper calls for using the entire set of visited nodes along the search path as\n        // potential candidates, but in practice we observe neighbor lists being completely filled using\n        // just the topK results.  (Since the Robust Prune algorithm prioritizes closer neighbors,\n        // this means that considering additional nodes from the search path, that are by definition\n        // farther away than the ones in the topK, would not change the result.)\n        var natural = toScratchCandidates(neighbors, naturalScratchPooled);\n        var concurrent = getConcurrentCandidates(layer, node, inProgressBefore, concurrentScratchPooled, ssp.scoreFunction());\n        updateNeighbors(layer, node, natural, concurrent);\n    }\n\n    @VisibleForTesting\n    public void setEntryPoint(int level, int node) {\n        graph.updateEntryNode(new NodeAtLevel(level, node));\n    }\n\n    public void markNodeDeleted(int node) {\n        graph.markDeleted(node);\n    }",
    "func": "    public synchronized long removeDeletedNodes() {\n        // Take a snapshot of the nodes to delete\n        var toDelete = graph.getDeletedNodes().copy();\n        var nRemoved = toDelete.cardinality();\n        if (nRemoved == 0) {\n            return 0;\n        }\n\n        for (int currentLevel = 0; currentLevel < graph.layers.size(); currentLevel++) {\n            final int level = currentLevel;  // Create effectively final copy for lambda\n            // Compute new edges to insert.  If node j is deleted, we add edges (i, k)\n            // whenever (i, j) and (j, k) are directed edges in the current graph.  This\n            // strategy is proposed in \"FreshDiskANN: A Fast and Accurate Graph-Based\n            // ANN Index for Streaming Similarity Search\" section 4.2.\n            var newEdges = new ConcurrentHashMap<Integer, Set<Integer>>(); // new edges for key k are values v\n            parallelExecutor.submit(() -> {\n                IntStream.range(0, graph.getIdUpperBound()).parallel().forEach(i -> {\n                    if (toDelete.get(i)) {\n                        return;\n                    }\n                    for (var it = graph.getNeighborsIterator(level, i); it.hasNext(); ) {\n                        var j = it.nextInt();\n                        if (toDelete.get(j)) {\n                            var newEdgesForI = newEdges.computeIfAbsent(i, __ -> ConcurrentHashMap.newKeySet());\n                            for (var jt = graph.getNeighborsIterator(level, j); jt.hasNext(); ) {\n                                int k = jt.nextInt();\n                                if (i != k && !toDelete.get(k)) {\n                                    newEdgesForI.add(k);\n                                }\n                            }\n                        }\n                    }\n                });\n            }).join();\n\n            // Remove deleted nodes from neighbors lists;\n            // Score the new edges, and connect the most diverse ones as neighbors\n            simdExecutor.submit(() -> {\n                newEdges.entrySet().stream().parallel().forEach(e -> {\n                    // turn the new edges into a NodeArray\n                    int node = e.getKey();\n                    // each deleted node has ALL of its neighbors added as candidates, so using approximate\n                    // scoring and then re-scoring only the best options later makes sense here\n                    var sf = scoreProvider.searchProviderFor(node).scoreFunction();\n                    var candidates = new NodeArray(graph.getDegree(level));\n                    for (var k : e.getValue()) {\n                        candidates.insertSorted(k, sf.similarityTo(k));\n                    }\n\n                    // it's unlikely, but possible, that all the potential replacement edges were to nodes that have also\n                    // been deleted.  if that happens, keep the graph connected by adding random edges.\n                    // (this is overly conservative -- really what we care about is that the end result of\n                    // replaceDeletedNeighbors not be empty -- but we want to avoid having the node temporarily\n                    // neighborless while concurrent searches run.  empirically, this only results in a little extra work.)\n                    if (candidates.size() == 0) {\n                        var R = ThreadLocalRandom.current();\n                        // doing actual sampling-without-replacement is expensive so we'll loop a fixed number of times instead\n                        for (int i = 0; i < 2 * graph.getDegree(level); i++) {\n                            int randomNode = R.nextInt(graph.getIdUpperBound());\n                            while(toDelete.get(randomNode)) {\n                                randomNode = R.nextInt(graph.getIdUpperBound());\n                            }\n                            if (randomNode != node && !candidates.contains(randomNode) && graph.layers.get(level).contains(randomNode)) {\n                                float score = sf.similarityTo(randomNode);\n                                candidates.insertSorted(randomNode, score);\n                            }\n                            if (candidates.size() == graph.getDegree(level)) {\n                                break;\n                            }\n                        }\n                    }\n\n                    // remove edges to deleted nodes and add the new connections, maintaining diversity\n                    graph.layers.get(level).replaceDeletedNeighbors(node, toDelete, candidates);\n                });\n            }).join();\n        }\n\n        // Generally we want to keep entryPoint update and node removal distinct, because both can be expensive,\n        // but if the entry point was deleted then we have no choice\n        if (toDelete.get(graph.entry().node)) {\n            // pick a random node at the top layer\n            int newLevel = graph.getMaxLevel();\n            int newEntry = -1;\n            outer:\n            while (newLevel >= 0) {\n                for (var it = graph.getNodes(newLevel); it.hasNext(); ){\n                    int i = it.nextInt();\n                    if (!toDelete.get(i)) {\n                        newEntry = i;\n                        break outer;\n                    }\n                }\n                newLevel--;\n            }\n\n            graph.updateEntryNode(newEntry >= 0 ? new NodeAtLevel(newLevel, newEntry) : null);\n        }\n\n        long memorySize = 0;\n\n        // Remove the deleted nodes from the graph\n        assert toDelete.cardinality() == nRemoved : \"cardinality changed\";\n        for (int i = toDelete.nextSetBit(0); i != NO_MORE_DOCS; i = toDelete.nextSetBit(i + 1)) {\n            int nDeletions = graph.removeNode(i);\n            for (var iLayer = 0; iLayer < nDeletions; iLayer++) {\n                memorySize += graph.ramBytesUsedOneNode(iLayer);\n            }\n        }\n        return memorySize;\n    }",
    "comment": "    /**\n     * Remove nodes marked for deletion from the graph, and update neighbor lists\n     * to maintain connectivity.  Not threadsafe with respect to other modifications;\n     * the `synchronized` flag only prevents concurrent calls to this method.\n     *\n     * @return approximate size of memory no longer used\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestDeletions.java::testMarkingAllNodesAsDeleted",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestDeletions.java::TestDeletions",
    "func_start": 633,
    "func_end": 743,
    "body_len": 110,
    "instruction": "Write a synchronized method `removeDeletedNodes()` that removes nodes marked for deletion from a layered graph structure, preserves connectivity by adding new edges between neighbors of deleted nodes (using the edge promotion strategy described in 'FreshDiskANN'), updates neighbor lists with diverse, high-scoring replacements, handles edge cases where replacement candidates are insufficient by adding random valid edges, updates the graph's entry point if it was deleted by selecting a valid replacement from the highest available layer, and returns an approximate count of the memory (in bytes) freed by removing the deleted nodes. The implementation must use parallel streams for performance, maintain thread safety for concurrent execution of this method, and coordinate with the graphs scoring, neighbor management, and memory tracking systems.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/GraphIndexBuilder.java::removeDeletedNodes",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestDeletions.java::testMarkingAllNodesAsDeleted"
    ],
    "test_start": 127,
    "test_end": 131,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestDeletions.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestDeletions#testMarkingAllNodesAsDeleted\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "    @Test\n    public void testMarkingAllNodesAsDeleted() {\n        testMarkingAllNodesAsDeleted(false);\n        testMarkingAllNodesAsDeleted(true);\n    }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/graph/disk/OnDiskGraphIndexWriter.java-writeInline",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/disk/OnDiskGraphIndexWriter.java",
    "func_name": "writeInline",
    "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph.disk;\n\nimport io.github.jbellis.jvector.disk.BufferedRandomAccessWriter;\nimport io.github.jbellis.jvector.disk.RandomAccessWriter;\nimport io.github.jbellis.jvector.graph.GraphIndex;\nimport io.github.jbellis.jvector.graph.OnHeapGraphIndex;\nimport io.github.jbellis.jvector.graph.disk.feature.Feature;\nimport io.github.jbellis.jvector.graph.disk.feature.FeatureId;\nimport io.github.jbellis.jvector.graph.disk.feature.InlineVectors;\nimport io.github.jbellis.jvector.graph.disk.feature.NVQ;\nimport io.github.jbellis.jvector.graph.disk.feature.SeparatedFeature;\nimport io.github.jbellis.jvector.graph.disk.feature.SeparatedNVQ;\nimport io.github.jbellis.jvector.graph.disk.feature.SeparatedVectors;\n\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\nimport java.nio.file.Path;\nimport java.util.EnumMap;\nimport java.util.Map;\nimport java.util.function.IntFunction;\n\n/**\n * Writes a graph index to disk in a format that can be loaded as an OnDiskGraphIndex.\n * <p>\n * The serialization process follows these steps:\n * \n * 1. File Layout:\n *    - CommonHeader: Contains version, dimension, entry node, and layer information\n *    - Header with Features: Contains feature-specific headers\n *    - Layer 0 data: Contains node ordinals, inline features, and edges for all nodes\n *    - Higher layer data (levels 1..N): Contains sparse node ordinals and edges\n *    - Separated features: Contains feature data stored separately from nodes\n * \n * 2. Serialization Process:\n *    - First, a placeholder header is written to reserve space\n *    - For each node in layer 0:\n *      - Write node ordinal\n *      - Write inline features (vectors, quantized data, etc.)\n *      - Write neighbor count and neighbor ordinals\n *    - For each higher layer (1..N):\n *      - Write only nodes that exist in that layer\n *      - For each node: write ordinal, neighbor count, and neighbor ordinals\n *    - For each separated feature:\n *      - Write feature data for all nodes sequentially\n *    - Finally, rewrite the header with correct offsets\n * \n * 3. Ordinal Mapping:\n *    - The writer uses an OrdinalMapper to map between original node IDs and \n *      the sequential IDs used in the on-disk format\n *    - This allows for compaction (removing \"holes\" from deleted nodes)\n *    - It also enables custom ID mapping schemes for specific use cases\n * \n * The class supports incremental writing through the writeInline method, which\n * allows writing features for individual nodes without writing the entire graph.\n */\npublic class OnDiskGraphIndexWriter extends AbstractGraphIndexWriter<RandomAccessWriter> {\n    private final long startOffset;\n\n    OnDiskGraphIndexWriter(RandomAccessWriter randomAccessWriter,\n                                   int version,\n                                   long startOffset,\n                                   GraphIndex graph,\n                                   OrdinalMapper oldToNewOrdinals,\n                                   int dimension,\n                                   EnumMap<FeatureId, Feature> features)\n    {\n        super(randomAccessWriter, version, graph, oldToNewOrdinals, dimension, features);\n        this.startOffset = startOffset;\n    }\n\n    /**\n     * Close the view and the output stream. Unlike the super method, for backwards compatibility reasons,\n     * this method assumes ownership of the output stream.\n     */\n    @Override\n    public synchronized void close() throws IOException {\n        view.close();\n        out.close();\n    }\n\n    /**\n     * Caller should synchronize on this OnDiskGraphIndexWriter instance if mixing usage of the\n     * output with calls to any of the synchronized methods in this class.\n     * <p>\n     * Provided for callers (like Cassandra) that want to add their own header/footer to the output.\n     */\n    public RandomAccessWriter getOutput() {\n        return out;\n    }",
    "func": "    public synchronized void writeInline(int ordinal, Map<FeatureId, Feature.State> stateMap) throws IOException\n    {\n        for (var featureId : stateMap.keySet()) {\n            if (!featureMap.containsKey(featureId)) {\n                throw new IllegalArgumentException(String.format(\"Feature %s not configured for index\", featureId));\n            }\n        }\n\n        out.seek(featureOffsetForOrdinal(ordinal));\n\n        for (var feature : inlineFeatures) {\n            var state = stateMap.get(feature.id());\n            if (state == null) {\n                out.seek(out.position() + feature.featureSize());\n            } else {\n                feature.writeInline(out, state);\n            }\n        }\n\n        maxOrdinalWritten = Math.max(maxOrdinalWritten, ordinal);\n    }",
    "comment": "    /**\n     * Write the inline features of the given ordinal to the output at the correct offset.\n     * Nothing else is written (no headers, no edges).  The output IS NOT flushed.\n     * <p>\n     * Note: the ordinal given is implicitly a \"new\" ordinal in the sense of the OrdinalMapper,\n     * but since no nodes or edges are involved (we just write the given State to the index file),\n     * the mapper is not invoked.\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testIncrementalWrites jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testV0WriteIncremental",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::TestOnDiskGraphIndex",
    "func_start": 115,
    "func_end": 135,
    "body_len": 20,
    "instruction": "Write a function `writeInline(int ordinal, Map<FeatureId, Feature.State> stateMap)` that writes only the inline feature data associated with the given ordinal to the appropriate position in an output stream, based on a precomputed offset. The function should validate that all feature IDs in the state map are configured in the index, seek to the correct file position using the ordinal, and then serialize each feature's state inlineif a feature's state is present, it should be written using the feature's specific write method; otherwise, the output position should be advanced by the feature's size to preserve alignment. The function should also update the maximum ordinal written so far but must not flush the output stream.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/disk/OnDiskGraphIndexWriter.java::writeInline",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testIncrementalWrites",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testV0WriteIncremental"
    ],
    "test_start": 462,
    "test_end": 526,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.disk.TestOnDiskGraphIndex#testIncrementalWrites\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "    @Test\n    public void testIncrementalWrites() throws IOException {\n        // generate 1000 node random graph\n        var graph = new TestUtil.RandomlyConnectedGraphIndex(1000, 32, getRandom());\n        var vectors = TestUtil.createRandomVectors(1000, 256);\n        var ravv = new ListRandomAccessVectorValues(vectors, 256);\n\n        // write out graph all at once\n        var bulkPath = testDirectory.resolve(\"bulk_graph\");\n        OnDiskGraphIndex.write(graph, ravv, bulkPath);\n\n        // write incrementally\n        var incrementalPath = testDirectory.resolve(\"bulk_graph\");\n        try (var writer = new OnDiskGraphIndexWriter.Builder(graph, incrementalPath)\n                .with(new InlineVectors(ravv.dimension()))\n                .build())\n        {\n            // write inline vectors incrementally\n            for (int i = 0; i < vectors.size(); i++) {\n                var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(i)));\n                writer.writeInline(i, state);\n            }\n\n            // write graph structure\n            writer.write(Map.of());\n        }\n\n        // all-at-once and incremental builds should be identical on disk\n        var bulkContents = Files.readAllBytes(bulkPath);\n        var incrementalContents = Files.readAllBytes(incrementalPath);\n        assertArrayEquals(bulkContents, incrementalContents);\n\n        // write incrementally and add Fused ADC Feature\n        var incrementalFadcPath = testDirectory.resolve(\"incremental_graph\");\n        var pq = ProductQuantization.compute(ravv, 64, 256, false);\n        var pqv = (PQVectors) pq.encodeAll(ravv);\n        try (var writer = new OnDiskGraphIndexWriter.Builder(graph, incrementalFadcPath)\n                .with(new InlineVectors(ravv.dimension()))\n                .with(new FusedADC(graph.getDegree(0), pq))\n                .build())\n        {\n            // write inline vectors incrementally\n            for (int i = 0; i < vectors.size(); i++) {\n                var state = Feature.singleState(FeatureId.INLINE_VECTORS, new InlineVectors.State(ravv.getVector(i)));\n                writer.writeInline(i, state);\n            }\n            // write graph structure, fused ADC\n            writer.write(Feature.singleStateFactory(FeatureId.FUSED_ADC, i -> new FusedADC.State(graph.getView(), pqv, i)));\n            writer.write(Map.of());\n        }\n\n        // graph and vectors should be identical\n        try (var bulkReaderSupplier = new SimpleMappedReader.Supplier(bulkPath.toAbsolutePath());\n             var bulkGraph = OnDiskGraphIndex.load(bulkReaderSupplier);\n             var incrementalReaderSupplier = new SimpleMappedReader.Supplier(incrementalFadcPath.toAbsolutePath());\n             var incrementalGraph = OnDiskGraphIndex.load(incrementalReaderSupplier);\n             var incrementalView = incrementalGraph.getView())\n        {\n            assertTrue(OnDiskGraphIndex.areHeadersEqual(incrementalGraph, bulkGraph));\n            TestUtil.assertGraphEquals(incrementalGraph, bulkGraph); // incremental and bulk graph should have same structure\n            validateVectors(incrementalView, ravv); // inline vectors should be the same\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/graph/disk/OnDiskGraphIndex.java-write",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/disk/OnDiskGraphIndex.java",
    "func_name": "write",
    "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph.disk;\n\nimport io.github.jbellis.jvector.annotations.VisibleForTesting;\nimport io.github.jbellis.jvector.disk.RandomAccessReader;\nimport io.github.jbellis.jvector.disk.ReaderSupplier;\nimport io.github.jbellis.jvector.graph.GraphIndex;\nimport io.github.jbellis.jvector.graph.NodesIterator;\nimport io.github.jbellis.jvector.graph.RandomAccessVectorValues;\nimport io.github.jbellis.jvector.graph.disk.feature.Feature;\nimport io.github.jbellis.jvector.graph.disk.feature.FeatureId;\nimport io.github.jbellis.jvector.graph.disk.feature.FeatureSource;\nimport io.github.jbellis.jvector.graph.disk.feature.FusedADC;\nimport io.github.jbellis.jvector.graph.disk.feature.InlineVectors;\nimport io.github.jbellis.jvector.graph.disk.feature.NVQ;\nimport io.github.jbellis.jvector.graph.disk.feature.SeparatedFeature;\nimport io.github.jbellis.jvector.graph.similarity.ScoreFunction;\nimport io.github.jbellis.jvector.util.Accountable;\nimport org.agrona.collections.Int2ObjectHashMap;\nimport java.util.ArrayList;\nimport io.github.jbellis.jvector.util.Bits;\nimport io.github.jbellis.jvector.util.RamUsageEstimator;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.VectorizationProvider;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\nimport java.io.IOException;\nimport java.io.UncheckedIOException;\nimport java.nio.file.Path;\nimport java.util.EnumMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport static io.github.jbellis.jvector.graph.disk.OnDiskSequentialGraphIndexWriter.*;\n\n/**\n * A class representing a graph index stored on disk. The base graph contains only graph structure.\n * <p> * The base graph\n\n * This graph may be extended with additional features, which are stored inline in the graph and in headers.\n * At runtime, this class may choose the best way to use these features.\n */\npublic class OnDiskGraphIndex implements GraphIndex, AutoCloseable, Accountable\n{\n    private static final Logger logger = LoggerFactory.getLogger(OnDiskGraphIndex.class);\n    public static final int CURRENT_VERSION = 5;\n    static final int MAGIC = 0xFFFF0D61; // FFFF to distinguish from old graphs, which should never start with a negative size \"ODGI\"\n    static final VectorTypeSupport vectorTypeSupport = VectorizationProvider.getInstance().getVectorTypeSupport();\n    final ReaderSupplier readerSupplier;\n    final int version;\n    final int dimension;\n    final NodeAtLevel entryNode;\n    final int idUpperBound;\n    final int inlineBlockSize; // total size of all inline elements contributed by features\n    final EnumMap<FeatureId, ? extends Feature> features;\n    final EnumMap<FeatureId, Integer> inlineOffsets;\n    private final List<CommonHeader.LayerInfo> layerInfo;\n    // offset of L0 adjacency data\n    private final long neighborsOffset;\n    /** For layers > 0, store adjacency fully in memory. */\n    private final AtomicReference<List<Int2ObjectHashMap<int[]>>> inMemoryNeighbors;\n\n    OnDiskGraphIndex(ReaderSupplier readerSupplier, Header header, long neighborsOffset)\n    {\n        this.readerSupplier = readerSupplier;\n        this.version = header.common.version;\n        this.layerInfo = header.common.layerInfo;\n        this.dimension = header.common.dimension;\n        this.entryNode = new NodeAtLevel(header.common.layerInfo.size() - 1, header.common.entryNode);\n        this.idUpperBound = header.common.idUpperBound;\n        this.features = header.features;\n        this.neighborsOffset = neighborsOffset;\n        var inlineBlockSize = 0;\n        inlineOffsets = new EnumMap<>(FeatureId.class);\n        for (var entry : features.entrySet()) {\n            var feature = entry.getValue();\n            if (!(feature instanceof SeparatedFeature)) {\n                inlineOffsets.put(entry.getKey(), inlineBlockSize);\n                inlineBlockSize += feature.featureSize();\n            }\n        }\n        this.inlineBlockSize = inlineBlockSize;\n        inMemoryNeighbors = new AtomicReference<>(null);\n    }\n\n    private List<Int2ObjectHashMap<int[]>> getInMemoryLayers(RandomAccessReader in) throws IOException {\n        return inMemoryNeighbors.updateAndGet(current -> {\n            if (current != null) {\n                return current;\n            }\n            try {\n                return loadInMemoryLayers(in);\n            } catch (IOException e) {\n                throw new UncheckedIOException(e);\n            }\n        });\n    }\n\n    private List<Int2ObjectHashMap<int[]>> loadInMemoryLayers(RandomAccessReader in) throws IOException {\n        var imn = new ArrayList<Int2ObjectHashMap<int[]>>(layerInfo.size());\n        // For levels > 0, we load adjacency into memory\n        imn.add(null); // L0 placeholder so we don't have to mangle indexing\n        long L0size = 0;\n        L0size = idUpperBound * (inlineBlockSize + Integer.BYTES * (1L + 1L + layerInfo.get(0).degree));\n        in.seek(neighborsOffset + L0size);\n\n        for (int lvl = 1; lvl < layerInfo.size(); lvl++) {\n            CommonHeader.LayerInfo info = layerInfo.get(lvl);\n            Int2ObjectHashMap<int[]> edges = new Int2ObjectHashMap<>();\n\n            for (int i = 0; i < info.size; i++) {\n                int nodeId = in.readInt();\n                assert nodeId >= 0 && nodeId < idUpperBound :\n                        String.format(\"Node ID %d out of bounds for layer %d\", nodeId, lvl);\n                int neighborCount = in.readInt();\n                assert neighborCount >= 0 && neighborCount <= info.degree\n                        : String.format(\"Node %d neighborCount %d > M %d\", nodeId, neighborCount, info.degree);\n                int[] neighbors = new int[neighborCount];\n                in.read(neighbors, 0, neighborCount);\n\n                // skip any padding up to 'degree' neighbors\n                int skip = info.degree - neighborCount;\n                if (skip > 0) in.seek(in.getPosition() + ((long) skip * Integer.BYTES));\n\n                edges.put(nodeId, neighbors);\n            }\n            imn.add(edges);\n        }\n        return imn;\n    }\n\n    /**\n     * Load an index from the given reader supplier where header and graph are located on the same file,\n     * where the index starts at `offset`.\n     *\n     * @param readerSupplier the reader supplier to use to read the graph and index.\n     * @param offset the offset in bytes from the start of the file where the index starts.\n     */\n    public static OnDiskGraphIndex load(ReaderSupplier readerSupplier, long offset) {\n        try (var reader = readerSupplier.get()) {\n            logger.debug(\"Loading OnDiskGraphIndex from offset={}\", offset);\n            var header = Header.load(reader, offset);\n\n            logger.debug(\"Header loaded: version={}, dimension={}, entryNode={}, layerInfoCount={}\",\n                    header.common.version, header.common.dimension, header.common.entryNode, header.common.layerInfo.size());\n            logger.debug(\"Position after reading header={}\",\n                    reader.getPosition());\n            if (header.common.version >= 5) {\n                logger.debug(\"Version 5+ onwards uses a footer instead of header for metadata. Loading from footer\");\n                return loadFromFooter(readerSupplier, reader.getPosition());\n            } else {\n                return new OnDiskGraphIndex(readerSupplier, header, reader.getPosition());\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(\"Error initializing OnDiskGraph at offset \" + offset, e);\n        }\n    }\n\n    /**\n     * Load an index from the given reader supplier where header and graph are located on the same file at offset 0.\n     *\n     * @param readerSupplier the reader supplier to use to read the graph index.\n     */\n    public static OnDiskGraphIndex load(ReaderSupplier readerSupplier) {\n        return load(readerSupplier, 0);\n    }\n\n    /**\n     * Load an index from the given reader supplier where we will use the footer of the file to find the header.\n     * In this implementation we will assume that the {@link ReaderSupplier} must vend slices of IndexOutput that contain the graph index and nothing else.\n     * @param readerSupplier the reader supplier to use to read the graph index.\n     *                       This reader supplier must vend slices of IndexOutput that contain the graph index and nothing else.\n     * @return the loaded index.\n     */\n    private static OnDiskGraphIndex loadFromFooter(ReaderSupplier readerSupplier, long neighborsOffset) {\n        try (var in = readerSupplier.get()) {\n            final long magicOffset = in.length() - FOOTER_MAGIC_SIZE;\n            logger.debug(\"Loading OnDiskGraphIndex footer from offset={}\", magicOffset);\n            in.seek(magicOffset);\n            int version = in.readInt();\n            if (version != FOOTER_MAGIC) {\n                logger.error(\"Found an invalid footer, magic doesn't match any known version: {}\", version);\n                throw new RuntimeException(\"Unsupported version \" + version);\n            }\n            final long metadataOffset = magicOffset - FOOTER_OFFSET_SIZE;\n            logger.debug(\"Loading header offset={}\", metadataOffset);\n            in.seek(metadataOffset);\n            final long headerOffset = in.readLong();\n            logger.debug(\"Loading OnDiskGraphIndex header from offset={}\", headerOffset);\n            var header = Header.load(in, headerOffset);\n            logger.debug(\"Header loaded: version={}, dimension={}, entryNode={}, layerInfoCount={}, Position after reading header={}\",\n                    header.common.version,\n                    header.common.dimension,\n                    header.common.entryNode,\n                    header.common.layerInfo.size(),\n                    in.getPosition());\n            return new OnDiskGraphIndex(readerSupplier, header, neighborsOffset);\n        } catch (Exception e) {\n            throw new RuntimeException(\"Error initializing OnDiskGraph\", e);\n        }\n    }\n\n    public Set<FeatureId> getFeatureSet() {\n        return features.keySet();\n    }\n\n    public int getDimension() {\n        return dimension;\n    }\n\n    @Override\n    public int size(int level) {\n        return layerInfo.get(level).size;\n    }\n\n    @Override\n    public int getDegree(int level) {\n        return layerInfo.get(level).degree;\n    }\n\n    @Override\n    public int getIdUpperBound() {\n        return idUpperBound;\n    }\n\n    @Override\n    public NodesIterator getNodes(int level) {\n        int size = size(level);\n        int maxDegree = getDegree(level);\n\n        long layer0NodeSize = (long) Integer.BYTES // ids\n                + inlineBlockSize // inline elements\n                + (Integer.BYTES * (long) (getDegree(0) + 1));\n        long layerUpperNodeSize = (long) Integer.BYTES // ids\n                + (Integer.BYTES * (long) (maxDegree + 1)); // neighbor count + neighbors)\n        long thisLayerNodeSide = level == 0? layer0NodeSize : layerUpperNodeSize;\n\n        long layerOffset = neighborsOffset;\n        layerOffset += level > 0? layer0NodeSize * size(0) : 0;\n        for (int lvl = 1; lvl < level; lvl++) {\n            layerOffset += layerUpperNodeSize * size(lvl);\n        }\n\n        try (var reader = readerSupplier.get()) {\n            if (level > 0) {\n                var imn = getInMemoryLayers(reader);\n                var validIntegerNodes = imn.get(level).keySet().stream().sorted().toArray(Integer[]::new);\n                var validNodes = new int[validIntegerNodes.length];\n                for (int i = 0; i < validNodes.length; i++) {\n                    validNodes[i] = validIntegerNodes[i];\n                }\n                return new NodesIterator.ArrayNodesIterator(validNodes, size);\n            }\n\n            int[] validNodes = new int[size(level)];\n            int upperBound = level == 0 ? getIdUpperBound() : size(level);\n            int pos = 0;\n            for (int nodeOrd = 0; nodeOrd < upperBound; nodeOrd++) {\n                long nodeOffset = layerOffset + (nodeOrd * thisLayerNodeSide);\n                reader.seek(nodeOffset);\n                int nodeId = reader.readInt();\n                if (nodeId != -1) {\n                    validNodes[pos++] = nodeId;\n                }\n            }\n            return new NodesIterator.ArrayNodesIterator(validNodes, size);\n        } catch (IOException e) {\n            throw new UncheckedIOException(e);\n        }\n    }\n\n    @Override\n    public long ramBytesUsed() {\n        return Long.BYTES + 6 * Integer.BYTES + RamUsageEstimator.NUM_BYTES_OBJECT_REF\n                + (long) 2 * RamUsageEstimator.NUM_BYTES_OBJECT_REF * FeatureId.values().length;\n    }\n\n    public void close() throws IOException {\n        // caller is responsible for closing ReaderSupplier\n    }\n\n    @Override\n    public String toString() {\n        return String.format(\"OnDiskGraphIndex(layers=%s, entryPoint=%s, features=%s)\", layerInfo, entryNode,\n                features.keySet().stream().map(Enum::name).collect(Collectors.joining(\",\")));\n    }\n\n    @Override\n    public int getMaxLevel() {\n        return entryNode.level;\n    }\n\n    @Override\n    public int maxDegree() {\n        return layerInfo.stream().mapToInt(li -> li.degree).max().orElseThrow();\n    }\n\n    // re-declared to specify type\n    @Override\n    public View getView() {\n        try {\n            return new View(readerSupplier.get());\n        } catch (IOException e) {\n            throw new UncheckedIOException(e);\n        }\n    }\n\n    public class View implements FeatureSource, ScoringView, RandomAccessVectorValues {\n        protected final RandomAccessReader reader;\n        private final int[] neighbors;\n\n        public View(RandomAccessReader reader) {\n            this.reader = reader;\n            this.neighbors = new int[layerInfo.stream().mapToInt(li -> li.degree).max().orElse(0)];\n        }\n\n        @Override\n        public int dimension() {\n            return dimension;\n        }\n\n        // getVector isn't called on the hot path, only getVectorInto, so we don't bother using a shared value\n        @Override\n        public boolean isValueShared() {\n            return false;\n        }\n\n        @Override\n        public RandomAccessVectorValues copy() {\n            throw new UnsupportedOperationException(); // need to copy reader\n        }\n\n        private long offsetFor(int node, FeatureId featureId) {\n            Feature feature = features.get(featureId);\n\n            // Separated features are just global offset + node offset\n            if (feature instanceof SeparatedFeature) {\n                SeparatedFeature sf = (SeparatedFeature) feature;\n                return sf.getOffset() + (node * (long) feature.featureSize());\n            }\n\n            // Inline features are in layer 0 only\n            // skip node ID and get to the desired inline feature\n            long skipInNode = Integer.BYTES + inlineOffsets.get(featureId);\n            return baseNodeOffsetFor(node) + skipInNode;\n        }\n\n        private long neighborsOffsetFor(int level, int node) {\n            assert level == 0; // higher layers are in memory\n\n            // skip node ID + inline features\n            long skipInline = Integer.BYTES + inlineBlockSize;\n            return baseNodeOffsetFor(node) + skipInline;\n        }\n\n        private long baseNodeOffsetFor(int node) {\n            int degree = layerInfo.get(0).degree;\n\n            // skip node ID + inline features\n            long skipInline = Integer.BYTES + inlineBlockSize;\n            long blockBytes = skipInline + (long) Integer.BYTES * (degree + 1);\n\n            long offsetWithinLayer = blockBytes * node;\n            return neighborsOffset + offsetWithinLayer;\n        }\n\n\n        @Override\n        public RandomAccessReader featureReaderForNode(int node, FeatureId featureId) throws IOException {\n            long offset = offsetFor(node, featureId);\n            reader.seek(offset);\n            return reader;\n        }\n\n        @Override\n        public VectorFloat<?> getVector(int node) {\n            VectorFloat<?> vec = vectorTypeSupport.createFloatVector(dimension);\n            getVectorInto(node, vec, 0);\n            return vec;\n        }\n\n        @Override\n        public void getVectorInto(int node, VectorFloat<?> vector, int offset) {\n            var feature = features.get(FeatureId.INLINE_VECTORS);\n            if (feature == null) {\n                feature = features.get(FeatureId.SEPARATED_VECTORS);\n            }\n            if (feature == null) {\n                throw new UnsupportedOperationException(\"No full-resolution vectors in this graph\");\n            }\n\n            try {\n                long diskOffset = offsetFor(node, feature.id());\n                reader.seek(diskOffset);\n                vectorTypeSupport.readFloatVector(reader, dimension, vector, offset);\n            } catch (IOException e) {\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        public NodesIterator getNeighborsIterator(int level, int node) {\n            try {\n                if (level == 0) {\n                    // For layer 0, read from disk\n                    reader.seek(neighborsOffsetFor(level, node));\n                    int neighborCount = reader.readInt();\n                    assert neighborCount <= neighbors.length\n                            : String.format(\"Node %d neighborCount %d > M %d\", node, neighborCount, neighbors.length);\n                    reader.read(neighbors, 0, neighborCount);\n                    return new NodesIterator.ArrayNodesIterator(neighbors, neighborCount);\n                } else {\n                    // For levels > 0, read from memory\n                    var imn = getInMemoryLayers(reader);\n                    int[] stored = imn.get(level).get(node);\n                    assert stored != null : String.format(\"No neighbors found for node %d at level %d\", node, level);\n                    return new NodesIterator.ArrayNodesIterator(stored, stored.length);\n                }\n            } catch (IOException e) {\n                throw new UncheckedIOException(e);\n            }\n        }\n\n        @Override\n        public int size() {\n            // For vector operations we only care about layer 0\n            return OnDiskGraphIndex.this.size(0);\n        }\n\n        @Override\n        public NodeAtLevel entryNode() {\n            return entryNode;\n        }\n\n        @Override\n        public int getIdUpperBound() {\n            return idUpperBound;\n        }\n\n        @Override\n        public Bits liveNodes() {\n            return Bits.ALL;\n        }\n\n        @Override\n        public void close() throws IOException {\n            reader.close();\n        }\n\n        @Override\n        public ScoreFunction.ExactScoreFunction rerankerFor(VectorFloat<?> queryVector, VectorSimilarityFunction vsf) {\n            if (features.containsKey(FeatureId.INLINE_VECTORS)) {\n                return RandomAccessVectorValues.super.rerankerFor(queryVector, vsf);\n            } else if (features.containsKey(FeatureId.NVQ_VECTORS)) {\n                return ((NVQ) features.get(FeatureId.NVQ_VECTORS)).rerankerFor(queryVector, vsf, this);\n            } else {\n                throw new UnsupportedOperationException(\"No reranker available for this graph\");\n            }\n        }\n\n        @Override\n        public ScoreFunction.ApproximateScoreFunction approximateScoreFunctionFor(VectorFloat<?> queryVector, VectorSimilarityFunction vsf) {\n            if (features.containsKey(FeatureId.FUSED_ADC)) {\n                return ((FusedADC) features.get(FeatureId.FUSED_ADC)).approximateScoreFunctionFor(queryVector, vsf, this, rerankerFor(queryVector, vsf));\n            } else {\n                throw new UnsupportedOperationException(\"No approximate score function available for this graph\");\n            }\n        }\n    }\n\n    /** Convenience function for writing a vanilla DiskANN-style index with no extra Features. */\n    public static void write(GraphIndex graph, RandomAccessVectorValues vectors, Path path) throws IOException {\n        write(graph, vectors, OnDiskGraphIndexWriter.sequentialRenumbering(graph), path);\n    }",
    "func": "    public static void write(GraphIndex graph,\n                             RandomAccessVectorValues vectors,\n                             Map<Integer, Integer> oldToNewOrdinals,\n                             Path path)\n            throws IOException\n    {\n        try (var writer = new OnDiskGraphIndexWriter.Builder(graph, path).withMap(oldToNewOrdinals)\n                .with(new InlineVectors(vectors.dimension()))\n                .build())\n        {\n            var suppliers = Feature.singleStateFactory(FeatureId.INLINE_VECTORS,\n                    nodeId -> new InlineVectors.State(vectors.getVector(nodeId)));\n            writer.write(suppliers);\n        }\n    }",
    "comment": "    /** Convenience function for writing a vanilla DiskANN-style index with no extra Features. */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testSimpleGraphs jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testReorderingWithHoles jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testIncrementalWrites jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testV0WriteIncremental jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::tearDown jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testV0Read jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testV0Write jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::validateVectors jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testRenumberingOnDelete jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testReorderingRenumbering jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testLargeGraph",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::TestOnDiskGraphIndex",
    "func_start": 497,
    "func_end": 511,
    "body_len": 14,
    "instruction": "Write a static method that writes a basic DiskANN-style graph index to disk without any additional features, using the provided graph structure, vector values, a mapping from old to new node ordinals, and an output path. The method should utilize an OnDiskGraphIndexWriter configured with the ordinal mapping and inline vector storage, and write the index by supplying vector data for each node via a feature supplier.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/disk/OnDiskGraphIndex.java::write",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testSimpleGraphs",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testReorderingWithHoles",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testIncrementalWrites",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testV0WriteIncremental",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::tearDown",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testV0Read",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testV0Write",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::validateVectors",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testRenumberingOnDelete",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testReorderingRenumbering",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java::testLargeGraph"
    ],
    "test_start": 76,
    "test_end": 92,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/disk/TestOnDiskGraphIndex.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.disk.TestOnDiskGraphIndex#testSimpleGraphs\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "    @Test\n    public void testSimpleGraphs() throws Exception {\n        for (var graph : List.of(fullyConnectedGraph, randomlyConnectedGraph))\n        {\n            var outputPath = testDirectory.resolve(\"test_graph_\" + graph.getClass().getSimpleName());\n            var ravv = new TestVectorGraph.CircularFloatVectorValues(graph.size());\n            TestUtil.writeGraph(graph, ravv, outputPath);\n            try (var readerSupplier = new SimpleMappedReader.Supplier(outputPath.toAbsolutePath());\n                 var onDiskGraph = OnDiskGraphIndex.load(readerSupplier))\n            {\n                TestUtil.assertGraphEquals(graph, onDiskGraph);\n                try (var onDiskView = onDiskGraph.getView()) {\n                    validateVectors(onDiskView, ravv);\n                }\n            }\n        }\n    }"
  },
  {
    "task-id": "jvector-jvector-base/src/main/java/io/github/jbellis/jvector/graph/similarity/BuildScoreProvider.java-randomAccessScoreProvider",
    "project": "jvector",
    "file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/similarity/BuildScoreProvider.java",
    "func_name": "randomAccessScoreProvider",
    "context": "/*\n * Copyright DataStax, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage io.github.jbellis.jvector.graph.similarity;\n\nimport io.github.jbellis.jvector.graph.RandomAccessVectorValues;\nimport io.github.jbellis.jvector.quantization.BQVectors;\nimport io.github.jbellis.jvector.quantization.PQVectors;\nimport io.github.jbellis.jvector.vector.VectorSimilarityFunction;\nimport io.github.jbellis.jvector.vector.VectorUtil;\nimport io.github.jbellis.jvector.vector.VectorizationProvider;\nimport io.github.jbellis.jvector.vector.types.VectorFloat;\nimport io.github.jbellis.jvector.vector.types.VectorTypeSupport;\n\n/**\n * Encapsulates comparing node distances for GraphIndexBuilder.\n */\npublic interface BuildScoreProvider {\n    VectorTypeSupport vts = VectorizationProvider.getInstance().getVectorTypeSupport();\n\n    /**\n     * @return true if the primary score functions used for construction are exact.  This\n     * is modestly redundant, but it saves having to allocate new Search/Diversity provider\n     * objects in some hot construction loops.\n     */\n    boolean isExact();\n\n    /**\n     * @return the approximate centroid of the known nodes.  We use the closest node\n     * to this centroid as the graph entry point, so this is called when the entry point is deleted\n     * or every time the graph size doubles.\n     * <p>\n     * This is not called on a path that blocks searches or modifications, so it is okay for it to be O(N).\n     */\n    VectorFloat<?> approximateCentroid();\n\n    /**\n     * Create a search score provider to use *internally* during construction.\n     * <p>\n     * \"Internally\" means that this may differ from a typical SSP in that it may use\n     * approximate scores *without* reranking.  (In this case, reranking will be done\n     * separately by the ConcurrentNeighborSet diversity code.)\n     * <p>\n     * @param vector the query vector to provide similarity scores against\n     */\n    SearchScoreProvider searchProviderFor(VectorFloat<?> vector);\n\n    /**\n     * Create a search score provider to use *internally* during construction.\n     * <p>\n     * \"Internally\" means that this may differ from a typical SSP in that it may use\n     * approximate scores *without* reranking.  (In this case, reranking will be done\n     * separately by the ConcurrentNeighborSet diversity code.)\n     * <p>\n     * @param node1 the graph node to provide similarity scores against\n     */\n    SearchScoreProvider searchProviderFor(int node1);\n\n    /**\n     * Create a score provider to use internally during construction.\n     * <p>\n     * The difference between the diversity provider and the search provider is\n     * that the diversity provider is only expected to be used a few dozen times per node,\n     * which influences the implementation choices.\n     * <p>\n     * When scoring is approximate, the scores from the search and diversity provider\n     * must be consistent, i.e. mixing different types of CompressedVectors will cause problems.\n     */\n    SearchScoreProvider diversityProviderFor(int node1);",
    "func": "    static BuildScoreProvider randomAccessScoreProvider(RandomAccessVectorValues ravv, VectorSimilarityFunction similarityFunction) {\n        // We need two sources of vectors in order to perform diversity check comparisons without\n        // colliding.  ThreadLocalSupplier makes this a no-op if the RAVV is actually un-shared.\n        var vectors = ravv.threadLocalSupplier();\n        var vectorsCopy = ravv.threadLocalSupplier();\n\n        return new BuildScoreProvider() {\n            @Override\n            public boolean isExact() {\n                return true;\n            }\n\n            @Override\n            public VectorFloat<?> approximateCentroid() {\n                var vv = vectors.get();\n                var centroid = vts.createFloatVector(vv.dimension());\n                for (int i = 0; i < vv.size(); i++) {\n                    var v = vv.getVector(i);\n                    if (v != null) { // MapRandomAccessVectorValues is not necessarily dense\n                        VectorUtil.addInPlace(centroid, v);\n                    }\n                }\n                VectorUtil.scale(centroid, 1.0f / vv.size());\n                return centroid;\n            }\n\n            @Override\n            public SearchScoreProvider searchProviderFor(VectorFloat<?> vector) {\n                var vc = vectorsCopy.get();\n                return DefaultSearchScoreProvider.exact(vector, similarityFunction, vc);\n            }\n\n            @Override\n            public SearchScoreProvider searchProviderFor(int node1) {\n                RandomAccessVectorValues randomAccessVectorValues = vectors.get();\n                var v = randomAccessVectorValues.getVector(node1);\n                return searchProviderFor(v);\n            }\n\n            @Override\n            public SearchScoreProvider diversityProviderFor(int node1) {\n                RandomAccessVectorValues randomAccessVectorValues = vectors.get();\n                var v = randomAccessVectorValues.getVector(node1);\n                var vc = vectorsCopy.get();\n                return DefaultSearchScoreProvider.exact(v, similarityFunction, vc);\n            }\n        };\n    }",
    "comment": "    /**\n     * Returns a BSP that performs exact score comparisons using the given RandomAccessVectorValues and VectorSimilarityFunction.\n     */",
    "test_funcs": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseConcurrent jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseRetainsNatural jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverse",
    "test_class": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::TestNeighbors",
    "func_start": 87,
    "func_end": 134,
    "body_len": 47,
    "instruction": "Write a static method `randomAccessScoreProvider` that takes a `RandomAccessVectorValues` and a `VectorSimilarityFunction` as inputs and returns a `BuildScoreProvider` implementation. The returned provider should support exact score computations by using thread-local instances of the vector values to avoid interference during concurrent access. It should provide an approximate centroid calculation by averaging all non-null vectors from the input `RandomAccessVectorValues`. Additionally, it should offer search score providers for a given vector, a node index, and a diversity check, all using exact similarity comparisons via the provided similarity function. Ensure that separate thread-local copies of the vector values are used for the main and comparison operations to prevent conflicts, especially during parallel execution, while optimizing for cases where the vector values are not shared across threads.",
    "func_name_with_file_path": "jvector-base/src/main/java/io/github/jbellis/jvector/graph/similarity/BuildScoreProvider.java::randomAccessScoreProvider",
    "test_funcs_split": [
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseConcurrent",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverseRetainsNatural",
      "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java::testInsertDiverse"
    ],
    "test_start": 59,
    "test_end": 82,
    "test_file": "jvector-tests/src/test/java/io/github/jbellis/jvector/graph/TestNeighbors.java",
    "test_instruction": "mvn test -Dtest=\"io.github.jbellis.jvector.graph.TestNeighbors#testInsertDiverseConcurrent\" -Dsurefire.failIfNoSpecifiedTests=false",
    "test_code": "  @Test\n  public void testInsertDiverseConcurrent() {\n    // set up BSP\n    var sf = VectorSimilarityFunction.DOT_PRODUCT;\n    var vectors = new TestVectorGraph.CircularFloatVectorValues(10);\n    var natural = new NodeArray(10);\n    var concurrent = new NodeArray(10);\n    var bsp = BuildScoreProvider.randomAccessScoreProvider(vectors, sf);\n    // \"natural\" candidates are [0..7), \"concurrent\" are [8..10)\n    IntStream.range(0, 7)\n        .forEach(i -> natural.insertSorted(i, scoreBetween(bsp, 7, i)));\n    IntStream.range(8, 10)\n        .forEach(\n            i -> concurrent.insertSorted(i, scoreBetween(bsp, 7, i)));\n\n    // only nodes 6 and 8 are diverse wrt 7\n    var cnm = new ConcurrentNeighborMap(new VamanaDiversityProvider(bsp, 1.0f), 10, 10);\n    cnm.addNode(7);\n    var neighbors = cnm.insertDiverse(7, NodeArray.merge(natural, concurrent));\n    assertEquals(2, neighbors.size());\n    assert neighbors.contains(8);\n    assert neighbors.contains(6);\n    validateSortedByScore(neighbors);\n  }"
  },
  {
    "task-id": "timefold-solver-benchmark/src/main/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtils.java-determineStandardDeviationDoubles",
    "project": "timefold-solver",
    "file_path": "benchmark/src/main/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtils.java",
    "func_name": "determineStandardDeviationDoubles",
    "context": "package ai.timefold.solver.benchmark.impl.statistic;\n\nimport java.text.DecimalFormat;\nimport java.text.DecimalFormatSymbols;\nimport java.util.List;\nimport java.util.Locale;\n\nimport ai.timefold.solver.benchmark.impl.result.BenchmarkResult;\nimport ai.timefold.solver.core.api.score.Score;\n\npublic class StatisticUtils {\n\n    private StatisticUtils() {\n        // This class is not instantiable\n    }",
    "func": "    public static double[] determineStandardDeviationDoubles(\n            List<? extends BenchmarkResult> benchmarkResultList, Score averageScore, int successCount) {\n        if (successCount <= 0) {\n            return new double[0];\n        }\n        if (averageScore == null) {\n            throw new IllegalArgumentException(\"Average score (\" + averageScore + \") cannot be null.\");\n        }\n        // averageScore can no longer be null\n        double[] differenceSquaredTotalDoubles = null;\n        for (BenchmarkResult benchmarkResult : benchmarkResultList) {\n            if (benchmarkResult.hasAllSuccess()) {\n                Score difference = benchmarkResult.getAverageScore().subtract(averageScore);\n                // Calculations done on doubles to avoid common overflow when executing with an int score > 500 000\n                double[] differenceDoubles = difference.toLevelDoubles();\n                if (differenceSquaredTotalDoubles == null) {\n                    differenceSquaredTotalDoubles = new double[differenceDoubles.length];\n                }\n                for (int i = 0; i < differenceDoubles.length; i++) {\n                    differenceSquaredTotalDoubles[i] += Math.pow(differenceDoubles[i], 2.0);\n                }\n            }\n        }\n\n        if (differenceSquaredTotalDoubles == null) { // no successful benchmarks\n            return new double[0];\n        }\n\n        double[] standardDeviationDoubles = new double[differenceSquaredTotalDoubles.length];\n        for (int i = 0; i < differenceSquaredTotalDoubles.length; i++) {\n            standardDeviationDoubles[i] = Math.pow(differenceSquaredTotalDoubles[i] / successCount, 0.5);\n        }\n        return standardDeviationDoubles;\n    }",
    "comment": "    /**\n     * Calculates standard deviation of {@link BenchmarkResult#getAverageScore()}s from {@code averageScore}.\n     *\n     * @param averageScore not null\n     * @return standard deviation double values\n     */",
    "test_funcs": "benchmark/src/test/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtilsTest.java::singleDetermineStandardDeviationDoubles benchmark/src/test/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtilsTest.java::multipleDetermineStandardDeviationDoubles benchmark/src/test/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtilsTest.java::largeDetermineStandardDeviationDoubles",
    "test_class": "benchmark/src/test/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtilsTest.java::StatisticUtilsTest",
    "func_start": 23,
    "func_end": 56,
    "body_len": 33,
    "instruction": "Write a function `determineStandardDeviationDoubles` that takes a list of `BenchmarkResult` objects, an `averageScore` of type `Score`, and an integer `successCount`. The function should compute the standard deviation of the average scores from the successful benchmark results (where `hasAllSuccess()` is true) relative to the given `averageScore`. The standard deviation should be calculated separately for each level in the score (using `toLevelDoubles()`), using double-precision arithmetic to prevent overflow. If `successCount` is zero or negative, or if there are no successful benchmarks, return an empty double array. If `averageScore` is null, throw an `IllegalArgumentException`. Otherwise, return an array of doubles representing the standard deviation per score level.",
    "func_name_with_file_path": "benchmark/src/main/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtils.java::determineStandardDeviationDoubles",
    "test_funcs_split": [
      "benchmark/src/test/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtilsTest.java::singleDetermineStandardDeviationDoubles",
      "benchmark/src/test/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtilsTest.java::multipleDetermineStandardDeviationDoubles",
      "benchmark/src/test/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtilsTest.java::largeDetermineStandardDeviationDoubles"
    ],
    "test_start": 23,
    "test_end": 30,
    "test_file": "benchmark/src/test/java/ai/timefold/solver/benchmark/impl/statistic/StatisticUtilsTest.java",
    "test_instruction": "mvn test -pl benchmark -Dtest=\"ai.timefold.solver.benchmark.impl.statistic.StatisticUtilsTest#singleDetermineStandardDeviationDoubles\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void singleDetermineStandardDeviationDoubles() {\n        List<SubSingleBenchmarkResult> subSingleBenchmarkResultList = Arrays\n                .asList(createSubSingleBenchmarkResult(SimpleScore.of(0), 0));\n        assertThat(StatisticUtils.determineStandardDeviationDoubles(subSingleBenchmarkResultList,\n                SimpleScore.of(0), subSingleBenchmarkResultList.size()))\n                .containsSequence(new double[] { 0d }, offset(DELTA));\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementor.java-createAccessorFor",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementor.java",
    "func_name": "createAccessorFor",
    "context": "package ai.timefold.solver.core.impl.domain.common.accessor.gizmo;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.AnnotatedElement;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Modifier;\nimport java.lang.reflect.Type;\nimport java.util.Arrays;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\nimport ai.timefold.solver.core.impl.domain.common.accessor.MemberAccessor;\nimport ai.timefold.solver.core.impl.util.MutableReference;\n\nimport io.quarkus.gizmo.ClassCreator;\nimport io.quarkus.gizmo.ClassOutput;\nimport io.quarkus.gizmo.FieldDescriptor;\nimport io.quarkus.gizmo.MethodCreator;\nimport io.quarkus.gizmo.MethodDescriptor;\nimport io.quarkus.gizmo.ResultHandle;\n\n/**\n * Generates the bytecode for the MemberAccessor of a particular Member\n */\npublic final class GizmoMemberAccessorImplementor {\n\n    final static String GENERIC_TYPE_FIELD = \"genericType\";\n    final static String ANNOTATED_ELEMENT_FIELD = \"annotatedElement\";\n\n    /**\n     * Generates the constructor and implementations of {@link AbstractGizmoMemberAccessor} methods for the given\n     * {@link Member}.\n     *\n     * @param className never null\n     * @param classOutput never null, defines how to write the bytecode\n     * @param memberInfo never null, member to generate MemberAccessor methods implementation for\n     */\n    public static void defineAccessorFor(String className, ClassOutput classOutput, GizmoMemberInfo memberInfo) {\n        Class<? extends AbstractGizmoMemberAccessor> superClass = getCorrectSuperclass(memberInfo);\n        try (ClassCreator classCreator = ClassCreator.builder()\n                .className(className)\n                .superClass(superClass)\n                .classOutput(classOutput)\n                .setFinal(true)\n                .build()) {\n            classCreator.getFieldCreator(\"genericType\", Type.class)\n                    .setModifiers(Modifier.FINAL);\n            classCreator.getFieldCreator(\"annotatedElement\", AnnotatedElement.class)\n                    .setModifiers(Modifier.FINAL);\n\n            // ************************************************************************\n            // MemberAccessor methods\n            // ************************************************************************\n            createConstructor(classCreator, memberInfo);\n            createGetDeclaringClass(classCreator, memberInfo);\n            createGetType(classCreator, memberInfo);\n            createGetGenericType(classCreator);\n            createGetName(classCreator, memberInfo);\n            createExecuteGetter(classCreator, memberInfo);\n            if (superClass == AbstractReadWriteGizmoMemberAccessor.class) {\n                createExecuteSetter(classCreator, memberInfo);\n            }\n            createGetAnnotation(classCreator);\n            createDeclaredAnnotationsByType(classCreator);\n        }\n    }\n\n    private static Class<? extends AbstractGizmoMemberAccessor> getCorrectSuperclass(GizmoMemberInfo memberInfo) {\n        AtomicBoolean supportsSetter = new AtomicBoolean();\n        memberInfo.descriptor().whenIsMethod(method -> {\n            supportsSetter.set(memberInfo.descriptor().getSetter().isPresent());\n        });\n        memberInfo.descriptor().whenIsField(field -> {\n            supportsSetter.set(true);\n        });\n        if (supportsSetter.get()) {\n            return AbstractReadWriteGizmoMemberAccessor.class;\n        } else {\n            return AbstractReadOnlyGizmoMemberAccessor.class;\n        }\n    }",
    "func": "    static MemberAccessor createAccessorFor(Member member, Class<? extends Annotation> annotationClass,\n            boolean returnTypeRequired, GizmoClassLoader gizmoClassLoader) {\n        String className = GizmoMemberAccessorFactory.getGeneratedClassName(member);\n        if (gizmoClassLoader.hasBytecodeFor(className)) {\n            return createInstance(className, gizmoClassLoader);\n        }\n        final MutableReference<byte[]> classBytecodeHolder = new MutableReference<>(null);\n        ClassOutput classOutput = (path, byteCode) -> classBytecodeHolder.setValue(byteCode);\n        GizmoMemberInfo memberInfo =\n                new GizmoMemberInfo(new GizmoMemberDescriptor(member), returnTypeRequired, annotationClass);\n        defineAccessorFor(className, classOutput, memberInfo);\n        byte[] classBytecode = classBytecodeHolder.getValue();\n\n        gizmoClassLoader.storeBytecode(className, classBytecode);\n        return createInstance(className, gizmoClassLoader);\n    }",
    "comment": "    /**\n     * Creates a MemberAccessor for a given member, generating\n     * the MemberAccessor bytecode if required\n     *\n     * @param member The member to generate a MemberAccessor for\n     * @param annotationClass The annotation it was annotated with (used for\n     *        error reporting)\n     * @param returnTypeRequired A flag that indicates if the return type is required or optional\n     * @param gizmoClassLoader never null\n     * @return A new MemberAccessor that uses Gizmo generated bytecode.\n     *         Will generate the bytecode the first type it is called\n     *         for a member, unless a classloader has been set,\n     *         in which case no Gizmo code will be generated.\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testThrowsWhenGetterMethodHasParameters core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForBooleanMethod core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testThrowsWhenReadMethodReturnVoid core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testThrowsWhenGetBooleanReturnsNonBoolean core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForField core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForPrimitiveField core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorSameClass core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForMethod core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testThrowsWhenGetterMethodReturnVoid core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForMethodWithoutSetter",
    "test_class": "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::GizmoMemberAccessorImplementorTest",
    "func_start": 99,
    "func_end": 114,
    "body_len": 15,
    "instruction": "Write a function `createAccessorFor` that takes a Member, an annotation class for error reporting, a boolean indicating whether the return type is required, and a non-null GizmoClassLoader. The function should return a MemberAccessor instance that uses Gizmo-generated bytecode. If the GizmoClassLoader already contains bytecode for the generated class name (derived from the member), reuse it by creating an instance directly. Otherwise, generate the bytecode for the accessor using a ClassOutput that captures the bytecode into a mutable reference, define the accessor class with the appropriate member information including descriptor, return type requirement, and annotation type, store the generated bytecode in the classloader, and then create and return a new MemberAccessor instance from the generated class.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementor.java::createAccessorFor",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testThrowsWhenGetterMethodHasParameters",
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForBooleanMethod",
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testThrowsWhenReadMethodReturnVoid",
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testThrowsWhenGetBooleanReturnsNonBoolean",
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForField",
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForPrimitiveField",
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorSameClass",
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForMethod",
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testThrowsWhenGetterMethodReturnVoid",
      "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java::testGeneratedMemberAccessorForMethodWithoutSetter"
    ],
    "test_start": 134,
    "test_end": 141,
    "test_file": "core/src/test/java/ai/timefold/solver/core/impl/domain/common/accessor/gizmo/GizmoMemberAccessorImplementorTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.domain.common.accessor.gizmo.GizmoMemberAccessorImplementorTest#testThrowsWhenGetterMethodHasParameters\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void testThrowsWhenGetterMethodHasParameters() throws NoSuchMethodException {\n        Member member = GizmoTestdataEntity.class.getMethod(\"methodWithParameters\", String.class);\n        assertThatCode(() -> {\n            GizmoMemberAccessorImplementor.createAccessorFor(member, PlanningVariable.class, true, new GizmoClassLoader());\n        }).hasMessage(\"The getterMethod (methodWithParameters) with a PlanningVariable annotation \" +\n                \"must not have any parameters, but has parameters ([Ljava/lang/String;]).\");\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/impl/solver/BestSolutionHolder.java-set",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/BestSolutionHolder.java",
    "func_name": "set",
    "context": "package ai.timefold.solver.core.impl.solver;\n\nimport java.math.BigInteger;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.SortedMap;\nimport java.util.TreeMap;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.BooleanSupplier;\nimport java.util.function.UnaryOperator;\n\nimport ai.timefold.solver.core.api.solver.Solver;\nimport ai.timefold.solver.core.api.solver.change.ProblemChange;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.NullMarked;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * The goal of this class is to register problem changes and best solutions in a thread-safe way.\n * Problem changes are {@link #addProblemChange(Solver, List) put in a queue}\n * and later associated with the best solution which contains them.\n * The best solution is associated with a version number\n * that is incremented each time a {@link #set new best solution is set}.\n * The best solution is {@link #take() taken} together with all problem changes\n * that were registered before the best solution was set.\n * \n * <p>\n * This class needs to be thread-safe.\n * \n * @param <Solution_>\n */\n@NullMarked\nfinal class BestSolutionHolder<Solution_> {\n\n    private final AtomicReference<BigInteger> lastProcessedVersion = new AtomicReference<>(BigInteger.valueOf(-1));\n\n    // These references are non-final and being accessed from multiple threads, \n    // therefore they need to be volatile and all access synchronized.\n    // Both the map and the best solution are based on the current version,\n    // and therefore access to both needs to be guarded by the same lock.\n    // The version is BigInteger to avoid long overflow.\n    // The solver can run potentially forever, so long overflow is a (remote) possibility.\n    private volatile SortedMap<BigInteger, List<CompletableFuture<Void>>> problemChangesPerVersionMap =\n            createNewProblemChangesMap();\n    private volatile @Nullable VersionedBestSolution<Solution_> versionedBestSolution = null;\n    private volatile BigInteger currentVersion = BigInteger.ZERO;\n\n    private static SortedMap<BigInteger, List<CompletableFuture<Void>>> createNewProblemChangesMap() {\n        return createNewProblemChangesMap(Collections.emptySortedMap());\n    }\n\n    private static SortedMap<BigInteger, List<CompletableFuture<Void>>>\n            createNewProblemChangesMap(SortedMap<BigInteger, List<CompletableFuture<Void>>> map) {\n        return new TreeMap<>(map);\n    }\n\n    synchronized boolean isEmpty() {\n        return this.versionedBestSolution == null;\n    }\n\n    /**\n     * @return the last best solution together with problem changes the solution contains.\n     *         If there is no new best solution, returns null.\n     */\n    @Nullable\n    BestSolutionContainingProblemChanges<Solution_> take() {\n        var latestVersionedBestSolution = resetVersionedBestSolution();\n        if (latestVersionedBestSolution == null) {\n            return null;\n        }\n\n        var bestSolutionVersion = latestVersionedBestSolution.version();\n        var latestProcessedVersion = this.lastProcessedVersion.getAndUpdate(bestSolutionVersion::max);\n        if (latestProcessedVersion.compareTo(bestSolutionVersion) > 0) {\n            // Corner case: The best solution has already been taken,\n            // because a later take() was scheduled to run before an earlier take().\n            // This causes the later take() to return the latest best solution and all the problem changes,\n            // and the earlier best solution to be skipped entirely.\n            return null;\n        }\n        // The map is replaced by a map containing only the problem changes that are not contained in the best solution.\n        // This is fully synchronized, so no other thread can access the old map anymore.\n        // The old map can then be processed by the current thread without synchronization.\n        // The copying of maps is possibly expensive, but due to the nature of problem changes,\n        // we do not expect the map to ever get too big.\n        // It is not practical to submit a problem change every second, as that gives the solver no time to react.\n        // This limits the size of the map on input.\n        // The solver also finds new best solutions, which regularly trims the size of the map as well.\n        var boundaryVersion = bestSolutionVersion.add(BigInteger.ONE);\n        var oldProblemChangesPerVersion =\n                replaceMapSynchronized(map -> createNewProblemChangesMap(map.tailMap(boundaryVersion)));\n        // At this point, the old map is not accessible to any other thread.\n        // We also do not need to clear it, because this being the only reference, \n        // garbage collector will do it for us.\n        var containedProblemChanges = oldProblemChangesPerVersion.headMap(boundaryVersion)\n                .values()\n                .stream()\n                .flatMap(Collection::stream)\n                .toList();\n        return new BestSolutionContainingProblemChanges<>(latestVersionedBestSolution.bestSolution(), containedProblemChanges);\n    }\n\n    private synchronized @Nullable VersionedBestSolution<Solution_> resetVersionedBestSolution() {\n        var oldVersionedBestSolution = this.versionedBestSolution;\n        this.versionedBestSolution = null;\n        return oldVersionedBestSolution;\n    }\n\n    private synchronized SortedMap<BigInteger, List<CompletableFuture<Void>>> replaceMapSynchronized(\n            UnaryOperator<SortedMap<BigInteger, List<CompletableFuture<Void>>>> replaceFunction) {\n        var oldMap = problemChangesPerVersionMap;\n        problemChangesPerVersionMap = replaceFunction.apply(oldMap);\n        return oldMap;\n    }",
    "func": "    void set(Solution_ bestSolution, BooleanSupplier isEveryProblemChangeProcessed) {\n        // The new best solution can be accepted only if there are no pending problem changes\n        // nor any additional changes may come during this operation.\n        // Otherwise, a race condition might occur\n        // that leads to associating problem changes with a solution that was created later,\n        // but does not contain them yet.\n        // As a result, CompletableFutures representing these changes would be completed too early.\n        if (isEveryProblemChangeProcessed.getAsBoolean()) {\n            synchronized (this) {\n                versionedBestSolution = new VersionedBestSolution<>(bestSolution, currentVersion);\n                currentVersion = currentVersion.add(BigInteger.ONE);\n            }\n        }\n    }",
    "comment": "    /**\n     * Sets the new best solution if all known problem changes have been processed\n     * and thus are contained in this best solution.\n     *\n     * @param bestSolution the new best solution that replaces the previous one if there is any\n     * @param isEveryProblemChangeProcessed a supplier that tells if all problem changes have been processed\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/impl/solver/BestSolutionHolderTest.java::setBestSolution core/src/test/java/ai/timefold/solver/core/impl/solver/DefaultSolverTest.java::checkDefaultMeters core/src/test/java/ai/timefold/solver/core/impl/solver/DefaultSolverTest.java::checkDefaultMetersTags core/src/test/java/ai/timefold/solver/core/impl/solver/ConsumerSupportTest.java::problemChangesComplete_afterFinalBestSolutionIsConsumed core/src/test/java/ai/timefold/solver/core/impl/solver/DefaultSolverTest.java::solveWithProblemChange core/src/test/java/ai/timefold/solver/core/impl/solver/BestSolutionHolderTest.java::cancelPendingChanges_noChangesRetrieved core/src/test/java/ai/timefold/solver/core/impl/solver/ConsumerSupportTest.java::skipAhead core/src/test/java/ai/timefold/solver/core/impl/solver/BestSolutionHolderTest.java::completeProblemChanges core/src/test/java/ai/timefold/solver/core/impl/solver/DefaultSolverTest.java::solveMetrics",
    "test_class": "core/src/test/java/ai/timefold/solver/core/impl/solver/DefaultSolverTest.java::DefaultSolverTest core/src/test/java/ai/timefold/solver/core/impl/solver/BestSolutionHolderTest.java::BestSolutionHolderTest core/src/test/java/ai/timefold/solver/core/impl/solver/ConsumerSupportTest.java::ConsumerSupportTest",
    "func_start": 127,
    "func_end": 140,
    "body_len": 13,
    "instruction": "Write a method `set` that takes a new best solution of type `Solution_` and a `BooleanSupplier` indicating whether all problem changes have been processed. The method should only update the current best solution if all changes are confirmed as processed. To prevent race conditions, the update must be synchronized: when the condition is met, store the new solution along with the current version number in a `versionedBestSolution` field and then increment the `currentVersion` by one using a `BigInteger`. If the condition is not met, the method should do nothing.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/BestSolutionHolder.java::set",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/impl/solver/BestSolutionHolderTest.java::setBestSolution",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/DefaultSolverTest.java::checkDefaultMeters",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/DefaultSolverTest.java::checkDefaultMetersTags",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/ConsumerSupportTest.java::problemChangesComplete_afterFinalBestSolutionIsConsumed",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/DefaultSolverTest.java::solveWithProblemChange",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/BestSolutionHolderTest.java::cancelPendingChanges_noChangesRetrieved",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/ConsumerSupportTest.java::skipAhead",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/BestSolutionHolderTest.java::completeProblemChanges",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/DefaultSolverTest.java::solveMetrics"
    ],
    "test_start": 35,
    "test_end": 54,
    "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/BestSolutionHolderTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.BestSolutionHolderTest#setBestSolution\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void setBestSolution() {\n        BestSolutionHolder<TestdataSolution> bestSolutionHolder = new BestSolutionHolder<>();\n        assertThat(bestSolutionHolder.take()).isNull();\n\n        TestdataSolution solution1 = TestdataSolution.generateSolution();\n        TestdataSolution solution2 = TestdataSolution.generateSolution();\n\n        bestSolutionHolder.set(solution1, () -> true);\n        assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution1);\n        assertThat(bestSolutionHolder.take()).isNull();\n\n        bestSolutionHolder.set(solution1, () -> true);\n        bestSolutionHolder.set(solution2, () -> false);\n        assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution1);\n\n        bestSolutionHolder.set(solution1, () -> true);\n        bestSolutionHolder.set(solution2, () -> true);\n        assertThat(bestSolutionHolder.take().getBestSolution()).isSameAs(solution2);\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/impl/solver/random/RandomUtils.java-nextLong",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/random/RandomUtils.java",
    "func_name": "nextLong",
    "context": "package ai.timefold.solver.core.impl.solver.random;\n\nimport java.util.Random;\n\npublic class RandomUtils {",
    "func": "    public static long nextLong(Random random, long n) {\n        // This code is based on java.util.Random#nextInt(int)'s javadoc.\n        if (n <= 0L) {\n            throw new IllegalArgumentException(\"n must be positive\");\n        }\n        if (n < Integer.MAX_VALUE) {\n            return random.nextInt((int) n);\n        }\n\n        long bits;\n        long val;\n        do {\n            bits = (random.nextLong() << 1) >>> 1;\n            val = bits % n;\n        } while (bits - val + (n - 1L) < 0L);\n        return val;\n    }",
    "comment": "    /**\n     * Mimics {@link Random#nextInt(int)} for longs.\n     *\n     * @param random never null\n     * @param n {@code > 0L}\n     * @return like {@link Random#nextInt(int)} but for a long\n     * @see Random#nextInt(int)\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/impl/solver/random/RandomUtilsTest.java::testNextLong",
    "test_class": "core/src/test/java/ai/timefold/solver/core/impl/solver/random/RandomUtilsTest.java::RandomUtilsTest",
    "func_start": 15,
    "func_end": 31,
    "body_len": 16,
    "instruction": "Write a function `nextLong(Random random, long n)` that generates a pseudorandom long value between 0 (inclusive) and n (exclusive), mimicking the behavior of `Random#nextInt(int)` but for long values. The function should accept a non-null Random instance and a positive long n (> 0L). For values of n less than Integer.MAX_VALUE, it should delegate to `nextInt`. For larger values, it should use a rejection sampling technique to ensure uniform distribution, avoiding bias from modulo operations, similar to the algorithm described in the Javadoc of `Random#nextInt(int)`.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/random/RandomUtils.java::nextLong",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/impl/solver/random/RandomUtilsTest.java::testNextLong"
    ],
    "test_start": 9,
    "test_end": 13,
    "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/random/RandomUtilsTest.java",
    "first_runnable_test": "core/src/test/java/ai/timefold/solver/core/impl/solver/random/RandomUtilsTest.java::testNextLong",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.random.RandomUtilsTest#testNextLong\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void testNextLong() {\n        Random random = new Random(37);\n        RandomUtils.nextLong(random, 10L + Integer.MAX_VALUE);\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTermination.java-calculateSolverTimeGradient",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTermination.java",
    "func_name": "calculateSolverTimeGradient",
    "context": "package ai.timefold.solver.core.impl.solver.termination;\n\nimport java.util.List;\n\nimport ai.timefold.solver.core.impl.phase.scope.AbstractPhaseScope;\nimport ai.timefold.solver.core.impl.solver.scope.SolverScope;\nimport ai.timefold.solver.core.impl.solver.thread.ChildThreadType;\n\nimport org.jspecify.annotations.NullMarked;\n\n@NullMarked\nfinal class AndCompositeTermination<Solution_>\n        extends AbstractCompositeTermination<Solution_>\n        implements ChildThreadSupportingTermination<Solution_, SolverScope<Solution_>> {\n\n    public AndCompositeTermination(List<Termination<Solution_>> terminationList) {\n        super(terminationList);\n    }\n\n    @SafeVarargs\n    public AndCompositeTermination(Termination<Solution_>... terminations) {\n        super(terminations);\n    }\n\n    /**\n     * @return true if all terminations are terminated.\n     */\n    @Override\n    public boolean isSolverTerminated(SolverScope<Solution_> solverScope) {\n        for (var termination : solverTerminationList) {\n            if (!termination.isSolverTerminated(solverScope)) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * @return true if all supported terminations are terminated.\n     */\n    @Override\n    public boolean isPhaseTerminated(AbstractPhaseScope<Solution_> phaseScope) {\n        for (var termination : phaseTerminationList) {\n            if (termination.isApplicableTo(phaseScope.getClass()) && !termination.isPhaseTerminated(phaseScope)) {\n                return false;\n            }\n        }\n        return true;\n    }",
    "func": "    @Override\n    public double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n        var timeGradient = 1.0;\n        for (var termination : solverTerminationList) {\n            var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.min(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }",
    "comment": "    /**\n     * Calculates the minimum timeGradient of all Terminations.\n     * Not supported timeGradients (-1.0) are ignored.\n     *\n     * @return the minimum timeGradient of the Terminations.\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristicAndLocalSearch core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImprovesTooLate_terminates core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImproves_terminationIsPostponed core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::solverTerminationWithConstructionHeuristic core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::calculateSolverTimeGradientTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristic core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::calculateSolverTimeGradientTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::solverTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::solveTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::solveTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::solveTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::solveTermination",
    "test_class": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::TimeMillisSpentTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::BestScoreFeasibleTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::AndCompositeTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::BestScoreTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::UnimprovedTimeMillisSpentTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::ScoreCalculationCountTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::OrCompositeTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest",
    "func_start": 57,
    "func_end": 67,
    "body_len": 10,
    "instruction": "Write a method `calculateSolverTimeGradient` that takes a `SolverScope<Solution_>` object as input and returns a double representing the minimum timeGradient across all terminations in the `solverTerminationList`. The method should iterate through each termination, compute its solver time gradient, and only consider those values that are non-negative (i.e., ignore any -1.0 values which indicate unsupported time gradients). Start with an initial timeGradient of 1.0 and update it to the smallest valid (non-negative) timeGradient found. If no termination returns a negative value, the result should be the minimum among them, bounded below by 0.0 implicitly through the use of `Math.min` starting from 1.0.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTermination.java::calculateSolverTimeGradient",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristicAndLocalSearch",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImprovesTooLate_terminates",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImproves_terminationIsPostponed",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::solverTerminationWithConstructionHeuristic",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::calculateSolverTimeGradientTest",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristic",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::calculateSolverTimeGradientTest",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::solverTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::solveTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::solveTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::solveTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::solveTermination"
    ],
    "test_start": 174,
    "test_end": 261,
    "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.termination.UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest#withConstructionHeuristicAndLocalSearch\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void withConstructionHeuristicAndLocalSearch() { // CH ignores unimproved time spent termination.\n        var solverScope = spy(new SolverScope<TestdataSolution>());\n        var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n        var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n        var clock = mock(Clock.class);\n\n        var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n                SimpleScore.of(7), clock);\n\n        doReturn(START_TIME_MILLIS).when(clock).millis();\n        doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n        doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n        termination.solvingStarted(solverScope);\n        termination.phaseStarted(phaseScope);\n        termination.stepEnded(stepScope);\n\n        // CH time has not yet run out\n        doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        termination.stepEnded(stepScope);\n\n        // CH time has not yet run out\n        doReturn(START_TIME_MILLIS + 1001).when(clock).millis();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        termination.phaseEnded(phaseScope);\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        var lsPhaseScope = spy(new LocalSearchPhaseScope<>(solverScope, 0));\n        var lsStepScope = spy(new LocalSearchStepScope<>(lsPhaseScope));\n\n        // second step - score has improved, but not beyond the threshold\n        doReturn(START_TIME_MILLIS + 1501).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 1501).when(lsPhaseScope).getStartingSystemTimeMillis();\n        doReturn(START_TIME_MILLIS + 1501).when(solverScope).getBestSolutionTimeMillis();\n        doReturn(true).when(lsStepScope).getBestScoreImproved();\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(5))).when(solverScope).getBestScore();\n        termination.phaseStarted(lsPhaseScope);\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.5, withPrecision(0.0));\n\n        // third step - score has improved beyond the threshold\n        doReturn(START_TIME_MILLIS + 1502).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 1502).when(solverScope).getBestSolutionTimeMillis();\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(10))).when(solverScope).getBestScore();\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.001, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.501, withPrecision(0.0));\n\n        doReturn(START_TIME_MILLIS + 2001).when(clock).millis();\n        termination.stepEnded(lsStepScope);\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.5, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n        // fourth step - no more improvements\n        doReturn(START_TIME_MILLIS + 2502).when(clock).millis();\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(1.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isTrue();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n        termination.phaseEnded(phaseScope);\n        termination.solvingEnded(solverScope);\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTermination.java-calculatePhaseTimeGradient",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTermination.java",
    "func_name": "calculatePhaseTimeGradient",
    "context": "package ai.timefold.solver.core.impl.solver.termination;\n\nimport java.util.List;\n\nimport ai.timefold.solver.core.impl.phase.scope.AbstractPhaseScope;\nimport ai.timefold.solver.core.impl.solver.scope.SolverScope;\nimport ai.timefold.solver.core.impl.solver.thread.ChildThreadType;\n\nimport org.jspecify.annotations.NullMarked;\n\n@NullMarked\nfinal class AndCompositeTermination<Solution_>\n        extends AbstractCompositeTermination<Solution_>\n        implements ChildThreadSupportingTermination<Solution_, SolverScope<Solution_>> {\n\n    public AndCompositeTermination(List<Termination<Solution_>> terminationList) {\n        super(terminationList);\n    }\n\n    @SafeVarargs\n    public AndCompositeTermination(Termination<Solution_>... terminations) {\n        super(terminations);\n    }\n\n    /**\n     * @return true if all terminations are terminated.\n     */\n    @Override\n    public boolean isSolverTerminated(SolverScope<Solution_> solverScope) {\n        for (var termination : solverTerminationList) {\n            if (!termination.isSolverTerminated(solverScope)) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * @return true if all supported terminations are terminated.\n     */\n    @Override\n    public boolean isPhaseTerminated(AbstractPhaseScope<Solution_> phaseScope) {\n        for (var termination : phaseTerminationList) {\n            if (termination.isApplicableTo(phaseScope.getClass()) && !termination.isPhaseTerminated(phaseScope)) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * Calculates the minimum timeGradient of all Terminations.\n     * Not supported timeGradients (-1.0) are ignored.\n     *\n     * @return the minimum timeGradient of the Terminations.\n     */\n    @Override\n    public double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n        var timeGradient = 1.0;\n        for (var termination : solverTerminationList) {\n            var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.min(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }",
    "func": "    @Override\n    public double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n        var timeGradient = 1.0;\n        for (var termination : phaseTerminationList) {\n            if (!termination.isApplicableTo(phaseScope.getClass())) {\n                continue;\n            }\n            var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.min(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }",
    "comment": "    /**\n     * Calculates the minimum timeGradient of all Terminations.\n     * Not supported timeGradients (-1.0) are ignored.\n     *\n     * @return the minimum timeGradient of the Terminations.\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::phaseTerminationWithConstructionHeuristic core/src/test/java/ai/timefold/solver/core/impl/solver/termination/StepCountTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristicAndLocalSearch core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImprovesTooLate_terminates core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImproves_terminationIsPostponed core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristic core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::calculatePhaseTimeGradientTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedStepCountTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::calculatePhaseTimeGradientTest",
    "test_class": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/StepCountTerminationTest.java::StepCountTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::TimeMillisSpentTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::BestScoreFeasibleTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::AndCompositeTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedStepCountTerminationTest.java::UnimprovedStepCountTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::BestScoreTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::UnimprovedTimeMillisSpentTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::ScoreCalculationCountTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::OrCompositeTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest",
    "func_start": 75,
    "func_end": 88,
    "body_len": 13,
    "instruction": "Write a method `calculatePhaseTimeGradient` that takes an `AbstractPhaseScope<Solution_>` as input and returns a double representing the minimum time gradient among all applicable terminations in the `phaseTerminationList`. The method should skip any termination that is not applicable to the given phase scope (using `isApplicableTo`) and ignore unsupported time gradients (values of -1.0 or less) by only considering non-negative values. The initial minimum time gradient should start at 1.0, and the method should return the smallest valid time gradient found, or 1.0 if no valid gradients are available.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTermination.java::calculatePhaseTimeGradient",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::phaseTerminationWithConstructionHeuristic",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/StepCountTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristicAndLocalSearch",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImprovesTooLate_terminates",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImproves_terminationIsPostponed",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristic",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::calculatePhaseTimeGradientTest",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedStepCountTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::calculatePhaseTimeGradientTest"
    ],
    "test_start": 62,
    "test_end": 94,
    "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.termination.BestScoreTerminationTest#phaseTermination\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void phaseTermination() {\n        ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n        when(scoreDefinition.getLevelsSize()).thenReturn(1);\n        var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n        AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n        when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-900))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTermination.java-calculateSolverTimeGradient",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTermination.java",
    "func_name": "calculateSolverTimeGradient",
    "context": "package ai.timefold.solver.core.impl.solver.termination;\n\nimport java.util.List;\n\nimport ai.timefold.solver.core.impl.phase.scope.AbstractPhaseScope;\nimport ai.timefold.solver.core.impl.solver.scope.SolverScope;\nimport ai.timefold.solver.core.impl.solver.thread.ChildThreadType;\n\nimport org.jspecify.annotations.NullMarked;\n\n@NullMarked\nfinal class OrCompositeTermination<Solution_>\n        extends AbstractCompositeTermination<Solution_>\n        implements ChildThreadSupportingTermination<Solution_, SolverScope<Solution_>> {\n\n    public OrCompositeTermination(List<Termination<Solution_>> terminationList) {\n        super(terminationList);\n    }\n\n    @SafeVarargs\n    public OrCompositeTermination(Termination<Solution_>... terminations) {\n        super(terminations);\n    }\n\n    /**\n     * @param solverScope never null\n     * @return true if any one of the terminations is terminated.\n     */\n    @Override\n    public boolean isSolverTerminated(SolverScope<Solution_> solverScope) {\n        for (var termination : solverTerminationList) {\n            if (termination.isSolverTerminated(solverScope)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    /**\n     * @return true if any one of the supported terminations is terminated.\n     */\n    @Override\n    public boolean isPhaseTerminated(AbstractPhaseScope<Solution_> phaseScope) {\n        for (var termination : phaseTerminationList) {\n            if (!termination.isApplicableTo(phaseScope.getClass())) {\n                continue;\n            }\n            if (termination.isPhaseTerminated(phaseScope)) {\n                return true;\n            }\n        }\n        return false;\n    }",
    "func": "    @Override\n    public double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n        var timeGradient = 0.0;\n        for (var termination : solverTerminationList) {\n            var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.max(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }",
    "comment": "    /**\n     * Calculates the maximum timeGradient of all Terminations.\n     * Not supported timeGradients (-1.0) are ignored.\n     *\n     * @return the maximum timeGradient of the terminations.\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristicAndLocalSearch core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImprovesTooLate_terminates core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImproves_terminationIsPostponed core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::solverTerminationWithConstructionHeuristic core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::calculateSolverTimeGradientTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristic core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::calculateSolverTimeGradientTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::solverTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::solveTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::solveTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::solveTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::solveTermination",
    "test_class": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::TimeMillisSpentTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::BestScoreFeasibleTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::AndCompositeTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::BestScoreTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::UnimprovedTimeMillisSpentTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::ScoreCalculationCountTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::OrCompositeTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest",
    "func_start": 61,
    "func_end": 71,
    "body_len": 10,
    "instruction": "Write a function `calculateSolverTimeGradient` that takes a `SolverScope<Solution_>` object as input and returns a double representing the maximum time gradient among all terminations in the `solverTerminationList`. The function should iterate over each termination, compute its individual time gradient using its own `calculateSolverTimeGradient` method, and ignore any termination that returns a negative time gradient (specifically -1.0, which indicates unsupported). The function should return 0.0 if all terminations return negative gradients, or the highest non-negative time gradient otherwise.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTermination.java::calculateSolverTimeGradient",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristicAndLocalSearch",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImprovesTooLate_terminates",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImproves_terminationIsPostponed",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::solverTerminationWithConstructionHeuristic",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::calculateSolverTimeGradientTest",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristic",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::calculateSolverTimeGradientTest",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::solverTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::solveTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::solveTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::solveTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::solveTermination"
    ],
    "test_start": 174,
    "test_end": 261,
    "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.termination.UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest#withConstructionHeuristicAndLocalSearch\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void withConstructionHeuristicAndLocalSearch() { // CH ignores unimproved time spent termination.\n        var solverScope = spy(new SolverScope<TestdataSolution>());\n        var phaseScope = spy(new ConstructionHeuristicPhaseScope<>(solverScope, 0));\n        var stepScope = spy(new ConstructionHeuristicStepScope<>(phaseScope));\n        var clock = mock(Clock.class);\n\n        var termination = new UnimprovedTimeMillisSpentScoreDifferenceThresholdTermination<TestdataSolution>(1000L,\n                SimpleScore.of(7), clock);\n\n        doReturn(START_TIME_MILLIS).when(clock).millis();\n        doReturn(0L).when(phaseScope).getStartingSystemTimeMillis();\n        doReturn(0L).when(solverScope).getBestSolutionTimeMillis();\n\n        termination.solvingStarted(solverScope);\n        termination.phaseStarted(phaseScope);\n        termination.stepEnded(stepScope);\n\n        // CH time has not yet run out\n        doReturn(START_TIME_MILLIS + 500).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 500).when(phaseScope).getStartingSystemTimeMillis();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        termination.stepEnded(stepScope);\n\n        // CH time has not yet run out\n        doReturn(START_TIME_MILLIS + 1001).when(clock).millis();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        termination.phaseEnded(phaseScope);\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.0, withPrecision(0.0));\n\n        var lsPhaseScope = spy(new LocalSearchPhaseScope<>(solverScope, 0));\n        var lsStepScope = spy(new LocalSearchStepScope<>(lsPhaseScope));\n\n        // second step - score has improved, but not beyond the threshold\n        doReturn(START_TIME_MILLIS + 1501).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 1501).when(lsPhaseScope).getStartingSystemTimeMillis();\n        doReturn(START_TIME_MILLIS + 1501).when(solverScope).getBestSolutionTimeMillis();\n        doReturn(true).when(lsStepScope).getBestScoreImproved();\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(5))).when(solverScope).getBestScore();\n        termination.phaseStarted(lsPhaseScope);\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.5, withPrecision(0.0));\n\n        // third step - score has improved beyond the threshold\n        doReturn(START_TIME_MILLIS + 1502).when(clock).millis();\n        doReturn(START_TIME_MILLIS + 1502).when(solverScope).getBestSolutionTimeMillis();\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(10))).when(solverScope).getBestScore();\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.001, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(0.501, withPrecision(0.0));\n\n        doReturn(START_TIME_MILLIS + 2001).when(clock).millis();\n        termination.stepEnded(lsStepScope);\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(0.5, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isFalse();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n        // fourth step - no more improvements\n        doReturn(START_TIME_MILLIS + 2502).when(clock).millis();\n        termination.stepEnded(lsStepScope);\n\n        assertThat(termination.isPhaseTerminated(lsPhaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(lsPhaseScope)).isEqualTo(1.0, withPrecision(0.0));\n        assertThat(termination.isSolverTerminated(solverScope)).isTrue();\n        assertThat(termination.calculateSolverTimeGradient(solverScope)).isEqualTo(1.0, withPrecision(0.0));\n\n        termination.phaseEnded(phaseScope);\n        termination.solvingEnded(solverScope);\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTermination.java-calculatePhaseTimeGradient",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTermination.java",
    "func_name": "calculatePhaseTimeGradient",
    "context": "package ai.timefold.solver.core.impl.solver.termination;\n\nimport java.util.List;\n\nimport ai.timefold.solver.core.impl.phase.scope.AbstractPhaseScope;\nimport ai.timefold.solver.core.impl.solver.scope.SolverScope;\nimport ai.timefold.solver.core.impl.solver.thread.ChildThreadType;\n\nimport org.jspecify.annotations.NullMarked;\n\n@NullMarked\nfinal class OrCompositeTermination<Solution_>\n        extends AbstractCompositeTermination<Solution_>\n        implements ChildThreadSupportingTermination<Solution_, SolverScope<Solution_>> {\n\n    public OrCompositeTermination(List<Termination<Solution_>> terminationList) {\n        super(terminationList);\n    }\n\n    @SafeVarargs\n    public OrCompositeTermination(Termination<Solution_>... terminations) {\n        super(terminations);\n    }\n\n    /**\n     * @param solverScope never null\n     * @return true if any one of the terminations is terminated.\n     */\n    @Override\n    public boolean isSolverTerminated(SolverScope<Solution_> solverScope) {\n        for (var termination : solverTerminationList) {\n            if (termination.isSolverTerminated(solverScope)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    /**\n     * @return true if any one of the supported terminations is terminated.\n     */\n    @Override\n    public boolean isPhaseTerminated(AbstractPhaseScope<Solution_> phaseScope) {\n        for (var termination : phaseTerminationList) {\n            if (!termination.isApplicableTo(phaseScope.getClass())) {\n                continue;\n            }\n            if (termination.isPhaseTerminated(phaseScope)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    /**\n     * Calculates the maximum timeGradient of all Terminations.\n     * Not supported timeGradients (-1.0) are ignored.\n     *\n     * @return the maximum timeGradient of the terminations.\n     */\n    @Override\n    public double calculateSolverTimeGradient(SolverScope<Solution_> solverScope) {\n        var timeGradient = 0.0;\n        for (var termination : solverTerminationList) {\n            var nextTimeGradient = termination.calculateSolverTimeGradient(solverScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.max(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }",
    "func": "    @Override\n    public double calculatePhaseTimeGradient(AbstractPhaseScope<Solution_> phaseScope) {\n        var timeGradient = 0.0;\n        for (var termination : phaseTerminationList) {\n            if (!termination.isApplicableTo(phaseScope.getClass())) {\n                continue;\n            }\n            var nextTimeGradient = termination.calculatePhaseTimeGradient(phaseScope);\n            if (nextTimeGradient >= 0.0) {\n                timeGradient = Math.max(timeGradient, nextTimeGradient);\n            }\n        }\n        return timeGradient;\n    }",
    "comment": "    /**\n     * Calculates the maximum timeGradient of all Terminations.\n     * Not supported timeGradients (-1.0) are ignored.\n     *\n     * @return the maximum timeGradient of the supported terminations.\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::phaseTerminationWithConstructionHeuristic core/src/test/java/ai/timefold/solver/core/impl/solver/termination/StepCountTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristicAndLocalSearch core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImprovesTooLate_terminates core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImproves_terminationIsPostponed core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristic core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::calculatePhaseTimeGradientTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedStepCountTerminationTest.java::phaseTermination core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::calculatePhaseTimeGradientTest",
    "test_class": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/StepCountTerminationTest.java::StepCountTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::TimeMillisSpentTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::BestScoreFeasibleTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::AndCompositeTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedStepCountTerminationTest.java::UnimprovedStepCountTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::BestScoreTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::UnimprovedTimeMillisSpentTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::ScoreCalculationCountTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::OrCompositeTerminationTest core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest",
    "func_start": 79,
    "func_end": 92,
    "body_len": 13,
    "instruction": "Write a function `calculatePhaseTimeGradient(phaseScope)` that takes a phase scope object and returns the maximum time gradient among all applicable terminations in the `phaseTerminationList`. The function should skip any termination that is not applicable to the given phase scope class and ignore unsupported time gradients (indicated by -1.0). Only non-negative time gradients should be considered, and the function should return 0.0 if no valid time gradients are found.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTermination.java::calculatePhaseTimeGradient",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::phaseTerminationWithConstructionHeuristic",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/StepCountTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristicAndLocalSearch",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImprovesTooLate_terminates",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreFeasibleTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/TimeMillisSpentTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/ScoreCalculationCountTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::scoreImproves_terminationIsPostponed",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedTimeMillisSpentScoreDifferenceThresholdTerminationTest.java::withConstructionHeuristic",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/AndCompositeTerminationTest.java::calculatePhaseTimeGradientTest",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/UnimprovedStepCountTerminationTest.java::phaseTermination",
      "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/OrCompositeTerminationTest.java::calculatePhaseTimeGradientTest"
    ],
    "test_start": 62,
    "test_end": 94,
    "test_file": "core/src/test/java/ai/timefold/solver/core/impl/solver/termination/BestScoreTerminationTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.solver.termination.BestScoreTerminationTest#phaseTermination\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void phaseTermination() {\n        ScoreDefinition<SimpleScore> scoreDefinition = mock(ScoreDefinition.class);\n        when(scoreDefinition.getLevelsSize()).thenReturn(1);\n        var termination = new BestScoreTermination<TestdataSolution>(scoreDefinition, SimpleScore.of(-1000), new double[] {});\n        AbstractPhaseScope<TestdataSolution> phaseScope = mock(AbstractPhaseScope.class);\n        when(phaseScope.isBestSolutionInitialized()).thenReturn(true);\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getStartingScore();\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1100))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1040))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isFalse();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(0.6, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-1000))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n\n        doReturn(InnerScore.fullyAssigned(SimpleScore.of(-900))).when(phaseScope).getBestScore();\n        assertThat(termination.isPhaseTerminated(phaseScope)).isTrue();\n        assertThat(termination.calculatePhaseTimeGradient(phaseScope)).isEqualTo(1.0, offset(0.0));\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtils.java-getCyclesForPermutation",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtils.java",
    "func_name": "getCyclesForPermutation",
    "context": "package ai.timefold.solver.core.impl.heuristic.selector.move.generic.list.kopt;\n\nimport java.util.ArrayList;\nimport java.util.BitSet;\nimport java.util.List;\nimport java.util.function.Function;\n\nimport ai.timefold.solver.core.api.function.TriPredicate;\nimport ai.timefold.solver.core.impl.domain.variable.ListVariableStateSupply;\nimport ai.timefold.solver.core.impl.domain.variable.index.IndexVariableSupply;\nimport ai.timefold.solver.core.impl.util.Pair;\n\nimport org.apache.commons.math3.util.CombinatoricsUtils;\n\nfinal class KOptUtils {\n\n    private KOptUtils() {\n    }",
    "func": "    static KOptCycle getCyclesForPermutation(KOptDescriptor<?> kOptDescriptor) {\n        var cycleCount = 0;\n        var removedEdgeIndexToTourOrder = kOptDescriptor.removedEdgeIndexToTourOrder();\n        var addedEdgeToOtherEndpoint = kOptDescriptor.addedEdgeToOtherEndpoint();\n        var inverseRemovedEdgeIndexToTourOrder = kOptDescriptor.inverseRemovedEdgeIndexToTourOrder();\n\n        var indexToCycle = new int[removedEdgeIndexToTourOrder.length];\n        var remaining = new BitSet(removedEdgeIndexToTourOrder.length);\n        remaining.set(1, removedEdgeIndexToTourOrder.length, true);\n\n        while (!remaining.isEmpty()) {\n            var currentEndpoint = remaining.nextSetBit(0);\n            while (remaining.get(currentEndpoint)) {\n                indexToCycle[currentEndpoint] = cycleCount;\n                remaining.clear(currentEndpoint);\n\n                // Go to the endpoint connected to this one by an added edge\n                var currentEndpointTourIndex = removedEdgeIndexToTourOrder[currentEndpoint];\n                var nextEndpointTourIndex = addedEdgeToOtherEndpoint[currentEndpointTourIndex];\n                currentEndpoint = inverseRemovedEdgeIndexToTourOrder[nextEndpointTourIndex];\n\n                indexToCycle[currentEndpoint] = cycleCount;\n                remaining.clear(currentEndpoint);\n\n                // Go to the endpoint after the added edge\n                currentEndpoint = currentEndpoint ^ 1;\n            }\n            cycleCount++;\n        }\n        return new KOptCycle(cycleCount, indexToCycle);\n    }",
    "comment": "    /**\n     * Calculate the disjoint k-cycles for {@link KOptDescriptor#removedEdgeIndexToTourOrder()}. <br />\n     * <br />\n     * Any permutation can be expressed as combination of k-cycles. A k-cycle is a sequence of\n     * unique elements (p_1, p_2, ..., p_k) where\n     * <ul>\n     * <li>p_1 maps to p_2 in the permutation</li>\n     * <li>p_2 maps to p_3 in the permutation</li>\n     * <li>p_(k-1) maps to p_k in the permutation</li>\n     * <li>p_k maps to p_1 in the permutation</li>\n     * <li>In general: p_i maps to p_(i+1) in the permutation</li>\n     * </ul>\n     * For instance, the permutation\n     * <ul>\n     * <li>1 -> 2</li>\n     * <li>2 -> 3</li>\n     * <li>3 -> 1</li>\n     * <li>4 -> 5</li>\n     * <li>5 -> 4</li>\n     * </ul>\n     * can be expressed as `(1, 2, 3)(4, 5)`.\n     *\n     * @return The {@link KOptCycle} corresponding to the permutation described by\n     *         {@link KOptDescriptor#removedEdgeIndexToTourOrder()}.\n     * @param kOptDescriptor The descriptor to calculate cycles for\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtilsTest.java::testGetCyclesForPermutationThreeCycle core/src/test/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtilsTest.java::testGetCyclesForPermutationOneCycle core/src/test/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtilsTest.java::testGetCyclesForPermutationTwoCycle",
    "test_class": "core/src/test/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtilsTest.java::KOptUtilsTest",
    "func_start": 46,
    "func_end": 76,
    "body_len": 30,
    "instruction": "Write a function `getCyclesForPermutation` that takes a `KOptDescriptor` object and computes the disjoint k-cycles of the permutation defined by its `removedEdgeIndexToTourOrder` mapping, using the `addedEdgeToOtherEndpoint` and `inverseRemovedEdgeIndexToTourOrder` mappings to traverse the permutation structure. The function should label each index with its corresponding cycle number by following the permutation cycle until all elements are processed, using a `BitSet` to track unvisited indices, and return a `KOptCycle` object containing the total number of cycles and an array mapping each endpoint index to its cycle index. The traversal should alternate between following added edges and moving to the next endpoint in the tour order, correctly handling edge pairings via XOR with 1.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtils.java::getCyclesForPermutation",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtilsTest.java::testGetCyclesForPermutationThreeCycle",
      "core/src/test/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtilsTest.java::testGetCyclesForPermutationOneCycle",
      "core/src/test/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtilsTest.java::testGetCyclesForPermutationTwoCycle"
    ],
    "test_start": 122,
    "test_end": 149,
    "test_file": "core/src/test/java/ai/timefold/solver/core/impl/heuristic/selector/move/generic/list/kopt/KOptUtilsTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.impl.heuristic.selector.move.generic.list.kopt.KOptUtilsTest#testGetCyclesForPermutationThreeCycle\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void testGetCyclesForPermutationThreeCycle() {\n        List<TestdataListValue> originalTour = List.of(v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12);\n        List<TestdataListValue> removedEdges = List.of(v1, v2,\n                v3, v4,\n                v5, v6,\n                v7, v8,\n                v9, v10,\n                v11, v12);\n        List<TestdataListValue> addedEdges = List.of(v1, v4,\n                v5, v12,\n                v3, v6,\n                v7, v2,\n                v8, v10,\n                v11, v9);\n\n        KOptDescriptor<TestdataListValue> kOptDescriptor = fromRemovedAndAddedEdges(originalTour,\n                removedEdges,\n                addedEdges);\n        KOptCycle cycleInfo = KOptUtils.getCyclesForPermutation(kOptDescriptor);\n        assertThat(cycleInfo.cycleCount()).isEqualTo(3);\n\n        // Cycles:\n        // v1 -> v4 -> v5 -> v12\n        // v2 -> v7 -> v6 -> v3\n        // v8 -> v10 -> v11 -> v9\n        assertThat(cycleInfo.indexToCycleIdentifier()).containsExactly(0, 0, 1, 1, 0, 0, 1, 1, 2, 2, 2, 2, 0);\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysis.java-diff",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysis.java",
    "func_name": "diff",
    "context": "package ai.timefold.solver.core.api.score.analysis;\n\nimport static java.util.Comparator.comparing;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.LinkedHashMap;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.function.Function;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport ai.timefold.solver.core.api.score.Score;\nimport ai.timefold.solver.core.api.score.ScoreExplanation;\nimport ai.timefold.solver.core.api.score.constraint.ConstraintRef;\nimport ai.timefold.solver.core.api.score.stream.Constraint;\nimport ai.timefold.solver.core.api.score.stream.ConstraintJustification;\nimport ai.timefold.solver.core.api.solver.ScoreAnalysisFetchPolicy;\nimport ai.timefold.solver.core.api.solver.SolutionManager;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * Represents the breakdown of a {@link Score} into individual {@link ConstraintAnalysis} instances,\n * one for each constraint.\n * Compared to {@link ScoreExplanation}, this is JSON-friendly and faster to generate.\n *\n * <p>\n * In order to be fully serializable to JSON, {@link MatchAnalysis} instances must be serializable to JSON\n * and that requires any implementations of {@link ConstraintJustification} to be serializable to JSON.\n * This is the responsibility of the user.\n *\n * <p>\n * For deserialization from JSON, the user needs to provide the deserializer themselves.\n * This is due to the fact that, once the {@link ScoreAnalysis} is received over the wire,\n * we no longer know which {@link Score} type or {@link ConstraintJustification} type was used.\n * The user has all of that information in their domain model,\n * and so they are the correct party to provide the deserializer.\n *\n * <p>\n * Note: the constructors of this record are off-limits.\n * We ask users to use exclusively {@link SolutionManager#analyze(Object)} to obtain instances of this record.\n *\n * @param score Score of the solution being analyzed.\n * @param constraintMap for each constraint identified by its {@link Constraint#getConstraintRef()},\n *        the {@link ConstraintAnalysis} that describes the impact of that constraint on the overall score.\n *        <p>\n *        Zero-weight constraints are never included, they are excluded from score calculation in the first place.\n *        Otherwise constraints are always included, even if they have no matches,\n *        unless the score analysis represents a diff between two other analyses.\n * \n *        <p>\n *        In the case of a diff:\n * \n *        <ul>\n *        <li>If the constraint weight diff is non-zero,\n *        or if the score diff for the constraint is non-zero,\n *        the constraint diff will be included.</li>\n *        <li>\n *        Otherwise if constraint matching is disabled ({@link ScoreAnalysisFetchPolicy#FETCH_SHALLOW})\n *        or if only match counts are available ({@link ScoreAnalysisFetchPolicy#FETCH_MATCH_COUNT}),\n *        constraint diff will only be included if it has a non-zero match count diff.\n *        </li>\n *        <li>\n *        Otherwise (when constraint matching is fully enabled with {@link ScoreAnalysisFetchPolicy#FETCH_ALL})\n *        the constraint diff will not be included if the diff of its constraint matches is empty.\n *        (In other words: when diffing, the analysis for a particular constraint won't be available\n *        if we can guarantee that the constraint matches are identical in both analyses.)\n *        </li>\n *        </ul>\n * \n *        <p>\n *        Entries in the map have a stable iteration order; items are ordered first by {@link ConstraintAnalysis#weight()},\n *        then by {@link ConstraintAnalysis#constraintRef()}.\n * @param isSolutionInitialized Whether the solution was fully initialized at the time of analysis.\n *\n * @param <Score_>\n */\npublic record ScoreAnalysis<Score_ extends Score<Score_>>(@NonNull Score_ score,\n        @NonNull Map<ConstraintRef, ConstraintAnalysis<Score_>> constraintMap,\n        boolean isSolutionInitialized) {\n\n    @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n    private static final Comparator<ConstraintAnalysis<?>> REVERSED_WEIGHT_COMPARATOR =\n            Comparator.<ConstraintAnalysis<?>, Score> comparing(ConstraintAnalysis::weight)\n                    .reversed();\n    private static final Comparator<ConstraintAnalysis<?>> MAP_COMPARATOR =\n            REVERSED_WEIGHT_COMPARATOR.thenComparing(ConstraintAnalysis::constraintRef);\n\n    static final int DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT = 3;\n\n    /**\n     * As defined by {@link #ScoreAnalysis(Score, Map, boolean)},\n     * with the final argument set to true.\n     */\n    public ScoreAnalysis(@NonNull Score_ score, @NonNull Map<ConstraintRef, ConstraintAnalysis<Score_>> constraintMap) {\n        this(score, constraintMap, true);\n    }\n\n    public ScoreAnalysis {\n        Objects.requireNonNull(score, \"score\");\n        Objects.requireNonNull(constraintMap, \"constraintMap\");\n        // Ensure consistent order and no external interference.\n        constraintMap = Collections.unmodifiableMap(constraintMap.values()\n                .stream()\n                .sorted(MAP_COMPARATOR)\n                .collect(Collectors.toMap(\n                        ConstraintAnalysis::constraintRef,\n                        Function.identity(),\n                        (constraintAnalysis, otherConstraintAnalysis) -> constraintAnalysis,\n                        LinkedHashMap::new)));\n    }\n\n    /**\n     * Performs a lookup on {@link #constraintMap()}.\n     * Equivalent to {@code constraintMap().get(constraintRef)}.\n     *\n     * @return null if no constraint matches of such constraint are present\n     */\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull ConstraintRef constraintRef) {\n        return constraintMap.get(constraintRef);\n    }\n\n    /**\n     * As defined by {@link #getConstraintAnalysis(ConstraintRef)}\n     * where the arguments are first composed into a singular constraint ID.\n     *\n     * @return null if no constraint matches of such constraint are present\n     * @deprecated Use {@link #getConstraintAnalysis(String)} instead.\n     */\n    @Deprecated(forRemoval = true, since = \"1.13.0\")\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull String constraintPackage,\n            @NonNull String constraintName) {\n        return getConstraintAnalysis(ConstraintRef.of(constraintPackage, constraintName));\n    }\n\n    /**\n     * As defined by {@link #getConstraintAnalysis(ConstraintRef)}.\n     *\n     * @return null if no constraint matches of such constraint are present\n     * @throws IllegalStateException if multiple constraints with the same name are present,\n     *         which is possible if they are in different constraint packages.\n     *         Constraint packages are deprecated, we recommend avoiding them and instead naming constraints uniquely.\n     *         If you must use constraint packages, see {@link #getConstraintAnalysis(String, String)}\n     *         (also deprecated) and reach out to us to discuss your use case.\n     */\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull String constraintName) {\n        var constraintAnalysisList = constraintMap.entrySet()\n                .stream()\n                .filter(entry -> entry.getKey().constraintName().equals(constraintName))\n                .map(Map.Entry::getValue)\n                .toList();\n        return switch (constraintAnalysisList.size()) {\n            case 0 -> null;\n            case 1 -> constraintAnalysisList.get(0);\n            default -> throw new IllegalStateException(\"\"\"\n                    Multiple constraints with the same name (%s) are present in the score analysis.\n                    This may be caused by the use of multiple constraint packages, a deprecated feature.\n                    Please avoid using constraint packages and keep constraint names unique.\"\"\"\n                    .formatted(constraintName));\n        };\n    }",
    "func": "    public @NonNull ScoreAnalysis<Score_> diff(@NonNull ScoreAnalysis<Score_> other) {\n        var result = Stream.concat(constraintMap.keySet().stream(),\n                other.constraintMap.keySet().stream())\n                .distinct()\n                .flatMap(constraintRef -> {\n                    var constraintAnalysis = getConstraintAnalysis(constraintRef);\n                    var otherConstraintAnalysis = other.getConstraintAnalysis(constraintRef);\n                    var diff = ConstraintAnalysis.diff(constraintRef, constraintAnalysis, otherConstraintAnalysis);\n                    // The following code implements logic to decide which information the user needs to see,\n                    // and which is information we can safely discard.\n                    // This is done so that the diff (which is likely to be serialized into JSON) is not bloated.\n                    if (!diff.weight().isZero() || !diff.score().isZero()) { // Guaranteed change.\n                        return Stream.of(diff);\n                    }\n                    // Figuring out whether constraint matches changed is tricky.\n                    // Can't use constraint weight; weight diff on the same constraint is zero if weight unchanged.\n                    // Can't use matchCount; matchCount diff can be zero if one match was added and another removed.\n                    // To detect if the constraint matches changed, we use the actual match diff.\n                    if (diff.matches() == null) {\n                        // If it is null, either justifications are disabled,\n                        // or constraint matching is disabled altogether.\n                        // This means we don't have enough information to make smarter decisions.\n                        if (diff.matchCount() == 0) {\n                            // Returning this makes no practical sense.\n                            // The result would be constraint name + zero weight + zero score + zero match count.\n                            return Stream.empty();\n                        } else {\n                            return Stream.of(diff);\n                        }\n                    } else if (!diff.matches().isEmpty()) {\n                        // We actually have constraint matches, and they are meaningfully different.\n                        return Stream.of(diff);\n                    } else {\n                        // This will be empty only if all matches are exactly the same.\n                        return Stream.empty();\n                    }\n                })\n                .collect(Collectors.toMap(\n                        ConstraintAnalysis::constraintRef,\n                        Function.identity(),\n                        (constraintRef, otherConstraintRef) -> constraintRef,\n                        HashMap::new));\n        return new ScoreAnalysis<>(score.subtract(other.score()), result, isSolutionInitialized);\n    }",
    "comment": "    /**\n     * Compare this {@link ScoreAnalysis} to another {@link ScoreAnalysis}\n     * and retrieve the difference between them.\n     * The comparison is in the direction of {@code this - other}.\n     * <p>\n     * Example: if {@code this} has a score of 100 and {@code other} has a score of 90,\n     * the returned {@link ScoreAnalysis#score} will be 10.\n     * If this and other were inverted, the score would have been -10.\n     * The same applies to all other properties of {@link ScoreAnalysis}.\n     *\n     * <p>\n     * In order to properly diff {@link MatchAnalysis} against each other,\n     * we rely on the user implementing {@link ConstraintJustification} equality correctly.\n     * In other words, the diff will consider two justifications equal if the user says they are equal,\n     * and it expects the hash code to be consistent with equals.\n     *\n     * <p>\n     * If one {@link ScoreAnalysis} provides {@link MatchAnalysis} and the other doesn't, exception is thrown.\n     * Such {@link ScoreAnalysis} instances are mutually incompatible.\n     * \n     * <p>\n     * If {@code this} came from a fully initialized solution,\n     * {@link #isSolutionInitialized} will be true.\n     * False otherwise.\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::compareWithConstraintMatchesWithoutMatchAnalysis core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::empty core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::compareWithConstraintMatchesAndMatchAnalysis",
    "test_class": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::ScoreAnalysisTest",
    "func_start": 193,
    "func_end": 236,
    "body_len": 43,
    "instruction": "Write a function `diff` in the `ScoreAnalysis` class that takes another `ScoreAnalysis` object as input and returns a new `ScoreAnalysis` representing the difference between the current object and the other, where the difference is computed as `this - other`. The function should compute differences in score, constraint weights, match counts, and detailed match analyses (using `ConstraintAnalysis.diff`). It should only include constraint analyses in the result if there is a non-zero difference in weight or score, or if there are meaningful changes in match details (detected via non-empty match diffs). If match data is unavailable or unchanged, it should be excluded to avoid bloating the result. The resulting `ScoreAnalysis` should carry the subtracted score, the filtered map of constraint differences, and preserve the `isSolutionInitialized` flag from `this`. The method must throw an exception if one `ScoreAnalysis` has match data and the other does not, ensuring compatibility.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysis.java::diff",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::compareWithConstraintMatchesWithoutMatchAnalysis",
      "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::empty",
      "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::compareWithConstraintMatchesAndMatchAnalysis"
    ],
    "test_start": 25,
    "test_end": 42,
    "test_file": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.api.score.analysis.ScoreAnalysisTest#empty\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void empty() {\n        var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n        var scoreAnalysis2 = new ScoreAnalysis<>(SimpleScore.of(0), Collections.emptyMap());\n\n        var diff = scoreAnalysis.diff(scoreAnalysis2);\n        assertSoftly(softly -> {\n            softly.assertThat(diff.score()).isEqualTo(SimpleScore.of(0));\n            softly.assertThat(diff.constraintMap()).isEmpty();\n        });\n\n        var summary = scoreAnalysis.summarize();\n        assertThat(summary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                        \"\"\");\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysis.java-summarize",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysis.java",
    "func_name": "summarize",
    "context": "package ai.timefold.solver.core.api.score.analysis;\n\nimport static java.util.Comparator.comparing;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.LinkedHashMap;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.function.Function;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport ai.timefold.solver.core.api.score.Score;\nimport ai.timefold.solver.core.api.score.ScoreExplanation;\nimport ai.timefold.solver.core.api.score.constraint.ConstraintRef;\nimport ai.timefold.solver.core.api.score.stream.Constraint;\nimport ai.timefold.solver.core.api.score.stream.ConstraintJustification;\nimport ai.timefold.solver.core.api.solver.ScoreAnalysisFetchPolicy;\nimport ai.timefold.solver.core.api.solver.SolutionManager;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * Represents the breakdown of a {@link Score} into individual {@link ConstraintAnalysis} instances,\n * one for each constraint.\n * Compared to {@link ScoreExplanation}, this is JSON-friendly and faster to generate.\n *\n * <p>\n * In order to be fully serializable to JSON, {@link MatchAnalysis} instances must be serializable to JSON\n * and that requires any implementations of {@link ConstraintJustification} to be serializable to JSON.\n * This is the responsibility of the user.\n *\n * <p>\n * For deserialization from JSON, the user needs to provide the deserializer themselves.\n * This is due to the fact that, once the {@link ScoreAnalysis} is received over the wire,\n * we no longer know which {@link Score} type or {@link ConstraintJustification} type was used.\n * The user has all of that information in their domain model,\n * and so they are the correct party to provide the deserializer.\n *\n * <p>\n * Note: the constructors of this record are off-limits.\n * We ask users to use exclusively {@link SolutionManager#analyze(Object)} to obtain instances of this record.\n *\n * @param score Score of the solution being analyzed.\n * @param constraintMap for each constraint identified by its {@link Constraint#getConstraintRef()},\n *        the {@link ConstraintAnalysis} that describes the impact of that constraint on the overall score.\n *        <p>\n *        Zero-weight constraints are never included, they are excluded from score calculation in the first place.\n *        Otherwise constraints are always included, even if they have no matches,\n *        unless the score analysis represents a diff between two other analyses.\n * \n *        <p>\n *        In the case of a diff:\n * \n *        <ul>\n *        <li>If the constraint weight diff is non-zero,\n *        or if the score diff for the constraint is non-zero,\n *        the constraint diff will be included.</li>\n *        <li>\n *        Otherwise if constraint matching is disabled ({@link ScoreAnalysisFetchPolicy#FETCH_SHALLOW})\n *        or if only match counts are available ({@link ScoreAnalysisFetchPolicy#FETCH_MATCH_COUNT}),\n *        constraint diff will only be included if it has a non-zero match count diff.\n *        </li>\n *        <li>\n *        Otherwise (when constraint matching is fully enabled with {@link ScoreAnalysisFetchPolicy#FETCH_ALL})\n *        the constraint diff will not be included if the diff of its constraint matches is empty.\n *        (In other words: when diffing, the analysis for a particular constraint won't be available\n *        if we can guarantee that the constraint matches are identical in both analyses.)\n *        </li>\n *        </ul>\n * \n *        <p>\n *        Entries in the map have a stable iteration order; items are ordered first by {@link ConstraintAnalysis#weight()},\n *        then by {@link ConstraintAnalysis#constraintRef()}.\n * @param isSolutionInitialized Whether the solution was fully initialized at the time of analysis.\n *\n * @param <Score_>\n */\npublic record ScoreAnalysis<Score_ extends Score<Score_>>(@NonNull Score_ score,\n        @NonNull Map<ConstraintRef, ConstraintAnalysis<Score_>> constraintMap,\n        boolean isSolutionInitialized) {\n\n    @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n    private static final Comparator<ConstraintAnalysis<?>> REVERSED_WEIGHT_COMPARATOR =\n            Comparator.<ConstraintAnalysis<?>, Score> comparing(ConstraintAnalysis::weight)\n                    .reversed();\n    private static final Comparator<ConstraintAnalysis<?>> MAP_COMPARATOR =\n            REVERSED_WEIGHT_COMPARATOR.thenComparing(ConstraintAnalysis::constraintRef);\n\n    static final int DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT = 3;\n\n    /**\n     * As defined by {@link #ScoreAnalysis(Score, Map, boolean)},\n     * with the final argument set to true.\n     */\n    public ScoreAnalysis(@NonNull Score_ score, @NonNull Map<ConstraintRef, ConstraintAnalysis<Score_>> constraintMap) {\n        this(score, constraintMap, true);\n    }\n\n    public ScoreAnalysis {\n        Objects.requireNonNull(score, \"score\");\n        Objects.requireNonNull(constraintMap, \"constraintMap\");\n        // Ensure consistent order and no external interference.\n        constraintMap = Collections.unmodifiableMap(constraintMap.values()\n                .stream()\n                .sorted(MAP_COMPARATOR)\n                .collect(Collectors.toMap(\n                        ConstraintAnalysis::constraintRef,\n                        Function.identity(),\n                        (constraintAnalysis, otherConstraintAnalysis) -> constraintAnalysis,\n                        LinkedHashMap::new)));\n    }\n\n    /**\n     * Performs a lookup on {@link #constraintMap()}.\n     * Equivalent to {@code constraintMap().get(constraintRef)}.\n     *\n     * @return null if no constraint matches of such constraint are present\n     */\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull ConstraintRef constraintRef) {\n        return constraintMap.get(constraintRef);\n    }\n\n    /**\n     * As defined by {@link #getConstraintAnalysis(ConstraintRef)}\n     * where the arguments are first composed into a singular constraint ID.\n     *\n     * @return null if no constraint matches of such constraint are present\n     * @deprecated Use {@link #getConstraintAnalysis(String)} instead.\n     */\n    @Deprecated(forRemoval = true, since = \"1.13.0\")\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull String constraintPackage,\n            @NonNull String constraintName) {\n        return getConstraintAnalysis(ConstraintRef.of(constraintPackage, constraintName));\n    }\n\n    /**\n     * As defined by {@link #getConstraintAnalysis(ConstraintRef)}.\n     *\n     * @return null if no constraint matches of such constraint are present\n     * @throws IllegalStateException if multiple constraints with the same name are present,\n     *         which is possible if they are in different constraint packages.\n     *         Constraint packages are deprecated, we recommend avoiding them and instead naming constraints uniquely.\n     *         If you must use constraint packages, see {@link #getConstraintAnalysis(String, String)}\n     *         (also deprecated) and reach out to us to discuss your use case.\n     */\n    public @Nullable ConstraintAnalysis<Score_> getConstraintAnalysis(@NonNull String constraintName) {\n        var constraintAnalysisList = constraintMap.entrySet()\n                .stream()\n                .filter(entry -> entry.getKey().constraintName().equals(constraintName))\n                .map(Map.Entry::getValue)\n                .toList();\n        return switch (constraintAnalysisList.size()) {\n            case 0 -> null;\n            case 1 -> constraintAnalysisList.get(0);\n            default -> throw new IllegalStateException(\"\"\"\n                    Multiple constraints with the same name (%s) are present in the score analysis.\n                    This may be caused by the use of multiple constraint packages, a deprecated feature.\n                    Please avoid using constraint packages and keep constraint names unique.\"\"\"\n                    .formatted(constraintName));\n        };\n    }\n\n    /**\n     * Compare this {@link ScoreAnalysis} to another {@link ScoreAnalysis}\n     * and retrieve the difference between them.\n     * The comparison is in the direction of {@code this - other}.\n     * <p>\n     * Example: if {@code this} has a score of 100 and {@code other} has a score of 90,\n     * the returned {@link ScoreAnalysis#score} will be 10.\n     * If this and other were inverted, the score would have been -10.\n     * The same applies to all other properties of {@link ScoreAnalysis}.\n     *\n     * <p>\n     * In order to properly diff {@link MatchAnalysis} against each other,\n     * we rely on the user implementing {@link ConstraintJustification} equality correctly.\n     * In other words, the diff will consider two justifications equal if the user says they are equal,\n     * and it expects the hash code to be consistent with equals.\n     *\n     * <p>\n     * If one {@link ScoreAnalysis} provides {@link MatchAnalysis} and the other doesn't, exception is thrown.\n     * Such {@link ScoreAnalysis} instances are mutually incompatible.\n     * \n     * <p>\n     * If {@code this} came from a fully initialized solution,\n     * {@link #isSolutionInitialized} will be true.\n     * False otherwise.\n     */\n    public @NonNull ScoreAnalysis<Score_> diff(@NonNull ScoreAnalysis<Score_> other) {\n        var result = Stream.concat(constraintMap.keySet().stream(),\n                other.constraintMap.keySet().stream())\n                .distinct()\n                .flatMap(constraintRef -> {\n                    var constraintAnalysis = getConstraintAnalysis(constraintRef);\n                    var otherConstraintAnalysis = other.getConstraintAnalysis(constraintRef);\n                    var diff = ConstraintAnalysis.diff(constraintRef, constraintAnalysis, otherConstraintAnalysis);\n                    // The following code implements logic to decide which information the user needs to see,\n                    // and which is information we can safely discard.\n                    // This is done so that the diff (which is likely to be serialized into JSON) is not bloated.\n                    if (!diff.weight().isZero() || !diff.score().isZero()) { // Guaranteed change.\n                        return Stream.of(diff);\n                    }\n                    // Figuring out whether constraint matches changed is tricky.\n                    // Can't use constraint weight; weight diff on the same constraint is zero if weight unchanged.\n                    // Can't use matchCount; matchCount diff can be zero if one match was added and another removed.\n                    // To detect if the constraint matches changed, we use the actual match diff.\n                    if (diff.matches() == null) {\n                        // If it is null, either justifications are disabled,\n                        // or constraint matching is disabled altogether.\n                        // This means we don't have enough information to make smarter decisions.\n                        if (diff.matchCount() == 0) {\n                            // Returning this makes no practical sense.\n                            // The result would be constraint name + zero weight + zero score + zero match count.\n                            return Stream.empty();\n                        } else {\n                            return Stream.of(diff);\n                        }\n                    } else if (!diff.matches().isEmpty()) {\n                        // We actually have constraint matches, and they are meaningfully different.\n                        return Stream.of(diff);\n                    } else {\n                        // This will be empty only if all matches are exactly the same.\n                        return Stream.empty();\n                    }\n                })\n                .collect(Collectors.toMap(\n                        ConstraintAnalysis::constraintRef,\n                        Function.identity(),\n                        (constraintRef, otherConstraintRef) -> constraintRef,\n                        HashMap::new));\n        return new ScoreAnalysis<>(score.subtract(other.score()), result, isSolutionInitialized);\n    }\n\n    /**\n     * Returns individual {@link ConstraintAnalysis} instances that make up this {@link ScoreAnalysis}.\n     *\n     * @return equivalent to {@code constraintMap().values()}\n     */\n    public Collection<ConstraintAnalysis<Score_>> constraintAnalyses() {\n        return constraintMap.values();\n    }",
    "func": "    @SuppressWarnings(\"java:S3457\")\n    public @NonNull String summarize() {\n        StringBuilder summary = new StringBuilder();\n        summary.append(\"\"\"\n                Explanation of score (%s):\n                    Constraint matches:\n                \"\"\".formatted(score));\n        Comparator<ConstraintAnalysis<Score_>> constraintsScoreComparator = comparing(ConstraintAnalysis::score);\n        Comparator<MatchAnalysis<Score_>> matchScoreComparator = comparing(MatchAnalysis::score);\n\n        constraintAnalyses().stream()\n                .sorted(constraintsScoreComparator)\n                .forEach(constraint -> {\n                    var matches = constraint.matches();\n                    if (matches == null) {\n                        throw new IllegalArgumentException(\"\"\"\n                                The constraint matches must be non-null.\n                                Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\n                                \"\"\");\n                    }\n                    if (matches.isEmpty()) {\n                        summary.append(\n                                \"%8s%s: constraint (%s) has no matches.\\n\".formatted(\" \", constraint.score().toShortString(),\n                                        constraint.constraintRef().constraintName()));\n                    } else {\n                        summary.append(\n                                \"%8s%s: constraint (%s) has %s matches:\\n\".formatted(\" \", constraint.score().toShortString(),\n                                        constraint.constraintRef().constraintName(), matches.size()));\n                    }\n                    matches.stream()\n                            .sorted(matchScoreComparator)\n                            .limit(DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT)\n                            .forEach(match -> summary\n                                    .append(\"%12s%s: justified with (%s)\\n\".formatted(\" \", match.score().toShortString(),\n                                            match.justification())));\n                    if (matches.size() > DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT) {\n                        summary.append(\"%12s%s\\n\".formatted(\" \", \"...\"));\n                    }\n                });\n\n        return summary.toString();\n    }",
    "comment": "    /**\n     * Returns a diagnostic text that explains the solution through the {@link ConstraintAnalysis} API to identify which\n     * constraints cause that score quality.\n     * The string is built fresh every time the method is called.\n     * <p>\n     * In case of an {@link Score#isFeasible() infeasible} solution, this can help diagnose the cause of that.\n     *\n     * <p>\n     * Do not parse the return value, its format may change without warning.\n     * Instead, provide this information in a UI or a service,\n     * use {@link ScoreAnalysis#constraintAnalyses()}\n     * and convert those into a domain-specific API.\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::summarizeUninitializedSolution core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::empty core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::summarize",
    "test_class": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::ScoreAnalysisTest",
    "func_start": 260,
    "func_end": 301,
    "body_len": 41,
    "instruction": "Write a function `summarize()` that returns a human-readable string explaining the score of a solution by analyzing constraints and their matches using the `ConstraintAnalysis` API. The function should format the output to show each constraint's score, name, and number of matches, then list up to a limited number of matches (using `DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT`) with their individual scores and justifications, sorted by score. If a constraint has no matches, indicate that clearly. If there are more matches than the limit, append an ellipsis. The summary should start with the overall score and be rebuilt fresh on each call. Include proper formatting for readability, and throw an `IllegalArgumentException` if any constraint's matches are null, suggesting the use of `ScoreAnalysisFetchPolicy.FETCH_ALL`.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysis.java::summarize",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::summarizeUninitializedSolution",
      "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::empty",
      "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::summarize"
    ],
    "test_start": 110,
    "test_end": 146,
    "test_file": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.api.score.analysis.ScoreAnalysisTest#summarizeUninitializedSolution\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void summarizeUninitializedSolution() {\n        var constraintPackage = \"constraintPackage\";\n        var constraintName1 = \"constraint1\";\n        var constraintName2 = \"constraint2\";\n        var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n        var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n        var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n        var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n        var constraintAnalysisMap = Map.of(\n                constraintMatchTotal.getConstraintRef(),\n                getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n                constraintMatchTotal2.getConstraintRef(),\n                getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n        var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n        // Single constraint analysis\n        var constraintSummary = constraintAnalysisMap.get(constraintMatchTotal.getConstraintRef()).summarize();\n        assertThat(constraintSummary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                                0: constraint (constraint1) has no matches.\n                        \"\"\");\n\n        // Complete score analysis\n        var summary = scoreAnalysis.summarize();\n        assertThat(scoreAnalysis.getConstraintAnalysis(constraintName1).matchCount()).isZero();\n        assertThat(summary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                                0: constraint (constraint1) has no matches.\n                                0: constraint (constraint2) has no matches.\n                        \"\"\");\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/api/score/analysis/ConstraintAnalysis.java-summarize",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/api/score/analysis/ConstraintAnalysis.java",
    "func_name": "summarize",
    "context": "package ai.timefold.solver.core.api.score.analysis;\n\nimport static ai.timefold.solver.core.api.score.analysis.ScoreAnalysis.DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT;\nimport static java.util.Comparator.comparing;\n\nimport java.util.Comparator;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Stream;\n\nimport ai.timefold.solver.core.api.score.Score;\nimport ai.timefold.solver.core.api.score.calculator.ConstraintMatchAwareIncrementalScoreCalculator;\nimport ai.timefold.solver.core.api.score.constraint.ConstraintRef;\nimport ai.timefold.solver.core.api.score.stream.ConstraintJustification;\nimport ai.timefold.solver.core.api.solver.SolutionManager;\nimport ai.timefold.solver.core.impl.score.constraint.DefaultConstraintMatchTotal;\nimport ai.timefold.solver.core.impl.util.CollectionUtils;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\n/**\n * Note: Users should never create instances of this type directly.\n * It is available transitively via {@link SolutionManager#analyze(Object)}.\n *\n * @param <Score_>\n * @param matches null if analysis not available;\n *        empty if constraint has no matches, but still non-zero constraint weight;\n *        non-empty if constraint has matches.\n *        This is a {@link List} to simplify access to individual elements,\n *        but it contains no duplicates just like {@link HashSet} wouldn't.\n * @param matchCount\n *        <ul>\n *        <li>For regular constraint analysis:\n *        -1 if analysis not available,\n *        0 if constraint has no matches,\n *        positive if constraint has matches.\n *        Equal to the size of the {@link #matches} list.</li>\n *        <li>For a {@link ScoreAnalysis#diff(ScoreAnalysis) diff of constraint analyses}:\n *        positive if the constraint has more matches in the new analysis,\n *        zero if the number of matches is the same in both,\n *        negative otherwise.\n *        Need not be equal to the size of the {@link #matches} list.</li>\n *        </ul>\n */\npublic record ConstraintAnalysis<Score_ extends Score<Score_>>(@NonNull ConstraintRef constraintRef, @NonNull Score_ weight,\n        @NonNull Score_ score, @Nullable List<MatchAnalysis<Score_>> matches, int matchCount) {\n\n    public ConstraintAnalysis(@NonNull ConstraintRef constraintRef, @NonNull Score_ weight, @NonNull Score_ score,\n            @Nullable List<MatchAnalysis<Score_>> matches) {\n        this(constraintRef, weight, score, matches, matches == null ? -1 : matches.size());\n    }\n\n    public ConstraintAnalysis {\n        Objects.requireNonNull(constraintRef);\n        /*\n         * Null only possible in ConstraintMatchAwareIncrementalScoreCalculator and/or tests.\n         * Easy doesn't support constraint analysis at all.\n         * CS always provides constraint weights.\n         */\n        Objects.requireNonNull(weight, () -> \"\"\"\n                The constraint weight must be non-null.\n                Maybe use a non-deprecated %s constructor in your %s implementation?\"\"\"\n                .formatted(DefaultConstraintMatchTotal.class.getSimpleName(),\n                        ConstraintMatchAwareIncrementalScoreCalculator.class.getSimpleName()));\n        Objects.requireNonNull(score);\n    }\n\n    @NonNull\n    ConstraintAnalysis<Score_> negate() {\n        // Only used to compute diff; use semantics for non-diff.\n        // A negative match count is only allowed within these semantics when matches == null.\n        if (matches == null) {\n            // At this point, matchCount is already negative, as matches == null.\n            return new ConstraintAnalysis<>(constraintRef, weight.negate(), score.negate(), null, matchCount);\n        } else {\n            // Within these semantics, match count == list size.\n            var negatedMatchAnalysesList = matches.stream()\n                    .map(MatchAnalysis::negate)\n                    .toList();\n            return new ConstraintAnalysis<>(constraintRef, weight.negate(), score.negate(), negatedMatchAnalysesList,\n                    matchCount);\n        }\n    }\n\n    static <Score_ extends Score<Score_>> @NonNull ConstraintAnalysis<Score_> diff(\n            @NonNull ConstraintRef constraintRef, @Nullable ConstraintAnalysis<Score_> constraintAnalysis,\n            @Nullable ConstraintAnalysis<Score_> otherConstraintAnalysis) {\n        if (constraintAnalysis == null) {\n            if (otherConstraintAnalysis == null) {\n                throw new IllegalStateException(\n                        \"Impossible state: none of the score explanations provided constraint matches for a constraint (%s).\"\n                                .formatted(constraintRef));\n            }\n            // No need to compute diff; this constraint is not present in this score explanation.\n            return otherConstraintAnalysis.negate();\n        } else if (otherConstraintAnalysis == null) {\n            // No need to compute diff; this constraint is not present in the other score explanation.\n            return constraintAnalysis;\n        }\n        var matchAnalyses = constraintAnalysis.matches();\n        var otherMatchAnalyses = otherConstraintAnalysis.matches();\n        if ((matchAnalyses == null && otherMatchAnalyses != null) || (matchAnalyses != null && otherMatchAnalyses == null)) {\n            throw new IllegalStateException(\n                    \"Impossible state: One of the score analyses (%s, %s) provided no match analysis for a constraint (%s).\"\n                            .formatted(constraintAnalysis, otherConstraintAnalysis, constraintRef));\n        }\n        // Compute the diff.\n        var constraintWeightDifference = constraintAnalysis.weight().subtract(otherConstraintAnalysis.weight());\n        var scoreDifference = constraintAnalysis.score().subtract(otherConstraintAnalysis.score());\n        if (matchAnalyses == null) {\n            var leftHasMatchCount = hasMatchCount(constraintAnalysis);\n            var rightHasMatchCount = hasMatchCount(otherConstraintAnalysis);\n            if ((!leftHasMatchCount && rightHasMatchCount) || (leftHasMatchCount && !rightHasMatchCount)) {\n                throw new IllegalStateException(\n                        \"Impossible state: One of the score analyses (%s, %s) provided no match count for a constraint (%s).\"\n                                .formatted(constraintAnalysis, otherConstraintAnalysis, constraintRef));\n            }\n            return new ConstraintAnalysis<>(constraintRef, constraintWeightDifference, scoreDifference, null,\n                    getMatchCount(constraintAnalysis, otherConstraintAnalysis));\n        }\n        var matchAnalysisMap = mapMatchesToJustifications(matchAnalyses);\n        var otherMatchAnalysisMap = mapMatchesToJustifications(otherMatchAnalyses);\n        var matchAnalysesList = Stream.concat(matchAnalysisMap.keySet().stream(), otherMatchAnalysisMap.keySet().stream())\n                .distinct()\n                .flatMap(justification -> {\n                    var matchAnalysis = matchAnalysisMap.get(justification);\n                    var otherMatchAnalysis = otherMatchAnalysisMap.get(justification);\n                    if (matchAnalysis == null) {\n                        if (otherMatchAnalysis == null) {\n                            throw new IllegalStateException(\n                                    \"Impossible state: none of the match analyses provided for a constraint (%s).\"\n                                            .formatted(constraintRef));\n                        }\n                        // No need to compute diff; this match is not present in this score explanation.\n                        return Stream.of(otherMatchAnalysis.negate());\n                    } else if (otherMatchAnalysis == null) {\n                        // No need to compute diff; this match is not present in the other score explanation.\n                        return Stream.of(matchAnalysis);\n                    } else if (!matchAnalysis.equals(otherMatchAnalysis)) { // Compute the diff.\n                        return Stream.of(new MatchAnalysis<>(constraintRef,\n                                matchAnalysis.score().subtract(otherMatchAnalysis.score()), justification));\n                    } else { // There is no difference; skip entirely.\n                        return Stream.empty();\n                    }\n                }).toList();\n        return new ConstraintAnalysis<>(constraintRef, constraintWeightDifference, scoreDifference, matchAnalysesList,\n                getMatchCount(constraintAnalysis, otherConstraintAnalysis));\n    }\n\n    private static boolean hasMatchCount(ConstraintAnalysis<?> analysis) {\n        return analysis.matchCount >= 0;\n    }\n\n    private static int getMatchCount(ConstraintAnalysis<?> analysis, ConstraintAnalysis<?> otherAnalysis) {\n        return analysis.matchCount() - otherAnalysis.matchCount();\n    }\n\n    private static <Score_ extends Score<Score_>> Map<ConstraintJustification, MatchAnalysis<Score_>>\n            mapMatchesToJustifications(List<MatchAnalysis<Score_>> matchAnalyses) {\n        Map<ConstraintJustification, MatchAnalysis<Score_>> matchAnalysisMap =\n                CollectionUtils.newLinkedHashMap(matchAnalyses.size());\n        for (var matchAnalysis : matchAnalyses) {\n            var previous = matchAnalysisMap.put(matchAnalysis.justification(), matchAnalysis);\n            if (previous != null) {\n                // Match analysis for the same justification should have been merged already.\n                throw new IllegalStateException(\n                        \"Impossible state: multiple constraint matches (%s, %s) have the same justification (%s).\"\n                                .formatted(previous, matchAnalysis, matchAnalysis.justification()));\n            }\n        }\n        return matchAnalysisMap;\n    }\n\n    /**\n     * Return package name of the constraint that this analysis is for.\n     *\n     * @return equal to {@code constraintRef.packageName()}\n     * @deprecated Do not rely on constraint package in user code.\n     */\n    @Deprecated(forRemoval = true, since = \"1.13.0\")\n    public String constraintPackage() {\n        return constraintRef.packageName();\n    }\n\n    /**\n     * Return name of the constraint that this analysis is for.\n     *\n     * @return equal to {@code constraintRef.constraintName()}\n     */\n    public @NonNull String constraintName() {\n        return constraintRef.constraintName();\n    }",
    "func": "    @SuppressWarnings(\"java:S3457\")\n    public @NonNull String summarize() {\n        var summary = new StringBuilder();\n        summary.append(\"\"\"\n                Explanation of score (%s):\n                    Constraint matches:\n                \"\"\".formatted(score));\n        Comparator<MatchAnalysis<Score_>> matchScoreComparator = comparing(MatchAnalysis::score);\n\n        var constraintMatches = matches();\n        if (constraintMatches == null) {\n            throw new IllegalArgumentException(\"\"\"\n                    The constraint matches must be non-null.\n                    Maybe use ScoreAnalysisFetchPolicy.FETCH_ALL to request the score analysis\n                    \"\"\");\n        }\n        if (constraintMatches.isEmpty()) {\n            summary.append(\n                    \"%8s%s: constraint (%s) has no matches.\\n\".formatted(\" \", score().toShortString(),\n                            constraintRef().constraintName()));\n        } else {\n            summary.append(\"%8s%s: constraint (%s) has %s matches:\\n\".formatted(\" \", score().toShortString(),\n                    constraintRef().constraintName(), constraintMatches.size()));\n        }\n        constraintMatches.stream()\n                .sorted(matchScoreComparator)\n                .limit(DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT)\n                .forEach(match -> summary.append(\"%12S%s: justified with (%s)\\n\".formatted(\" \", match.score().toShortString(),\n                        match.justification())));\n        if (constraintMatches.size() > DEFAULT_SUMMARY_CONSTRAINT_MATCH_LIMIT) {\n            summary.append(\"%12s%s\\n\".formatted(\" \", \"...\"));\n        }\n\n        return summary.toString();\n    }",
    "comment": "    /**\n     * Returns a diagnostic text that explains part of the score quality through the {@link ConstraintAnalysis} API.\n     * The string is built fresh every time the method is called.\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::summarizeUninitializedSolution core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::empty core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::summarize",
    "test_class": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::ScoreAnalysisTest",
    "func_start": 201,
    "func_end": 235,
    "body_len": 34,
    "instruction": "Write a method `summarize()` that returns a non-null string providing a human-readable diagnostic explanation of a score's quality based on constraint matches. The summary should start with the overall score, list the number of matches for the constraint, and include a sorted (by score) and limited (to a default maximum) list of match details, each showing the match score and justification. If there are more matches than the limit, append an ellipsis. If there are no matches, indicate that clearly. Include proper formatting for readability and throw an IllegalArgumentException if the constraint matches are null, suggesting the use of a specific fetch policy to obtain the data.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/api/score/analysis/ConstraintAnalysis.java::summarize",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::summarizeUninitializedSolution",
      "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::empty",
      "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java::summarize"
    ],
    "test_start": 110,
    "test_end": 146,
    "test_file": "core/src/test/java/ai/timefold/solver/core/api/score/analysis/ScoreAnalysisTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.api.score.analysis.ScoreAnalysisTest#summarizeUninitializedSolution\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void summarizeUninitializedSolution() {\n        var constraintPackage = \"constraintPackage\";\n        var constraintName1 = \"constraint1\";\n        var constraintName2 = \"constraint2\";\n        var constraintId1 = ConstraintRef.of(constraintPackage, constraintName1);\n        var constraintId2 = ConstraintRef.of(constraintPackage, constraintName2);\n\n        var constraintMatchTotal = new DefaultConstraintMatchTotal<>(constraintId1, SimpleScore.of(0));\n        var constraintMatchTotal2 = new DefaultConstraintMatchTotal<>(constraintId2, SimpleScore.of(0));\n        var constraintAnalysisMap = Map.of(\n                constraintMatchTotal.getConstraintRef(),\n                getConstraintAnalysis(constraintMatchTotal, ScoreAnalysisFetchPolicy.FETCH_ALL),\n                constraintMatchTotal2.getConstraintRef(),\n                getConstraintAnalysis(constraintMatchTotal2, ScoreAnalysisFetchPolicy.FETCH_ALL));\n        var scoreAnalysis = new ScoreAnalysis<>(SimpleScore.ZERO, constraintAnalysisMap, false);\n\n        // Single constraint analysis\n        var constraintSummary = constraintAnalysisMap.get(constraintMatchTotal.getConstraintRef()).summarize();\n        assertThat(constraintSummary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                                0: constraint (constraint1) has no matches.\n                        \"\"\");\n\n        // Complete score analysis\n        var summary = scoreAnalysis.summarize();\n        assertThat(scoreAnalysis.getConstraintAnalysis(constraintName1).matchCount()).isZero();\n        assertThat(summary)\n                .isEqualTo(\"\"\"\n                        Explanation of score (0):\n                            Constraint matches:\n                                0: constraint (constraint1) has no matches.\n                                0: constraint (constraint2) has no matches.\n                        \"\"\");\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/api/solver/ProblemSizeStatistics.java-format",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/api/solver/ProblemSizeStatistics.java",
    "func_name": "format",
    "context": "package ai.timefold.solver.core.api.solver;\n\nimport java.text.DecimalFormat;\nimport java.text.DecimalFormatSymbols;\nimport java.util.Locale;\n\nimport ai.timefold.solver.core.impl.util.MathUtils;\n\nimport org.jspecify.annotations.NonNull;\n\n/**\n * The statistics of a given problem submitted to a {@link Solver}.\n *\n * @param entityCount The number of genuine entities defined by the problem.\n * @param variableCount The number of genuine variables defined by the problem.\n * @param approximateValueCount The estimated number of values defined by the problem.\n *        Can be larger than the actual value count.\n * @param approximateProblemSizeLog The estimated log_10 of the problem's search space size.\n */\npublic record ProblemSizeStatistics(long entityCount,\n        long variableCount,\n        long approximateValueCount,\n        double approximateProblemSizeLog) {\n\n    private static final Locale FORMATTER_LOCALE = Locale.getDefault();\n    private static final DecimalFormat BASIC_FORMATTER = new DecimalFormat(\"#,###\");\n\n    // Exponent should not use grouping, unlike basic\n    private static final DecimalFormat EXPONENT_FORMATTER = new DecimalFormat(\"#\");\n    private static final DecimalFormat SIGNIFICANT_FIGURE_FORMATTER = new DecimalFormat(\"0.######\");\n\n    /**\n     * Return the {@link #approximateProblemSizeLog} as a fixed point integer.\n     */\n    public long approximateProblemScaleLogAsFixedPointLong() {\n        return Math.round(approximateProblemSizeLog * MathUtils.LOG_PRECISION);\n    }\n\n    public String approximateProblemScaleAsFormattedString() {\n        return approximateProblemScaleAsFormattedString(Locale.getDefault());\n    }\n\n    String approximateProblemScaleAsFormattedString(Locale locale) {\n        if (Double.isNaN(approximateProblemSizeLog) || Double.isInfinite(approximateProblemSizeLog)) {\n            return \"0\";\n        }\n\n        if (approximateProblemSizeLog < 10) { // log_10(10_000_000_000) = 10\n            return \"%s\".formatted(format(Math.pow(10d, approximateProblemSizeLog), BASIC_FORMATTER, locale));\n        }\n        // The actual number will often be too large to fit in a double, so cannot use normal\n        // formatting.\n        // Separate the exponent into its integral and fractional parts\n        // Use the integral part as the power of 10, and the fractional part as the significant digits.\n        double exponentPart = Math.floor(approximateProblemSizeLog);\n        double remainderPartAsExponent = approximateProblemSizeLog - exponentPart;\n        double remainderPart = Math.pow(10, remainderPartAsExponent);\n        return \"%s  10^%s\".formatted(\n                format(remainderPart, SIGNIFICANT_FIGURE_FORMATTER, locale),\n                format(exponentPart, EXPONENT_FORMATTER, locale));\n    }",
    "func": "    private static @NonNull String format(double number, @NonNull DecimalFormat decimalFormat, @NonNull Locale locale) {\n        if (locale.equals(FORMATTER_LOCALE)) {\n            return decimalFormat.format(number);\n        }\n        try { // Slow path for corner cases where input locale doesn't match the default locale.\n            decimalFormat.setDecimalFormatSymbols(DecimalFormatSymbols.getInstance(locale));\n            return decimalFormat.format(number);\n        } finally {\n            decimalFormat.setDecimalFormatSymbols(DecimalFormatSymbols.getInstance(FORMATTER_LOCALE));\n        }\n    }",
    "comment": "    /**\n     * In order for tests to work currently regardless of the default system locale,\n     * we need to set the locale to a known value before running the tests.\n     * And because the {@link DecimalFormat} instances are initialized statically for reasons of performance,\n     * we cannot expect them to be in the locale that the test expects them to be in.\n     * This method exists to allow for an override.\n     *\n     * @return the given decimalFormat with the given locale\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/api/solver/SolverManagerTest.java::assertSolveWithConsumer core/src/test/java/ai/timefold/solver/core/api/solver/SolverManagerTest.java::assertSolveWithoutConsumer core/src/test/java/ai/timefold/solver/core/api/solver/SolverManagerTest.java::submitMoreProblemsThanCpus_allGetSolved",
    "test_class": "core/src/test/java/ai/timefold/solver/core/api/solver/SolverManagerTest.java::SolverManagerTest",
    "func_start": 72,
    "func_end": 82,
    "body_len": 10,
    "instruction": "Write a function `format` that takes a double `number`, a non-null `DecimalFormat` object, and a non-null `Locale` object, and returns a formatted string representation of the number using the given DecimalFormat, but temporarily adjusts the DecimalFormat's symbols to match the provided locale if it differs from a predefined constant `FORMATTER_LOCALE`. The function must ensure that after formatting, the DecimalFormat is restored to use `FORMATTER_LOCALE` symbols, even if an exception occurs during formatting, to maintain consistency across subsequent calls, especially in test environments where locale independence is required.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/api/solver/ProblemSizeStatistics.java::format",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/api/solver/SolverManagerTest.java::assertSolveWithConsumer",
      "core/src/test/java/ai/timefold/solver/core/api/solver/SolverManagerTest.java::assertSolveWithoutConsumer",
      "core/src/test/java/ai/timefold/solver/core/api/solver/SolverManagerTest.java::submitMoreProblemsThanCpus_allGetSolved"
    ],
    "test_start": 934,
    "test_end": 944,
    "test_file": "core/src/test/java/ai/timefold/solver/core/api/solver/SolverManagerTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.api.solver.SolverManagerTest#submitMoreProblemsThanCpus_allGetSolved\" -Dspotless.skip=true",
    "test_code": "    @Test\n    @Timeout(60)\n    void submitMoreProblemsThanCpus_allGetSolved() throws InterruptedException, ExecutionException {\n        // Use twice the amount of problems than available processors.\n        var problemCount = Runtime.getRuntime().availableProcessors() * 2;\n        try (var solverManager = createSolverManagerTestableByDifferentConsumers()) {\n            assertSolveWithoutConsumer(problemCount, solverManager);\n            assertSolveWithConsumer(problemCount, solverManager, true);\n            assertSolveWithConsumer(problemCount, solverManager, false);\n        }\n    }"
  },
  {
    "task-id": "timefold-solver-core/src/main/java/ai/timefold/solver/core/config/util/ConfigUtils.java-meldProperty",
    "project": "timefold-solver",
    "file_path": "core/src/main/java/ai/timefold/solver/core/config/util/ConfigUtils.java",
    "func_name": "meldProperty",
    "context": "package ai.timefold.solver.core.config.util;\n\nimport static ai.timefold.solver.core.impl.domain.common.accessor.MemberAccessorFactory.MemberAccessorType.FIELD_OR_READ_METHOD;\nimport static ai.timefold.solver.core.impl.domain.solution.cloner.DeepCloningUtils.IMMUTABLE_CLASSES;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.AnnotatedElement;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Modifier;\nimport java.lang.reflect.ParameterizedType;\nimport java.lang.reflect.Type;\nimport java.lang.reflect.WildcardType;\nimport java.math.BigDecimal;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.EnumSet;\nimport java.util.LinkedHashMap;\nimport java.util.LinkedHashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.function.Supplier;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport ai.timefold.solver.core.api.domain.common.DomainAccessType;\nimport ai.timefold.solver.core.api.domain.lookup.PlanningId;\nimport ai.timefold.solver.core.config.AbstractConfig;\nimport ai.timefold.solver.core.impl.domain.common.AlphabeticMemberComparator;\nimport ai.timefold.solver.core.impl.domain.common.ReflectionHelper;\nimport ai.timefold.solver.core.impl.domain.common.accessor.MemberAccessor;\nimport ai.timefold.solver.core.impl.domain.common.accessor.MemberAccessorFactory;\n\nimport org.jspecify.annotations.NonNull;\nimport org.jspecify.annotations.Nullable;\n\npublic class ConfigUtils {\n\n    private static final AlphabeticMemberComparator alphabeticMemberComparator = new AlphabeticMemberComparator();\n\n    /**\n     * Create a new instance of clazz from a config's property.\n     * <p>\n     * If the instantiation fails, the simple class name of {@code configBean} will be used as the owner of\n     * {@code propertyName}.\n     * <p>\n     * Intended usage:\n     *\n     * <pre>\n     * selectionFilter = ConfigUtils.newInstance(config, \"filterClass\", config.getFilterClass());\n     * </pre>\n     *\n     * @param configBean the bean holding the {@code clazz} to be instantiated\n     * @param propertyName {@code configBean}'s property holding {@code clazz}\n     * @param clazz {@code Class} representation of the type {@code T}\n     * @param <T> the new instance type\n     * @return new instance of clazz\n     */\n    public static <T> @NonNull T newInstance(@Nullable Object configBean, @NonNull String propertyName,\n            @NonNull Class<T> clazz) {\n        return newInstance(() -> (configBean == null ? \"?\" : configBean.getClass().getSimpleName()), propertyName, clazz);\n    }\n\n    /**\n     * Create a new instance of clazz from a general source.\n     * <p>\n     * If the instantiation fails, the result of {@code ownerDescriptor} will be used to describe the owner of\n     * {@code propertyName}.\n     *\n     * @param ownerDescriptor describes the owner of {@code propertyName}\n     * @param propertyName property holding the {@code clazz}\n     * @param clazz {@code Class} representation of the type {@code T}\n     * @param <T> the new instance type\n     * @return new instance of clazz\n     */\n    public static <T> @NonNull T newInstance(@NonNull Supplier<String> ownerDescriptor, @NonNull String propertyName,\n            @NonNull Class<T> clazz) {\n        try {\n            return clazz.getDeclaredConstructor().newInstance();\n        } catch (NoSuchMethodException | InstantiationException | IllegalAccessException | InvocationTargetException e) {\n            // Inner classes include local, anonymous and non-static member classes\n            throw new IllegalArgumentException(\"The %s's %s (%s) does not have a public no-arg constructor%s\"\n                    .formatted(ownerDescriptor.get(), propertyName, clazz.getName(),\n                            ((clazz.isLocalClass() || clazz.isAnonymousClass() || clazz.isMemberClass())\n                                    && !Modifier.isStatic(clazz.getModifiers()) ? \" because it is an inner class.\" : \".\")),\n                    e);\n        }\n    }\n\n    public static void applyCustomProperties(@NonNull Object bean, @NonNull String beanClassPropertyName,\n            @Nullable Map<@NonNull String, @NonNull String> customProperties, @NonNull String customPropertiesPropertyName) {\n        if (customProperties == null) {\n            return;\n        }\n        var beanClass = bean.getClass();\n        customProperties.forEach((propertyName, valueString) -> {\n            var setterMethod = ReflectionHelper.getSetterMethod(beanClass, propertyName);\n            if (setterMethod == null) {\n                throw new IllegalStateException(\n                        \"\"\"\n                                The custom property %s (%s) in the %s cannot be set on the %s (%s) because that class has no public setter for that property.\n                                Maybe add a public setter for that custom property (%s) on that class (%s).\n                                Maybe don't configure that custom property %s (%s) in the %s.\"\"\"\n                                .formatted(propertyName, valueString, customPropertiesPropertyName, beanClassPropertyName,\n                                        beanClass, propertyName, beanClass.getSimpleName(), propertyName, valueString,\n                                        customPropertiesPropertyName));\n            }\n            var propertyType = setterMethod.getParameterTypes()[0];\n            Object typedValue;\n            try {\n                if (propertyType.equals(String.class)) {\n                    typedValue = valueString;\n                } else if (propertyType.equals(Boolean.TYPE) || propertyType.equals(Boolean.class)) {\n                    typedValue = Boolean.parseBoolean(valueString);\n                } else if (propertyType.equals(Integer.TYPE) || propertyType.equals(Integer.class)) {\n                    typedValue = Integer.parseInt(valueString);\n                } else if (propertyType.equals(Long.TYPE) || propertyType.equals(Long.class)) {\n                    typedValue = Long.parseLong(valueString);\n                } else if (propertyType.equals(Float.TYPE) || propertyType.equals(Float.class)) {\n                    typedValue = Float.parseFloat(valueString);\n                } else if (propertyType.equals(Double.TYPE) || propertyType.equals(Double.class)) {\n                    typedValue = Double.parseDouble(valueString);\n                } else if (propertyType.equals(BigDecimal.class)) {\n                    typedValue = new BigDecimal(valueString);\n                } else if (propertyType.isEnum()) {\n                    typedValue = Enum.valueOf((Class<? extends Enum>) propertyType, valueString);\n                } else {\n                    throw new IllegalStateException(\n                            \"The custom property %s (%s) in the %s has an unsupported propertyType (%s) for value (%s).\"\n                                    .formatted(propertyName, valueString, customPropertiesPropertyName, propertyType,\n                                            valueString));\n                }\n            } catch (NumberFormatException e) {\n                throw new IllegalStateException(\n                        \"The custom property %s (%s) in the %s cannot be parsed to the propertyType (%s) of the setterMethod (%s).\"\n                                .formatted(propertyName, valueString, customPropertiesPropertyName, propertyType,\n                                        setterMethod));\n            }\n            try {\n                setterMethod.invoke(bean, typedValue);\n            } catch (IllegalAccessException e) {\n                throw new IllegalStateException(\n                        \"The custom property %s (%s) in the %s has a setterMethod (%s) on the beanClass (%s) that cannot be called for the typedValue (%s).\"\n                                .formatted(propertyName, valueString, customPropertiesPropertyName, setterMethod, beanClass,\n                                        typedValue),\n                        e);\n            } catch (InvocationTargetException e) {\n                throw new IllegalStateException(\n                        \"The custom property %s (%s) in the %s has a setterMethod (%s) on the beanClass (%s) that throws an exception for the typedValue (%s).\"\n                                .formatted(propertyName, valueString, customPropertiesPropertyName, setterMethod, beanClass,\n                                        typedValue),\n                        e.getCause());\n            }\n        });\n    }\n\n    public static <Config_ extends AbstractConfig<Config_>> @Nullable Config_ inheritConfig(@Nullable Config_ original,\n            @Nullable Config_ inherited) {\n        if (inherited != null) {\n            if (original == null) {\n                original = inherited.copyConfig();\n            } else {\n                original.inherit(inherited);\n            }\n        }\n        return original;\n    }\n\n    public static <Config_ extends AbstractConfig<Config_>> @Nullable List<Config_> inheritMergeableListConfig(\n            @Nullable List<Config_> originalList, @Nullable List<Config_> inheritedList) {\n        if (inheritedList != null) {\n            List<Config_> mergedList = new ArrayList<>(inheritedList.size()\n                    + (originalList == null ? 0 : originalList.size()));\n            // The inheritedList should be before the originalList\n            for (var inherited : inheritedList) {\n                var copy = inherited.copyConfig();\n                mergedList.add(copy);\n            }\n            if (originalList != null) {\n                mergedList.addAll(originalList);\n            }\n            originalList = mergedList;\n        }\n        return originalList;\n    }\n\n    public static <T> @Nullable T inheritOverwritableProperty(@Nullable T original, @Nullable T inherited) {\n        if (original != null) {\n            // Original overwrites inherited\n            return original;\n        } else {\n            return inherited;\n        }\n    }\n\n    public static <T> @Nullable List<T> inheritMergeableListProperty(@Nullable List<T> originalList,\n            @Nullable List<T> inheritedList) {\n        if (inheritedList == null) {\n            return originalList;\n        } else if (originalList == null) {\n            // Shallow clone due to modifications after calling inherit\n            return new ArrayList<>(inheritedList);\n        } else {\n            // The inheritedList should be before the originalList\n            List<T> mergedList = new ArrayList<>(inheritedList);\n            mergedList.addAll(originalList);\n            return mergedList;\n        }\n    }\n\n    public static <E extends Enum<E>> @Nullable Set<E> inheritMergeableEnumSetProperty(@Nullable Set<E> originalSet,\n            @Nullable Set<E> inheritedSet) {\n        if (inheritedSet == null) {\n            return originalSet;\n        } else if (originalSet == null) {\n            return EnumSet.copyOf(inheritedSet);\n        } else {\n            var newSet = EnumSet.copyOf(originalSet);\n            newSet.addAll(inheritedSet);\n            return newSet;\n        }\n    }\n\n    public static <T> @Nullable List<T> inheritUniqueMergeableListProperty(@Nullable List<T> originalList,\n            @Nullable List<T> inheritedList) {\n        if (inheritedList == null) {\n            return originalList;\n        } else if (originalList == null) {\n            // Shallow clone due to modifications after calling inherit\n            return new ArrayList<>(inheritedList);\n        } else {\n            // The inheritedMap should be before the originalMap\n            Set<T> mergedSet = new LinkedHashSet<>(inheritedList);\n            mergedSet.addAll(originalList);\n            return new ArrayList<>(mergedSet);\n        }\n    }\n\n    public static <K, T> @Nullable Map<K, T> inheritMergeableMapProperty(@Nullable Map<K, T> originalMap,\n            @Nullable Map<K, T> inheritedMap) {\n        if (inheritedMap == null) {\n            return originalMap;\n        } else if (originalMap == null) {\n            return inheritedMap;\n        } else {\n            Map<K, T> mergedMap = new LinkedHashMap<>(inheritedMap);\n            mergedMap.putAll(originalMap);\n            return mergedMap;\n        }\n    }\n\n    public static <T> @Nullable T mergeProperty(@Nullable T a, @Nullable T b) {\n        return Objects.equals(a, b) ? a : null;\n    }",
    "func": "    public static <T> @Nullable T meldProperty(@Nullable T a, @Nullable T b) {\n        if (a == null && b == null) {\n            return null;\n        }\n        if (a == null) {\n            return b;\n        }\n        if (b == null) {\n            return a;\n        }\n        return mergeProperty(a, b);\n    }",
    "comment": "    /**\n     * A relaxed version of {@link #mergeProperty(Object, Object)}. Used primarily for merging failed benchmarks,\n     * where a property remains the same over benchmark runs (for example: dataset problem size), but the property in\n     * the failed benchmark isn't initialized, therefore null. When merging, we can still use the correctly initialized\n     * property of the benchmark that didn't fail.\n     * <p>\n     * Null-handling:\n     * <ul>\n     * <li>if <strong>both</strong> properties <strong>are null</strong>, returns null</li>\n     * <li>if <strong>only one</strong> of the properties <strong>is not null</strong>, returns that property</li>\n     * <li>if <strong>both</strong> properties <strong>are not null</strong>, returns\n     * {@link #mergeProperty(Object, Object)}</li>\n     * </ul>\n     *\n     * @see #mergeProperty(Object, Object)\n     * @param a property {@code a}\n     * @param b property {@code b}\n     * @param <T> the type of property {@code a} and {@code b}\n     * @return sometimes null\n     */",
    "test_funcs": "core/src/test/java/ai/timefold/solver/core/config/util/ConfigUtilsTest.java::meldProperty",
    "test_class": "core/src/test/java/ai/timefold/solver/core/config/util/ConfigUtilsTest.java::ConfigUtilsTest",
    "func_start": 284,
    "func_end": 295,
    "body_len": 11,
    "instruction": "Write a function `meldProperty` that takes two nullable objects of the same type and merges them according to relaxed null-handling rules: if both are null, return null; if only one is non-null, return the non-null one; if both are non-null, delegate to another method `mergeProperty` to perform the actual merge. The function should be generic to work with any type and include proper nullability annotations to reflect that the result may be null.",
    "func_name_with_file_path": "core/src/main/java/ai/timefold/solver/core/config/util/ConfigUtils.java::meldProperty",
    "test_funcs_split": [
      "core/src/test/java/ai/timefold/solver/core/config/util/ConfigUtilsTest.java::meldProperty"
    ],
    "test_start": 31,
    "test_end": 37,
    "test_file": "core/src/test/java/ai/timefold/solver/core/config/util/ConfigUtilsTest.java",
    "test_instruction": "mvn test -pl core -Dtest=\"ai.timefold.solver.core.config.util.ConfigUtilsTest#meldProperty\" -Dspotless.skip=true",
    "test_code": "    @Test\n    void meldProperty() {\n        assertThat(ConfigUtils.<Integer> meldProperty(null, null)).isNull();\n        assertThat(ConfigUtils.<Integer> meldProperty(1, null)).isEqualTo(1);\n        assertThat(ConfigUtils.<Integer> meldProperty(null, 10)).isEqualTo(10);\n        assertThat(ConfigUtils.<Integer> meldProperty(1, 10)).isEqualTo(ConfigUtils.mergeProperty(1, 10));\n    }"
  }
]