from ..agent.coder import CoderAgent,CodegenEvaluatorAgent
from ..data import BigCodeBenchReader,AutoCodeBenchReader,MTDataManager,MultiTurnDataInstance,PerTurnDataInstance
from ..data.log_manager import LogManager,LogItem,InteractionItem
from .sandbox_client import BigCodeSandboxClient,AutoCodeBenchSandboxClient, MrgBenchSandboxClient

from typing import Annotated,TypedDict,List,Dict,Any,Literal,Optional
from langgraph.graph import StateGraph,START,END
from langgraph.graph.state import CompiledStateGraph
from langgraph.types import Command,interrupt
from langchain_core.runnables import Runnable
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langgraph.graph.message import add_messages
from pydantic import BaseModel,Field
from operator import add
from pathlib import Path
from datetime import datetime

class CodegenState(TypedDict):
    """ä»£ç ç”Ÿæˆç›¸å…³çŠ¶æ€"""
    # input state
    ## æ•°æ®
    hash_id: str
    turn_datas: List[PerTurnDataInstance]
    total_turn: int
    metadata: Dict[str,Any]
    mt_id: Optional[int]
    # output state
    ## coder
    code: str
    test: str
    ## eval
    feedback: str
    # log state
    interactions: Annotated[List[InteractionItem],add]
    fail_reason: str = None
    total_tokens: int = 0
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    # config state
    iter_number: int
    current_codegen_turn: int
    # human in the loop
    human_feedback: Dict = None

class CodegenModule:
    def __init__(self,
        dataset:Optional[str] = None,
        exclude_module:List[Literal["evaluator","distinctiveness"]] = [],
        db_path: Optional[str] = None,
        log_db_path: Optional[str] = None,
        max_iter_number: int = 10,
        auto_skip: bool = False
    ) -> None:
        """
        åˆå§‹åŒ–ä»£ç ç”Ÿæˆæ¨¡å—
        å‚æ•°ï¼š
            - dataset: æ•°æ®é›†åç§°
            - exclude_module: æ¶ˆèå®éªŒç”¨ï¼Œç”¨äºå»é™¤æŸäº›èŠ‚ç‚¹
        """
        self.dataset = dataset
        if dataset not in ['bigcodebench','autocodebench','mrgbench','deveval']:
            raise ValueError("Dataset not support.")
        
        # æ•°æ®ç®¡ç†æ¨¡å—: å«æŒ‡ä»¤æ•°æ®åŠ è½½å’Œæœ€ç»ˆæ•°æ®ç®¡ç†
        self.data_manager = MTDataManager(db_path=db_path)

        # æ—¥å¿—ç®¡ç†å™¨
        self.log_manager = LogManager(db_path=log_db_path)

        # Agentç®¡ç†
        if self.dataset == 'bigcodebench':
            self.coder_llm = CoderAgent(name="coder",dataset=self.dataset,system_prompt_name="coder")
        elif self.dataset == 'autocodebench':
            self.coder_llm = CoderAgent(name="coder_autocodebench",dataset=self.dataset,system_prompt_name="coder_autocodebench")
        elif self.dataset == 'mrgbench':
            self.coder_llm = CoderAgent(name="coder_mrgbench",dataset=self.dataset,system_prompt_name="coder_mrgbench")
        
        self.evaluator_llm = CodegenEvaluatorAgent()

        # æ²™ç›’å®¢æˆ·ç«¯
        if dataset == 'bigcodebench':
            self.sandbox_client = BigCodeSandboxClient()
        elif dataset == 'autocodebench':
            self.sandbox_client = AutoCodeBenchSandboxClient()
        elif dataset == 'mrgbench':
            self.sandbox_client = MrgBenchSandboxClient()

        # graph
        self.graph = self._build_codegen_graph()

        # é…ç½®å‚æ•°
        self.max_iter_number = max_iter_number
        self.auto_skip = auto_skip

        # æ¶ˆèå®éªŒ
        self.exclude_module = exclude_module
    
    def _coder_node(self,state:CodegenState
    ) -> Command[Literal["success_handle",
                         "failure_handle",
                         "check_correctness",
                         "__end__"
                         ]]:
        """ CoderèŠ‚ç‚¹ï¼Œä¸“æ³¨ä»£ç å’Œæµ‹è¯•ç”Ÿæˆ """

        iter_number = state.get("iter_number")
        current_codegen_turn = state.get("current_codegen_turn")
        print(f"è¿›å…¥CoderèŠ‚ç‚¹,è¿­ä»£æ¬¡æ•°:{iter_number},å½“å‰è½®æ¬¡:{current_codegen_turn}")
        if iter_number >= self.max_iter_number:
            print("âš ï¸è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ­£åœ¨è·³è½¬åˆ°ENDèŠ‚ç‚¹")
            return Command(
                goto="failure_handle",
                update={
                    "fail_reason": "è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°",
                }
            )
        if current_codegen_turn > state.get("total_turn"):
            print("âš ï¸è­¦å‘Š: å½“å‰è½®æ¬¡è¶…è¿‡æ€»å…±è½®æ¬¡ï¼Œä½†ä»æ—§è¿›å…¥CoderèŠ‚ç‚¹ï¼Œè¯·æ£€æŸ¥ä»£ç é€»è¾‘")
            print("å°†ç›´æ¥è·³åˆ°ENDèŠ‚ç‚¹ï¼Œä¸æ¥å—æœ¬æ¡æ ·æœ¬")
            return Command(
                goto="__end__"
            )

        # 1. æ„é€ æç¤º
        current_instruction = state.get("turn_datas")[current_codegen_turn-1].instruction
        feedback = state.get("feedback")
        if current_codegen_turn > 1:
            previous_solution = state.get("turn_datas")[current_codegen_turn-2].solution
        else:
            previous_solution = "None. Because the current round is the first round."
        
        if self.dataset == 'mrgbench':
            original_code = state.get("metadata").get("func","")
            original_test = state.get("metadata").get("test_code","")

            if not feedback:
                user_prompt = f"""--- Current Turn ---
{current_codegen_turn}
--- Current Instruction ---
{current_instruction}
--- Original Repository Code ---
```java
{original_code}
```
--- Original Test Code ---
```java
{original_test}
```
--- Previous Round's Solution (if any) ---
```java
{previous_solution}
```
"""
            else:
                user_prompt = f"""--- Current Turn ---
{current_codegen_turn}
--- Current Instruction ---
{current_instruction}
--- Original Repository Code ---
```java
{original_code}
```
--- Original Test Code ---
```java
{original_test}
```
--- Previous Round's Solution (if any) ---
```java
{previous_solution}
```
--- The current round of code and test that you provided to me in the last interaction ---
Code you provided to me in the last interaction:
```java
{state.get("code")}
```
Test you provided to me in the last interaction:
```java
{state.get("test")}
```
--- Feedback ---
{feedback}
"""
            
        elif not feedback:
            # æ— feedback
            user_prompt = f"""--- Current Round ---
{current_codegen_turn}
--- Current Instruction ---
{current_instruction}
--- Previous Round's Solution (if any) ---
```
{previous_solution}
```
"""
        else:
           # æœ‰feedback  
           user_prompt = f"""--- Current Round ---
{current_codegen_turn}
--- Current Instruction ---
{current_instruction}
--- Previous Round's Solution (if any) ---
```
{previous_solution}
```
--- The current round of code and test that you provided to me in the last interaction ---
Code you provided to me in the last interaction: 
```
{state.get("code")}
```
Test you provided to me in the last interaction:
```
{state.get("test")}
```
--- Feedback ---
{feedback}
""" 
        
        # 2. æ‰§è¡Œç”Ÿæˆ
        codegen_result,response = self.coder_llm.single_turn_chat_with_structure_output(user_prompt)
        # è®°å½•æ—¥å¿—æ•°æ®
        interaction_log = self._generation_interaction_log("coder",response.response_metadata,codegen_result)
        token_infos = self._get_updated_usage(state,response)

        # 3. è¿›å…¥æ­£ç¡®æ€§æµ‹è¯•
        return Command(
            goto="check_correctness",
            update={
                # ç”Ÿæˆç»“æœ
                'code': codegen_result.solution,
                'test': codegen_result.test,
                # å‚æ•°
                'iter_number': state.get('iter_number')+1,
                # æ—¥å¿—
                'interactions': [interaction_log],
                **token_infos
            }
        )
    
    def _check_correctness_node(self,state:CodegenState) -> Command[Literal["coder","check_distinctiveness"]]:
        """æ ¡éªŒç”Ÿæˆä»£ç æ­£ç¡®æ€§çš„èŠ‚ç‚¹"""
        print("è¿›å…¥check_correctnessèŠ‚ç‚¹")
        code = state.get("code")
        test = state.get("test")
        # æ²™ç›’æ‰§è¡Œ
        if self.dataset == 'mrgbench':
            exec_result = self.sandbox_client.execute_code_with_test(code,test,state.get("metadata"))
        else:
            exec_result = self.sandbox_client.execute_code_with_test(code,test)
        exec_status,exec_detail = exec_result["status"],exec_result["detail"]

        # è®°å½•æ—¥å¿—
        interaction_log = self._generation_interaction_log("check_correctness",None,exec_result)

        if exec_status == 'pass':
            print("æ­£ç¡®æ€§æ ¡éªŒé€šè¿‡")
            # åˆ¤æ–­è½®æ¬¡ï¼Œåªæœ‰è½®æ¬¡å¤§äº1æ‰éœ€è¦è¿›å…¥åŒºåˆ†åº¦æµ‹è¯•
            current_codegen_turn = state.get("current_codegen_turn")
            if current_codegen_turn > 1:
                # è¿›å…¥åŒºåˆ†åº¦æ£€éªŒ
                return Command(
                    goto="check_distinctiveness",
                    update={
                        "interactions": [interaction_log]
                    }
                )
            else:
                print("å½“å‰è½®æ¬¡ä¸ºç¬¬ä¸€è½®ï¼Œç›´æ¥è¿›å…¥Evaluator")
                return Command(
                    goto="codegen_evaluator",
                    update={
                        "interactions": [interaction_log]
                    }
                )

        else:
            print(f"æ­£ç¡®æ€§æ ¡éªŒå¤±è´¥ï¼Œæ­£åœ¨æ„å»ºfeedbackå¹¶è¿”å›coder.\næ‰§è¡Œè¾“å‡º:{exec_detail}")
            # å›åˆ°coderï¼Œæä¾›åé¦ˆ
            feedback = f"""Based on execution environment, the solution code in current round fails the test case you haved generated.
Status: {exec_status}
Stdout: {exec_detail}
"""
            return Command(
                goto="coder",
                update={
                    "feedback": feedback,
                    "interactions": [interaction_log]
                }
            )
        
    def _check_distinctiveness_node(self,state:CodegenState) -> Command[Literal["coder","codegen_evaluator"]]:
        """åŒºåˆ†åº¦æ ¡éªŒ"""
        print("è¿›å…¥check_distinctivenessèŠ‚ç‚¹")
        # å¦‚æœå¼€å¯äº†æ¶ˆèå®éªŒï¼Œç›´æ¥è¿›å…¥evaluator
        if "distinctiveness" in self.exclude_module:
            return Command(
                goto="codegen_evaluator",
            )
        test = state.get("test")
        current_codegen_turn = state.get("current_codegen_turn")
        if current_codegen_turn > 1:
            previous_solution = state.get("turn_datas")[current_codegen_turn-2].solution
        # æ²™ç›’æ‰§è¡Œ
        if self.dataset == 'mrgbench':
            exec_result = self.sandbox_client.execute_code_with_test(previous_solution,test,state.get("metadata"))
        else:
            exec_result = self.sandbox_client.execute_code_with_test(previous_solution,test)
        exec_status,exec_detail = exec_result["status"],exec_result["detail"]

        # è®°å½•æ—¥å¿—
        interaction_log = self._generation_interaction_log("check_distinctiveness",None,exec_result)

        if exec_status == 'fail':
            print("åŒºåˆ†åº¦æ ¡éªŒé€šè¿‡")
            # è¿›å…¥å¯¹é½æ™ºèƒ½ä½“
            return Command(
                goto="codegen_evaluator",
                update={
                    "interactions": [interaction_log]
                }
            )
        else:
            print("åŒºåˆ†åº¦æ ¡éªŒæœªé€šè¿‡ï¼Œå‡†å¤‡æä¾›feedbackå¹¶è¿”å›coder")
            # å›åˆ°coderï¼Œæä¾›åé¦ˆ
            feedback = f"""Based on the execution environment, although the current round of code you generated can pass the test cases, the previous round of code also passed the test cases. This means that the discriminability of the test cases is insufficient.
"""
            return Command(
                goto="coder",
                update={
                    "feedback": feedback,
                    "interactions": [interaction_log]
                }
            )

    def _codegen_evaluator_node(self,state:CodegenState) -> Command[Literal["coder","human_in_the_loop","success_handle"]]:
        """æŒ‡ä»¤-æµ‹è¯•å¯¹é½æ™ºèƒ½ä½“"""
        print("è¿›å…¥evaluatorèŠ‚ç‚¹")

        # å¯é€‰è·³è¿‡è¿™ä¸ªèŠ‚ç‚¹
        if "evaluator" in self.exclude_module:
            current_codegen_turn = state.get("current_codegen_turn")
            # 1. è®°å½•æœ¬è½®æ¬¡ç”Ÿæˆçš„æ•°æ®
            turn_datas = state.get("turn_datas")
            turn_datas[current_codegen_turn-1].solution = state.get("code")
            turn_datas[current_codegen_turn-1].test = state.get("test") 
            # 2. æ£€æŸ¥è½®æ¬¡ä¿¡æ¯ï¼Œå†³å®šè·³è½¬
            if current_codegen_turn == state.get("total_turn"):
                print("å·²å®Œæˆæ‰€æœ‰è½®æ¬¡ï¼Œå‡†å¤‡å‰å¾€success_handle")
                # å®Œæˆæ‰€æœ‰è½®æ¬¡
                return Command(
                    goto="success_handle",
                    update={
                        # æ›´æ–°ç”Ÿæˆæ•°æ®
                        "turn_datas": turn_datas,
                    }
                )
            elif current_codegen_turn < state.get("total_turn"):
                print(f"å·²å®Œæˆå½“å‰è½®æ¬¡[{current_codegen_turn}/{state.get("total_turn")}]ï¼Œå‡†å¤‡å‰å¾€coderèŠ‚ç‚¹")
                # è¿›å…¥ä¸‹ä¸€è½®çš„ç”Ÿæˆ
                return Command(
                    goto="coder",
                    update={
                        # æ›´æ–°ç”Ÿæˆæ•°æ®
                        "turn_datas": turn_datas,
                        # æ¸…ç©ºfeedback
                        "feedback": None,
                        # å‚æ•°
                        "iter_number": 0,
                        "current_codegen_turn": current_codegen_turn + 1,
                    }
                )

        current_codegen_turn = state.get("current_codegen_turn")
        current_instruction = state.get("turn_datas")[current_codegen_turn-1].instruction
        current_test = state.get("test")
        current_code = state.get("code")
        if current_codegen_turn > 1:
            previous_turn_datas = state.get("turn_datas")[:current_codegen_turn-1]
            previous_instruction = '\n'.join([turn.instruction for turn in previous_turn_datas])
        else:
            previous_instruction = "None, becauses this is the first turn."
        
        user_prompt = f"""--- Previous Instructions ---
{previous_instruction}
--- Current Instruction ---
{current_instruction}
--- SOLUTION ---
{current_code}
--- TEST FUNCTION ---
{current_test}
"""
        # æ‰§è¡Œè¯„ä¼°
        eval_result,response = self.evaluator_llm.single_turn_chat_with_structure_output(user_prompt)
        eval_decision, eval_feedback = eval_result.decision, eval_result.feedback
        # è®°å½•æ—¥å¿—
        interaction_log = self._generation_interaction_log("codegen_evaluator",response.response_metadata,eval_result)
        token_infos = self._get_updated_usage(state,response)

        if eval_decision == 'RETAIN':
            print(f"decision == {eval_decision}")
            # è½®æ¬¡æˆåŠŸ
            # 1. è®°å½•æœ¬è½®æ¬¡ç”Ÿæˆçš„æ•°æ®
            turn_datas = state.get("turn_datas")
            turn_datas[current_codegen_turn-1].solution = state.get("code")
            turn_datas[current_codegen_turn-1].test = state.get("test")
            # 2. æ£€æŸ¥è½®æ¬¡ä¿¡æ¯ï¼Œå†³å®šè·³è½¬
            if current_codegen_turn == state.get("total_turn"):
                print("å·²å®Œæˆæ‰€æœ‰è½®æ¬¡ï¼Œå‡†å¤‡å‰å¾€success_handle")
                # å®Œæˆæ‰€æœ‰è½®æ¬¡
                return Command(
                    goto="success_handle",
                    update={
                        # æ›´æ–°ç”Ÿæˆæ•°æ®
                        "turn_datas": turn_datas,
                        # eval
                        "feedback": eval_feedback,
                        # æ—¥å¿—
                        "interactions": [interaction_log],
                        **token_infos,
                    }
                )
            elif current_codegen_turn < state.get("total_turn"):
                print(f"å·²å®Œæˆå½“å‰è½®æ¬¡[{current_codegen_turn}/{state.get("total_turn")}]ï¼Œå‡†å¤‡å‰å¾€coderèŠ‚ç‚¹")
                # è¿›å…¥ä¸‹ä¸€è½®çš„ç”Ÿæˆ
                return Command(
                    goto="coder",
                    update={
                        # æ›´æ–°ç”Ÿæˆæ•°æ®
                        "turn_datas": turn_datas,
                        # æ¸…ç©ºfeedback
                        "feedback": None,
                        # å‚æ•°
                        "iter_number": 0,
                        "current_codegen_turn": current_codegen_turn + 1,
                        # æ—¥å¿—
                        "interactions": [interaction_log],
                        **token_infos,
                    }
                )
            else:
                print("âŒæ‰§è¡Œåˆ°é”™è¯¯åˆ†æ”¯: å½“å‰è½®æ¬¡ > total_turnï¼Œè¯·æ£€æŸ¥ä»£ç é€»è¾‘")
                return Command(
                    goto="__end__"
                )
        elif eval_decision == 'TEST_REFINE':
            print(f"evaluatorçŠ¶æ€{eval_decision}ï¼Œå‡†å¤‡è¿”å›coderèŠ‚ç‚¹.\nå…ˆå‰æµ‹è¯•:\n {current_test}\nå…ˆå‰ä»£ç :\n{current_code}\nåé¦ˆ:\n{eval_feedback}\nè§„åˆ™ç¬¦åˆæƒ…å†µ:\n{eval_result.rule_results}")
            # æµ‹è¯•æ”¹è¿›ï¼Œè¿”å›coderèŠ‚ç‚¹
            return Command(
                goto="coder",
                update={
                    "feedback": eval_feedback,
                    # æ—¥å¿—
                    "interactions": [interaction_log],
                    **token_infos,
                }
            )
        elif eval_decision == 'QUESTION_REFINE':
            print(f"evaluatorçŠ¶æ€{eval_decision}ï¼Œå‡†å¤‡è¿›å…¥äººç±»å¹²é¢„èŠ‚ç‚¹.\n ä»£ç :{current_code}\n æµ‹è¯•:{current_test}\n åé¦ˆ:\n{eval_feedback}, ")
            # æŒ‡ä»¤æ”¹è¿›ï¼Œè¿›å…¥HumanèŠ‚ç‚¹
            return Command(
                goto="human_in_the_loop",
                update={
                    "feedback": eval_feedback,
                    # æ—¥å¿—
                    "interactions": [interaction_log],
                    **token_infos,
                }
            )

    def _success_handle_node(self,state:CodegenState) -> Command[Literal["__end__"]]:
        """æ ·æœ¬ç”ŸæˆæˆåŠŸï¼Œæ›´æ–°æ•°æ®åº“"""
        new_mt_instance = MultiTurnDataInstance(
            hash_id=state.get("hash_id"),
            total_turn=state.get("total_turn"),
            turn_datas=state.get("turn_datas"),
            metadata=state.get("metadata"),
            mt_id=state.get("mt_id")
        )
        
        # æ›´æ–°æ•°æ®åº“
        status = self.data_manager.update(new_mt_instance)
        if status:
            print(f"ä¸€æ¡æ•°æ®ç”Ÿæˆå®Œæ¯•ï¼æ›´æ–°æ•°æ®åº“æˆåŠŸï¼Œæ•°æ®ID:{new_mt_instance.mt_id}")
        else:
            print("âŒæ›´æ–°æ•°æ®åº“å¤±è´¥,æ•°æ®ä¿¡æ¯å¦‚ä¸‹:")
            print(new_mt_instance)
        
        # æ›´æ–°æ—¥å¿—
        log = LogItem(
            status="success",
            task="codegen",
            fail_reason=None,
            mt_id=new_mt_instance.mt_id,
            hash_id=new_mt_instance.hash_id,
            source=new_mt_instance.metadata["source"],
            interactions=state.get("interactions"),
            total_prompt_tokens=state.get("total_prompt_tokens"),
            total_completion_tokens=state.get("total_completion_tokens"),
            total_tokens=state.get("total_tokens"),
            interaction_number=len(state.get("interactions")),
        )
        self.log_manager.add(log)
        print_info = f"""âœ…æˆåŠŸåˆ¶ä½œä¸€æ¡å¤šè½®æ•°æ®ï¼Œæ±‡æ€»ä¿¡æ¯å¦‚ä¸‹:
- æ•°æ®ID:{new_mt_instance.mt_id}, æŒ‡ä»¤è½®æ•°:{new_mt_instance.total_turn}
- Tokenæ€»æ¶ˆè€—:{log.total_tokens}, è¾“å…¥Tokenæ€»æ¶ˆè€—:{log.total_prompt_tokens}
- è¯„ä¼°è¿­ä»£æ¬¡æ•°: {state.get("iter_number")}, äº¤äº’æ¬¡æ•°:{log.interaction_number}"""
        print(print_info)

    def _failure_handle_node(self,state:CodegenState) -> Command[Literal["__end__"]]:
        """æ ·æœ¬ç”Ÿæˆå¤±è´¥ï¼Œè®°å½•æ—¥å¿—"""
        fail_reason = state.get("fail_reason")
        print(f"âš ï¸ID={state.get("mt_id")}è¿›å…¥å¤±è´¥èŠ‚ç‚¹,æ­£åœ¨è®°å½•æ—¥å¿—,å¤±è´¥åŸå› :{fail_reason}")
        log = LogItem(
            status="fail",
            task="codegen",
            fail_reason=fail_reason,
            mt_id=state.get("mt_id"),
            hash_id=state.get("hash_id"),
            source=state.get("metadata").get("source"),
            interactions=state.get("interactions"),
            total_prompt_tokens=state.get("total_prompt_tokens"),
            total_completion_tokens=state.get("total_completion_tokens"),
            total_tokens=state.get("total_tokens"),
            interaction_number=len(state.get("interactions"))
        )
        self.log_manager.add(log)

    def _human_in_the_loop_node(self,state:CodegenState)->Command[Literal["coder"]]:
        """æ›´æ–°æŒ‡ä»¤æ•°æ®ï¼Œäººç±»æ ‡æ³¨è€…å¯ä»¥å†³å®šæ˜¯ç›´æ¥é€šè¿‡ æˆ–è€… é‡æ–°è¿›å…¥coderèŠ‚ç‚¹"""
        # wait for human feedback
        human_feedback = interrupt({
            "current_codegen_turn": state.get("current_codegen_turn"),
            "current_instruction": state.get("turn_datas")[state.get("current_codegen_turn")-1].instruction,
            "feedback": state.get("feedback")
        })
        # æ›´æ–°æŒ‡ä»¤
        print(f"å¾—åˆ°ç”¨æˆ·åé¦ˆ: {human_feedback}")
        turn_datas = state.get("turn_datas")
        current_codegen_turn = state.get("current_codegen_turn")
        turn_datas[current_codegen_turn-1].instruction = human_feedback["new_instruction"]
        # æ—¥å¿—
        interaction_log = self._generation_interaction_log("human_in_the_loop",None,human_feedback)
        # æ­¤æ—¶åº”è¯¥å›åˆ°Evaluator
        return Command(
            goto="coder",
            update={
                "turn_datas": turn_datas,
                # ä¿å­˜äººç±»åé¦ˆï¼Œæ¸…ç©ºåŸæ¥çš„åé¦ˆ
                "human_feedback": human_feedback,
                "feedback": None,
                # æ—¥å¿—
                "interactions": [interaction_log]
            }
        )

    def _build_codegen_graph(self) -> CompiledStateGraph:
        graph_builder = StateGraph(CodegenState)
        # node
        graph_builder.add_node("coder",self._coder_node)
        graph_builder.add_node("codegen_evaluator",self._codegen_evaluator_node)
        graph_builder.add_node("check_correctness",self._check_correctness_node)
        graph_builder.add_node("check_distinctiveness",self._check_distinctiveness_node)
        graph_builder.add_node("success_handle",self._success_handle_node)
        graph_builder.add_node("failure_handle",self._failure_handle_node)
        graph_builder.add_node("human_in_the_loop",self._human_in_the_loop_node)
        # edge
        graph_builder.add_edge(START,"coder")
        graph_builder.add_edge("success_handle",END)
        graph_builder.add_edge("failure_handle",END)
        # checkpoint
        memory = InMemorySaver()
        return graph_builder.compile(checkpointer=memory)
    
    # def execute_codegen_loop_multi_thread(self) -> None:
    #     """è¯»å–æ•°æ®åº“ä¸­çš„æŒ‡ä»¤å¹¶å¼€å§‹è¿›è¡Œä»£ç å’Œæµ‹è¯•çš„å¾ªç¯ï¼ˆå¤šçº¿ç¨‹å¹¶å‘ç‰ˆæœ¬ï¼‰"""
    #     import threading
    #     from concurrent.futures import ThreadPoolExecutor, as_completed

    #     print("å‡†å¤‡æ‰§è¡Œcodegen_loopï¼ˆå¤šçº¿ç¨‹æ¨¡å¼ï¼‰")
        
    #     # è·å–æ•°æ®
    #     instances = self.data_manager.get_incomplete_instances(source=self.dataset)
    #     print(f"ä»æ•°æ®åº“ä¸­è·å–åˆ°æœªå®Œæˆçš„æ•°æ®: {len(instances)}æ¡.")

    #     # çº¿ç¨‹å®‰å…¨çš„å…±äº«ç»“æœæ”¶é›†ï¼ˆå¦‚æœéœ€è¦æ”¶é›†è¿”å›å€¼ï¼‰
    #     # è¿™é‡Œæˆ‘ä»¬åªæ˜¯å¤„ç†ï¼Œä¸éœ€è¦è¿”å›å¤ªå¤šä¸œè¥¿ï¼Œä½†å¯ä»¥è®°å½•å¤±è´¥/æˆåŠŸ
    #     success_count = 0
    #     failure_count = 0
    #     lock = threading.Lock()  # ç”¨äºä¿æŠ¤å…±äº«è®¡æ•°å™¨å’Œæ‰“å°

    #     def process_instance(instance) -> bool:
    #         nonlocal success_count, failure_count
    #         if instance.total_turn > 5:
    #             with lock:
    #                 print(f"è·³è¿‡: MT_ID={instance.mt_id}, total_turn > 5")
    #             return False

    #         graph_input_init: dict = {
    #             'mt_id': instance.mt_id,
    #             'hash_id': instance.hash_id,
    #             'turn_datas': instance.turn_datas,
    #             'metadata': instance.metadata,
    #             'total_turn': instance.total_turn,
    #             'iter_number': 0,
    #             'current_codegen_turn': 1,
    #             'fail_reason': None,
    #             'total_tokens': 0,
    #             'total_prompt_tokens': 0,
    #             'total_completion_tokens': 0,
    #             'interactions': []
    #         }

    #         thread_config = {"configurable": {"thread_id": f"instance_{instance.mt_id}"}, "recursion_limit": 80}
    #         graph_input = graph_input_init
    #         idx = 1
    #         try:
    #             with lock:
    #                 print(f"æ­£åœ¨å¤„ç†æ•°æ®: MT_ID={instance.mt_id}, TASK_ID={instance.metadata['task_id']}, ç¬¬{idx}æ¬¡è°ƒç”¨graph.")
    #             self.graph.invoke(graph_input, thread_config)
    #             with lock:
    #                 success_count += 1
    #             return True

    #         except Exception as e:
    #             with lock:
    #                 print(f"å‘ç”Ÿå¼‚å¸¸: {e}ï¼Œå·²è·³è¿‡ instance: MT_ID={instance.mt_id}, TASK_ID={instance.metadata['task_id']}")
    #                 failure_count += 1
    #             return False

    #     # å¹¶è¡Œæ‰§è¡Œï¼ˆå¯æ ¹æ® CPU/IO æƒ…å†µè°ƒæ•´ max_workersï¼‰
    #     max_workers = 10  # å»ºè®®æ ¹æ®ä½ çš„æ¨¡å‹æ¨ç†ååèƒ½åŠ›è°ƒæ•´ï¼ˆå¦‚ API é™é€Ÿåˆ™è°ƒå°ï¼‰
    #     with ThreadPoolExecutor(max_workers=max_workers) as executor:
    #         futures = [executor.submit(process_instance, instance) for instance in instances]
    #         # å¯é€‰ï¼šå®æ—¶æŸ¥çœ‹è¿›åº¦
    #         for future in as_completed(futures):
    #             try:
    #                 future.result()  # å¯æ•è·å¼‚å¸¸ï¼ˆå·²åœ¨å‡½æ•°å†…å¤„ç†ï¼‰
    #             except Exception as e:
    #                 with lock:
    #                     print(f"çº¿ç¨‹æ‰§è¡Œå‡ºé”™: {e}")

    #     # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå¯¼å‡ºç»“æœï¼ˆå¿…é¡»åœ¨ä¸»çº¿ç¨‹ï¼‰
    #     timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    #     output_path = (Path(__file__).parent / f"codegen_output_{timestamp_str}.jsonl").resolve()
    #     self.data_manager.export_jsonl(output_path)

    #     print(f"æ•´ä¸ªä»£ç ç”Ÿæˆè¿‡ç¨‹å®Œæ¯•ï¼ŒæˆåŠŸå¤„ç† {success_count} æ¡ï¼Œå¤±è´¥ {failure_count} æ¡ã€‚")
    #     print(f"å·²å¯¼å‡ºä¸º jsonl è¾“å‡ºåˆ°: {output_path}")

    def execute_codegen_loop(self) -> None:
        """è¯»å–æ•°æ®åº“ä¸­çš„æŒ‡ä»¤å¹¶å¼€å§‹è¿›è¡Œä»£ç å’Œæµ‹è¯•çš„å¾ªç¯"""
        print("å‡†å¤‡æ‰§è¡Œcodegen_loop")
        # è·å–æ•°æ®
        instances = self.data_manager.get_incomplete_instances(source=self.dataset)
        print(f"ä»æ•°æ®åº“ä¸­è·å–åˆ°æœªå®Œæˆçš„æ•°æ®: {len(instances)}æ¡.")

        for instance in instances:
            if instance.total_turn > 5:
                print("total_turn > 5ï¼Œå·²ç»è·³è¿‡æœ¬æ ·æœ¬")
                continue
            graph_input_init:CodegenState = {
                # è¾“å…¥æ•°æ®
                'mt_id': instance.mt_id,
                'hash_id': instance.hash_id,
                'turn_datas': instance.turn_datas,
                'metadata': instance.metadata,
                'total_turn': instance.total_turn,
                # é…ç½®å‚æ•°
                'iter_number': 0,
                'current_codegen_turn': 1,
                # æ—¥å¿—æ•°æ®
                'fail_reason': None,
                "total_tokens": 0,
                "total_prompt_tokens": 0,
                "total_completion_tokens": 0,
                'interactions': []
            }
            thread = {"configurable": {"thread_id": f"instance_{instance.mt_id}"},"recursion_limit":80}

            graph_input = graph_input_init
            idx = 1
            try:
                while True:
                    print(f"æ­£åœ¨å¤„ç†æ•°æ®: MT_ID={instance.mt_id}, ,ç¬¬{idx}æ¬¡è°ƒç”¨graph.")
                    graph_state = self.graph.invoke(graph_input,thread)
                    # æ£€æµ‹ä¸­æ–­
                    if self.graph.get_state(thread).interrupts:
                        print(f"æ£€æµ‹åˆ°ä¸­æ–­ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥âŒ›ï¸.")
                        # å­˜åœ¨ä¸­æ–­ï¼Œè·å–ç”¨æˆ·è¾“å…¥
                        human_feedback = self._get_human_input(graph_state)
                        # é‡æ–°æ‰§è¡Œ
                        graph_input = Command(resume={"new_instruction": human_feedback})
                        idx += 1
                    else:
                        break
            except Exception as e:
                import traceback
                traceback.print_exc()
                print(f"å‘ç”Ÿå¼‚å¸¸: {e}\n, å·²è·³è¿‡instance: MT_ID={instance.mt_id}")
                continue
        
        # æœ€ç»ˆè¾“å‡ºjsonl
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = (Path(__file__).parent / f"codegen_output_{timestamp_str}.jsonl").resolve()
        self.data_manager.export_jsonl(output_path)

        print(f"æ•´ä¸ªä»£ç ç”Ÿæˆè¿‡ç¨‹å®Œæ¯•ï¼Œå·²å¯¼å‡ºä¸ºjsonlè¾“å‡ºåˆ°: {output_path}")

    def _get_human_input(self, state:Dict) -> str:
        """
        è·å–äººç±»ç”¨æˆ·çš„æ–°æŒ‡ä»¤è¾“å…¥
        
        Args:
            interrupt_data: ä¸­æ–­äº‹ä»¶çš„æ•°æ®
            
        Returns:
            str: ç”¨æˆ·è¾“å…¥çš„æ–°æŒ‡ä»¤
        """
        print("\n" + "="*60)
        print("âš ï¸ codegen_evaluatorè®¤ä¸ºæŒ‡ä»¤å­˜åœ¨ä¸å¯¹é½æƒ…å†µï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†!")
        print("ğŸ¤– è¿›å…¥HUMAN-IN-THE-LOOP æ¨¡å¼")
        print("="*60)
        print(f"å½“å‰è½®æ¬¡: {state.get('current_codegen_turn')}")
        print(f"åŸå§‹æŒ‡ä»¤: {state.get("turn_datas")[state.get("current_codegen_turn")-1].instruction}")
        print(f"evaluatorçš„å»ºè®®: {state.get('feedback')}")
        print("\nè¯·æ ¹æ®è¯„ä¼°åé¦ˆï¼Œè¾“å…¥æ”¹è¿›åçš„æ–°æŒ‡ä»¤:")
        print("(è¾“å…¥ 'skip' è¡¨ç¤ºä¸å¤„ç†)")
        print("-"*60)
        
        while True:
            if self.auto_skip:
                print("â­ï¸ å·²å¯åŠ¨autoskipï¼Œè‡ªåŠ¨è·³è¿‡Human In the Loopã€‚è¿”å›åŸå§‹æŒ‡ä»¤")
                return state.get("turn_datas")[state.get("current_codegen_turn")-1].instruction
                
            user_input = input("è¯·è¾“å…¥æ–°æŒ‡ä»¤: ").strip()

            if user_input.lower() == 'skip':
                print("â­ï¸ å·²è·³è¿‡æœ¬å®ä¾‹ï¼Œè¿”å›åŸå§‹æŒ‡ä»¤")
                return state.get("turn_datas")[state.get("current_codegen_turn")-1].instruction
            elif user_input:
                print(f"âœ… å·²æ¥æ”¶æ–°æŒ‡ä»¤: {user_input[:100]}...")
                return user_input
            else:
                print("âŒ è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
    
    def _generation_interaction_log(self, agent: str, response_metadata: Any, content: BaseModel | Dict) -> InteractionItem:
        """è¿”å›ä¸€ä¸ªäº¤äº’æ—¥å¿—å¯¹è±¡ï¼Œcontentå¯ä»¥æ˜¯BaseModelæˆ–Dict"""
        if isinstance(content, dict):
            content_data = content
        else:
            content_data = content.model_dump()
        return InteractionItem(
            agent=agent,
            response_metadata=response_metadata,
            content=content_data
        )
    
    def _get_updated_usage(self,state:CodegenState,response:Any) ->Dict[str,Any]:
        """
        Params:
            state: çŠ¶æ€
            response: æ¨¡å‹å›å¤, ä¸€èˆ¬æ˜¯BaseMessageç±»å‹
        
        Returns: æ›´æ–°äº†ä½¿ç”¨æƒ…å†µçš„å­—å…¸
        """
        total_tokens = state.get("total_tokens",0) + response.response_metadata['token_usage']['total_tokens']
        total_prompt_tokens = state.get("total_prompt_tokens",0) + response.response_metadata['token_usage']['prompt_tokens']
        total_completion_tokens = state.get("total_completion_tokens",0)+ response.response_metadata['token_usage']['completion_tokens']
        return {
            "total_tokens": total_tokens,
            "total_prompt_tokens": total_prompt_tokens,
            "total_completion_tokens": total_completion_tokens
        }
